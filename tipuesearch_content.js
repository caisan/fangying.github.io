var tipuesearch = {"pages":[{"title":"About","url":"https://kernelgo.org/pages/about.html","loc":"https://kernelgo.org/pages/about.html","text":"Hi，我叫Fang Ying，Yori Fang是我的英文名字。 出生于1990年7月，毕业于： 浙江大学 信息与电子工程学院 (Master) 2013-2016 东北大学 计算机科学与工程学院 (Bachelor) 2009-2013 现在从事与linux内核和操作系统虚拟化方面的工作。主要工作在QEMU/KVM虚拟方向，致力于使能ICT云计算基础架构。 您可以通过下面的方式联系到我： Mail: fangying2725@gmail.com Github: https://github.com/fangying WeChat: Yori Fang 也可以通过WeChat赞赏来支持这个站点（所有的赞赏都会列出在本站中，非营利性用途，用于支付域名和Cloud Server的费用）。 捐赠列表 2020.10.8 CDX 捐赠100 RMB 2020.6.1 cmmjill 捐赠5 RMB 2020.5.29 飞舞~夏天 捐赠300 RMB 2020.4.25 Fan Yang 捐赠5 RMB 2020.4.12 Barry Song 捐赠10 RMB 2019.10.16 Limin Chang 捐赠 50 RMB Hi all, my name is Fang Ying and Yori Fang is my English name. I was born in July 1990. Graduated From: College of Electronic and Information Zhejiang University as Master 2013-2016 College of Computer Science and Engineering Northeastern University as Bachelor 2009-2013 I'm now working on things related to linux kernel and OS virtualization. I'm a virtualization developer, working on KVM and QEMU and dedicated to enable Cloud Computing for ICT infrastructure. Mail: fangying2725@gmail.com Github: https://github.com/fangying WeChat: Yori Fang You can conntact with me via WeChat, and can also encourage me to keep writting by donation. Donation list 2020.10.8 CDX donated 100 RMB 2020.6.1 cmmjill donated 5 RMB 2020.5.29 Daning Summuer donated 300 RMB 2020.4.25 Fan Yang donated 5 RMB 2020.4.12 Barry Song donated 10 RMB 2019.10.16 Limin Chang donated 50 RMB","tags":"pages"},{"title":"Use NeoVim on Apple M1 MacOS bigSur","url":"https://kernelgo.org/neovim-bigSur-apple-m1.html","loc":"https://kernelgo.org/neovim-bigSur-apple-m1.html","text":"最近开始切换到neovim了，毕竟neovim的设计理念相对vim8要超前很多。 经过一番配置折腾，我成功在bigSur Apple M1的MBP上配置好了neovim， 这里强烈推荐coc.nvim，这个插件能帮助我们实现类似vscode的强大功能， 它提供了LSP的支持，并且支持多种编程语言，用起来如丝般顺滑，个人体验感觉甩掉YCM几条街。 终于可以一个插件搞定函数定义跳转，引用查找，自动提示等一系列功能了。 安装neovim 在Apple M1 bigSur上现在已经支持通过 homebrew 方式安装neovim了。 xcode-select --install brew install --HEAD tree-sitter brew install --HEAD luajit brew install --HEAD neovim 安装好后nvim的默认路径是：/opt/homebrew/bin/nvim， 可以将/opt/homebrew/bin添加的PATH环境变量中。 nvim -v NVIM v0.5.0-dev+nightly Build type: Release LuaJIT 2 .1.0-beta3 Compilation: /usr/bin/clang -U_FORTIFY_SOURCE -D_FORTIFY_SOURCE = 1 -DNDEBUG -Wall -Wextra -pedantic -Wno-unused-parameter -Wstrict-prototypes -std = gnu99 -Wshadow -Wconversion -Wmissing-prototypes -Wimplicit-fallthrough -Wvla -fstack-protector-strong -fno-common -fdiagnostics-color = auto -DINCLUDE_GENERATED_DECLARATIONS -D_GNU_SOURCE -DNVIM_MSGPACK_HAS_FLOAT32 -DNVIM_UNIBI_HAS_VAR_FROM -DMIN_LOG_LEVEL = 3 -I/tmp/neovim-20210222-25344-s2v8tf/build/config -I/tmp/neovim-20210222-25344-s2v8tf/src -I/opt/homebrew/include -I/tmp/neovim-20210222-25344-s2v8tf/deps-build/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/opt/homebrew/opt/gettext/include -I/tmp/neovim-20210222-25344-s2v8tf/build/src/nvim/auto -I/tmp/neovim-20210222-25344-s2v8tf/build/include 编译者 yingfang@fang Features: +acl +iconv +tui See \":help feature-compile\" 系统 vimrc 文件: \" $VIM /sysinit.vim\" $VIM 预设值: \"/opt/homebrew/Cellar/neovim/HEAD-595f6e4/share/nvim\" Run :checkhealth for more info 安装coc.nvim nvim的默认配置文件路径是：~/.config/nvim/init.vim，这里自己创建一下就好了。 mkdir -pv ~/.config/nvim touch ~/.config/nvim/init.vim coc.nvim的安装很简单，使用 vim-plug 插件管理器即可： \" Use release branch (recommend) Plug 'neoclide/coc.nvim', {'branch': 'release'} 执行nvim +PlugInstall +qall来安装插件。 coc.nvim依赖于nodejs，所以这里要安装一下nodejs。 另外安装一下几个常见的coc插件。 brew install nodejs # vim执行命令 :CocInstall coc-json coc-snippets coc-toml coc-clangd coc-clangd coc-rust-analyzer :CocIntsall coc-marketplace # 这个插件帮助我们查看全部的插件列表，很实用 这里直接使用coc.nvim推荐的默认配置， 从 coc.nvim 直接拷贝就行了，追加到init.vim。 \" TextEdit might fail if hidden is not set. set hidden \" Some servers have issues with backup files, see #649. set nobackup set nowritebackup \" Give more space for displaying messages. set cmdheight = 2 \" Having longer updatetime (default is 4000 ms = 4 s) leads to noticeable \" delays and poor user experience. set updatetime = 300 \" Don't pass messages to |ins-completion-menu|. set shortmess += c \" Always show the signcolumn, otherwise it would shift the text each time \" diagnostics appear/become resolved. if has ( \"patch-8.1.1564\" ) \" Recently vim can merge signcolumn and number column into one set signcolumn = number else set signcolumn = yes endif \" Use tab for trigger completion with characters ahead and navigate. \" NOTE: Use command ':verbose imap <tab>' to make sure tab is not mapped by \" other plugin before putting this into your config. inoremap < silent >< expr > < TAB > \\ pumvisible () ? \"\\<C-n>\" : \\ < SID > check_back_space () ? \"\\<TAB>\" : \\ coc # refresh () inoremap < expr >< S - TAB > pumvisible () ? \"\\<C-p>\" : \"\\<C-h>\" function ! s: check_back_space () abort let col = col ( '.' ) - 1 return ! col || getline ( '.' )[ col - 1 ] =~ # '\\s' endfunction \" Use <c-space> to trigger completion. if has (' nvim ') inoremap < silent >< expr > < c - space > coc # refresh () else inoremap < silent >< expr > < c - @ > coc # refresh () endif \" Make <CR> auto-select the first completion item and notify coc.nvim to \" format on enter, <cr> could be remapped by other vim plugin inoremap < silent >< expr > < cr > pumvisible () ? coc # _select_confirm () \\: \"\\<C-g>u\\<CR>\\<c-r>=coc#on_enter()\\<CR>\" \" Use `[g` and `]g` to navigate diagnostics \" Use `:CocDiagnostics` to get all diagnostics of current buffer in location list. nmap < silent > [ g < Plug > ( coc - diagnostic - prev ) nmap < silent > ] g < Plug > ( coc - diagnostic - next ) \" GoTo code navigation. nmap < silent > gd < Plug > ( coc - definition ) nmap < silent > gy < Plug > ( coc - type - definition ) nmap < silent > gi < Plug > ( coc - implementation ) nmap < silent > gr < Plug > ( coc - references ) \" Use K to show documentation in preview window. nnoremap < silent > K : call < SID > show_documentation () < CR > function ! s: show_documentation () if ( index ([' vim ',' help '], & filetype ) >= 0 ) execute ' h '. expand (' < cword > ') elseif ( coc # rpc # ready ()) call CocActionAsync (' doHover ') else execute '!' . & keywordprg . \" \" . expand (' < cword > ') endif endfunction \" Highlight the symbol and its references when holding the cursor. autocmd CursorHold * silent call CocActionAsync (' highlight ') \" Symbol renaming. nmap < leader > rn < Plug > ( coc - rename ) \" Formatting selected code. xmap < leader > f < Plug > ( coc - format - selected ) nmap < leader > f < Plug > ( coc - format - selected ) augroup mygroup autocmd ! \" Setup formatexpr specified filetype(s). autocmd FileType typescript , json setl formatexpr = CocAction (' formatSelected ') \" Update signature help on jump placeholder. autocmd User CocJumpPlaceholder call CocActionAsync (' showSignatureHelp ') augroup end \" Applying codeAction to the selected region. \" Example: `<leader>aap` for current paragraph xmap < leader > a < Plug > ( coc - codeaction - selected ) nmap < leader > a < Plug > ( coc - codeaction - selected ) \" Remap keys for applying codeAction to the current buffer. nmap < leader > ac < Plug > ( coc - codeaction ) \" Apply AutoFix to problem on the current line. nmap < leader > qf < Plug > ( coc - fix - current ) \" Map function and class text objects \" NOTE: Requires 'textDocument.documentSymbol' support from the language server. xmap if < Plug > ( coc - funcobj - i ) omap if < Plug > ( coc - funcobj - i ) xmap af < Plug > ( coc - funcobj - a ) omap af < Plug > ( coc - funcobj - a ) xmap ic < Plug > ( coc - classobj - i ) omap ic < Plug > ( coc - classobj - i ) xmap ac < Plug > ( coc - classobj - a ) omap ac < Plug > ( coc - classobj - a ) \" Remap <C-f> and <C-b> for scroll float windows/popups. if has (' nvim - 0.4.0 ') || has (' patch - 8.2.0750 ') nnoremap < silent >< nowait >< expr > < C - f > coc # float # has_scroll () ? coc # float # scroll ( 1 ) : \"\\<C-f>\" nnoremap < silent >< nowait >< expr > < C - b > coc # float # has_scroll () ? coc # float # scroll ( 0 ) : \"\\<C-b>\" inoremap < silent >< nowait >< expr > < C - f > coc # float # has_scroll () ? \"\\<c-r>=coc#float#scroll(1)\\<cr>\" : \"\\<Right>\" inoremap < silent >< nowait >< expr > < C - b > coc # float # has_scroll () ? \"\\<c-r>=coc#float#scroll(0)\\<cr>\" : \"\\<Left>\" vnoremap < silent >< nowait >< expr > < C - f > coc # float # has_scroll () ? coc # float # scroll ( 1 ) : \"\\<C-f>\" vnoremap < silent >< nowait >< expr > < C - b > coc # float # has_scroll () ? coc # float # scroll ( 0 ) : \"\\<C-b>\" endif \" Use CTRL-S for selections ranges. \" Requires 'textDocument/selectionRange' support of language server. nmap < silent > < C - s > < Plug > ( coc - range - select ) xmap < silent > < C - s > < Plug > ( coc - range - select ) \" Add `:Format` command to format current buffer. command ! - nargs = 0 Format : call CocAction (' format ') \" Add `:Fold` command to fold current buffer. command ! - nargs =? Fold : call CocAction (' fold ', < f - args > ) \" Add `:OR` command for organize imports of the current buffer. command ! - nargs = 0 OR : call CocAction (' runCommand ', ' editor . action . organizeImport ') \" Add (Neo)Vim's native statusline support. \" NOTE: Please see `:h coc-status` for integrations with external plugins that \" provide custom statusline: lightline.vim, vim-airline. set statusline &#94;=% { coc # status ()} % { get ( b: ,' coc_current_function ',' ')} \" Mappings for CoCList \" Show all diagnostics. nnoremap < silent >< nowait > < space > a :< C - u > CocList diagnostics < cr > \" Manage extensions. nnoremap < silent >< nowait > < space > e :< C - u > CocList extensions < cr > \" Show commands. nnoremap < silent >< nowait > < space > c :< C - u > CocList commands < cr > \" Find symbol of current document. nnoremap < silent >< nowait > < space > o :< C - u > CocList outline < cr > \" Search workspace symbols. nnoremap < silent >< nowait > < space > s :< C - u > CocList - I symbols < cr > \" Do default action for next item. nnoremap < silent >< nowait > < space > j :< C - u > CocNext < CR > \" Do default action for previous item. nnoremap < silent >< nowait > < space > k :< C - u > CocPrev < CR > \" Resume latest coc list. nnoremap < silent >< nowait > < space > p :< C - u > CocListResume < CR > 可以参考我的neovim配置文件： https://github.com/fangying/vim/blob/master/init.vim 配置rust-analyzer的支持 使用:CocConfig命令，新增配置文件，开启对rust-analyzer的支持。 记得提前下载rust-analyzer二进制，放到~/.cargo/bin目录下，不然会提升找不到。 { \" coc.preferences.formatOnSaveFiletypes \" : [ \" rust \" ], \" diagnostic.errorSign \" : \" ✘ \" , \" diagnostic.infoSign \" : \" i \" , \" languageserver \" : { \" rust \" : { \" command \" : \" rust-analyzer \" , \" filetypes \" : [ \" rust \" ], \" rootPatterns \" : [ \" Cargo.toml \" ] }, \" ccls \" : { \" command \" : \" ccls \" , \" args \" : [ \" --log-file=/tmp/ccls.log \" , \" -v=1 \" ], \" filetypes \" : [ \" c \" , \" cc \" , \" cpp \" , \" c++ \" , \" objc \" , \" objcpp \" ], \" rootPatterns \" : [ \" .ccls \" , \" compile_commands.json \" , \" .git/ \" , \" .hg/ \" ], \" initializationOptions \" : { \" cache \" : { \" directory \" : \" .ccls-cache \" }, \" client \" : { \" snippetSupport \" : true }, \" clang \" : { \" extraArgs \" : [ \" -isystem \" , \" /opt/homebrew/Cellar/llvm/11.0.1/include/c++/v1 \" ], \" resourceDir \" : \" /Library/Developer/CommandLineTools/usr/lib/clang/12.0.0/ \" } } } }, \" rust-analyzer.server.path \" : \" ~/.cargo/bin/rust-analyzer \" , \" rust-analyzer.diagnostics.enable \" : true , \" rust-analyzer.inlayHints.chainingHints \" : true , \" rust-analyzer.inlayHints.typeHints \" : true }","tags":"misc"},{"title":"Use vim8 on Apple M1 MacOS bigSur","url":"https://kernelgo.org/vim-bigSur-apple-m1.html","loc":"https://kernelgo.org/vim-bigSur-apple-m1.html","text":"年前入手了Apple M1 MacBookPro，这是Apple第一款自研芯片的MBP，大小是13寸相对较为实用。 由于是ARM64架构，在刚上来的时候很多研发工具软件对系统的支持还不是太完善，但经过几个月的 发展目前已经基本能够满足日常开发需求。 安利3个传送门，可以方便地查询到各种生态软件对Apple M1的支持最新进展情况。 传送门1： https://isapplesiliconready.com/zh 传送门2： https://github.com/ThatGuySam/doesitarm 传送门3： https://doesitarm.com/ 由于我们是从事系统开发的软件行业，vim成为了日常看代码的必备工具，为此有必要设置一下Apple M1下的vim8工作环境。 配置vim8 M1上默认安装了vim 8.2版本，编译特性也很全： vim --version VIM - Vi IMproved 8 .2 ( 2019 Dec 12 , compiled Dec 21 2020 20 :40:21 ) macOS version Included patches: 1 -850, 1972 Compiled by root@apple.com vim插件配置可以参考: https://kernelgo.org/vim8 请自行阅读，这里不再详述。 在新的Apple M1上主要遇到了2个问题，导致vim8无法很好正常工作。 编译gnu global 由于要使用global来查看函数调用关系，所以我们要自己编译global来完成这个重要功能。 但是编译的时候会报错无法找到'realpath'。问题社区已经有报告: https://lists.gnu.org/archive/html/bug-global/2021-01/msg00001.html checking for dirent . h that defines DIR ... yes checking for library containing opendir ... none required checking whether POSIX . 1 - 2008 realpath is equipped ... no configure : error : POSIX . 1 - 2008 realpath ( 3 ) is required . 原因是configure脚本还没有适配好M1版本，这里提供一种规避方案： diff -- git a / configure b / configure index 100 a690 . .4e54 a52 100755 --- a / configure +++ b / configure @@ -14273 , 33 + 14273 , 6 @@ case \"$host_os\" in mingw *|* djgpp * ) ;; * ) - { $as_echo \"$as_me:${as_lineno-$LINENO}: checking whether POSIX.1-2008 realpath is equipped\" >& 5 - $as_echo_n \"checking whether POSIX.1-2008 realpath is equipped... \" >& 6 ; } - if $ { ac_cv_posix1_2008_realpath +: } false ; then : - $as_echo_n \"(cached) \" >& 6 - else - if test \"$cross_compiling\" = yes ; then : - { { $as_echo \"$as_me:${as_lineno-$LINENO}: error: in \\`$ac_pwd':\" >& 5 - $as_echo \"$as_me: error: in \\`$ac_pwd':\" >& 2 ;} - as_fn_error $ ? \"cannot run test program while cross compiling - See \\` config . log ' for more details \" \" $LINENO \" 5; } - else - cat confdefs . h - << _ACEOF > conftest . $ac_ext - /* end confdefs.h. */ - - main (){ ( void ) realpath ( \"/./tmp\" , ( void * ) 0 ); return 0 ; } - - _ACEOF - if ac_fn_c_try_run \"$LINENO\" ; then : - ac_cv_posix1_2008_realpath = yes - else - ac_cv_posix1_2008_realpath = no - fi - rm - f core * . core core . conftest . * gmon . out bb . out conftest$ac_exeext \\ - conftest . $ac_objext conftest . beam conftest . $ac_ext - fi - - fi { $as_echo \"$as_me:${as_lineno-$LINENO}: result: $ac_cv_posix1_2008_realpath\" >& 5 $as_echo \"$ac_cv_posix1_2008_realpath\" >& 6 ; } 打上这个补丁之后，再次编译就能够顺利编译通过了。 ./configure --with-universal-ctags=/usr/local/bin/ctags make -j sudo make install 修改gutentags配置参数 我们用universal-ctags来帮我们生成形系统符号表，这样我们可以用ctrl + ]来实现符号跳转。 在M1上需要注意的是，默认安装后可能会出现E433错误。 E433 : No tags file 这里需要打开日志开关，取消下面配置的注释： let g:gutentags_trace = 1 这样再次出现E433错误后，输入\":messages\"命令，可以看到错误的原因： gutentags : Scanning buffer '' for gutentags setup ... gutentags : No specific project type . gutentags : Setting gutentags for buffer '' gutentags : Generating missing tags file : / Users / yingfang / . cache / tags / Users - yingfang - Code - opensrc - qemu - . tags gutentags : Wildignore options file is up to date . gutentags : Running : [ ' / Users / yingfang / . vim / plugged / vim - gutentags / plat / unix / update_tags . sh ' , ' - e ' , ' ctags ' , ' - t ' , ' / Users / yingfang / . cache / tags / U sers - yingfang - Code - opensrc - qemu - . tags ' , ' - p ' , ' / Users / yingfang / Code / opensrc / qemu ' , ' - o ' , ' / Users / yingfang / . vim / plugged / vim - gutentags / res / ctags_ recursive . options ' , ' - O ' , ' -- fields =+ niazS ' , ' - O ' , ' -- extra =+ q ' , ' - O ' , ' -- c ++- kinds =+ pxI ' , ' - O ' , ' -- c - kinds =+ px ' , ' - O ' , ' -- output - format = e - ctag s ' , ' - l ' , ' / Users / yingfang / . cache / tags / Users - yingfang - Code - opensrc - qemu - . tags . log ' ] gutentags : In : / Users / yingfang / Code / opensrc / qemu gutentags : Generating tags file : / Users / yingfang / . cache / tags / Users - yingfang - Code - opensrc - qemu / GTAGS gutentags : Running : [ ' gtags ' , ' -- incremental ' , ' / Users / yingfang / . cache / tags / Users - yingfang - Code - opensrc - qemu ' ] gutentags : In : / Users / yingfang / Code / opensrc / qemu gutentags : gutentags : [ job stdout ] : ' Locking tags file ... ' gutentags : [ job stdout ] : ' Running ctags on whole project ' gutentags : [ job stdout ] : ' ctags - f \"/Users/yingfang/.cache/tags/Users-yingfang-Code-opensrc-qemu-.tags.temp\" \"--options=/Users/yingfang/.vim/pl ugged / vim - gutentags / res / ctags_recursive . options \" --fields=+niazS --extra=+q --c++-kinds=+pxI --c-kinds=+px --output-format=e-ctags \" / Users / yin gfang / Code / opensrc / qemu \"' gutentags : [ job stderr ] : ' ctags : Warning : -- extra option is obsolete ; use -- extras instead ' gutentags : [ job stderr ] : ' ctags : Warning : Unsupported kind : ' 'I' ' for -- c ++- kinds option ' gutentags : Finished gtags_cscope job . 原因是在M1上我们编译的ctags支持的参数命令上和X86平台上差异。 解决办法：修改-extra参数为-extras，去掉c++-kinds上的'I'。 修改后对应的配置项为: let g:gutentags_ctags_extra_args = ['--fields=+niazS', '--extras=+q'] let g:gutentags_ctags_extra_args += ['--c++-kinds=+px'] let g:gutentags_ctags_extra_args += ['--c-kinds=+px'] 改完之后应该就没有E433错误了。 到这里就可以在Apple M1上愉快的使用vim8来看代码了。 Peace.","tags":"misc"},{"title":"Understanding C11/C++11 Memory Model","url":"https://kernelgo.org/memory-model.html","loc":"https://kernelgo.org/memory-model.html","text":"现代计算机体系结构上，CPU执行指令的速度远远大于CPU访问内存的速度，于是引入Cache机制来加速内存访问速度。 除了Cache以外，分支预测和指令预取也在很大程度上提升了CPU的执行速度。 随着SMP的出现，多线程编程模型被广泛应用，在多线程模型下对共享变量的访问变成了一个复杂的问题。 于是我们有必要了解一下内存模型，这是多处理器架构下并发编程里必须掌握的一个基础概念。 1. 什么是内存模型？ 到底什么是内存模型呢？看到有两种不同的观点： A：内存模型是从来描述编程语言在支持多线程编程中对共享内存访问的顺序。 B：内存模型的本质是指在单线程情况下CPU指令在多大程度上发生指令重排(reorder)[1]。 实际上A，B两种说法都是正确的，只不过是在尝试从不同的角度去说明memory model的概念。 个人认为，内存模型表达为\"内存顺序模型\"可能更加贴切一点。 一个良好的memory model定义包含3个方面： Atomic Operations Partial order of operations Visable effects of operations 这里要强调的是 ： 我们这里所说的内存模型和CPU的体系结构、编译器实现和编程语言规范3个层面都有关系。 首先， 不同的CPU体系结构内存顺序模型是不一样的 ，但大致分为两种： Architecture Memory Model x86_64 Total Store Order Sparc Total Store Order ARMv8 Weakly Ordered PowerPC Weakly Ordered MIPS Weakly Ordered x86_64和Sparc是强顺序模型（Total Store Order），这是一种接近程序顺序的顺序模型。 所谓Total，就是说，内存（在写操作上）是有一个全局的顺序的（所有人看到的一样的顺序）， 就好像在内存上的每个Store动作必须有一个排队，一个弄完才轮到另一个，这个顺序和你的程序顺序直接相关。 所有的行为组合只会是所有CPU内存程序顺序的交织，不会发生和程序顺序不一致的地方[4]。 TSO模型有利于多线程程序的编写，对程序员更加友好，但对芯片实现者不友好。 CPU为了TSO的承诺，会牺牲一些并发上的执行效率。 弱内存模型（简称WMO，Weak Memory Ordering），是把是否要求强制顺序这个要求直接交给程序员的方法。 换句话说，CPU不去保证这个顺序模型（除非他们在一个CPU上就有依赖）， 程序员要主动插入内存屏障指令来强化这个\"可见性\"[4]。 ARMv8，PowerPC和MIPS等体系结构都是弱内存模型。 每种弱内存模型的体系架构都有自己的内存屏障指令，语义也不完全相同。 弱内存模型下，硬件实现起来相对简单，处理器执行的效率也高， 只要没有遇到显式的屏障指令，CPU可以对局部指令进行reorder以提高执行效率。 对于多线程程序开发来说，对并发的数据访问我们一般到做同步操作， 可以使用mutex，semaphore，conditional等重量级方案对共享数据进行保护。 但为了实现更高的并发，需要使用内存共享变量做通信（Message Passing）， 这就对程序员的要求很高了，程序员必须时时刻刻必须很清楚自己在做什么， 否则写出来的程序的执行行为会让人很是迷惑！ 值得一提的是，并发虽好，如果能够简单粗暴实现，就不要搞太多投机取巧！ 要实现lock-free无锁编程真的有点难。 其次， 不同的编程语言对内存模型都有自己的规范 ，例如： C/C++和Java等不同的编程语言都有定义内存模型相关规范。 2011年发布的C11/C++11 ISO Standard为我们带来了memory order的支持， 引用C++11里的一段描述： The memory model means that C ++ code now has a standardized library to call regardless of who made the compiler and on what platform it ' s running . There ' s a standard way to control how different threads talk to the processor ' s memory .[ 7 ] memory order的问题就是因为指令重排引起的, 指令重排导致 原来的内存可见顺序发生了变化, 在单线程执行起来的时候是没有问题的, 但是放到 多核/多线程执行的时候就出现问题了, 为了效率引入的额外复杂逻辑的的弊端就出现了[8]。 C++11引入memory order的意义在于： 在语言层提供了一个与运行平台无关和编译器无关的标准库， 让我们可以在high level languange层面实现对多处理器对共享内存的交互式控制 。 我们的多线程终于可以跨平台啦！我们可以借助内存模型写出更好更安全的并发代码。 真棒，简直不要太优秀~ C11/C++11使用memory order来描述memory model， 而用来联系memory order的是atomic变量， atomic操作可以用load()和release()语义来描述。 一个简单的atomic变量赋值可描述为： atomic_var1 . store ( atomic_var2 . load ()); // atomic variables vs var1 = var2 ; // regular variables 为了更好地描述内存模型，有4种关系术语需要了解一下。 sequenced-before 同一个线程之内，语句A的执行顺序在语句B前面，那么就成为A sequenced-before B。 它不仅仅表示两个操作之间的先后顺序，还表示了操作结果之间的可见性关系。 两个操作A和操作B，如果有A sequenced-before B，除了表示操作A的顺序在B之前，还表示了操作A的结果操作B可见。 例如：语句A是sequenced-before语句B的。 r2 = x . load ( std :: memory_order_relaxed ); // A y . store ( 42 , std :: memory_order_relaxed ); // B happens-before happens-before关系表示的不同线程之间的操作先后顺序。 如果A happens-before B，则A的内存状态将在B操作执行之前就可见。 happends-before关系满足传递性、非自反性和非对称性。 happens before包含了inter-thread happens before和synchronizes-with两种关系。 synchronizes-with synchronizes-with关系强调的是变量被修改之后的传播关系（propagate）， 即如果一个线程修改某变量的之后的结果能被其它线程可见，那么就是满足synchronizes-with关系的[9]。 另外synchronizes-with可以被认为是跨线程间的happends-before关系。 显然，满足synchronizes-with关系的操作一定满足happens-before关系了。 Carries dependency 同一个线程内，表达式A sequenced-before 表达式B，并且表达式B的值是受表达式A的影响的一种关系， 称之为\"Carries dependency\"。这个很好理解，例如： int * a = & var1 ; // A int * b = & var2 ; // B c = * a + * b ; // C 执行语句A和B与语句C之间就存在\"Carries dependency\"关系， 因为c的值直接依赖于*a和*b的值。 了解了上面一些基本概念，下面我们来一起学习一下内存模型吧。 2. C11/C++11内存模型 C/C++11标准中提供了6种memory order，来描述内存模型[6]: enum memory_order { memory_order_relaxed , memory_order_consume , memory_order_acquire , memory_order_release , memory_order_acq_rel , memory_order_seq_cst }; 每种memory order的规则可以简要描述为： 枚举值 定义规则 memory_order_relaxed 不对执行顺序做任何保证 memory_order_consume 本线程中，所有后续的有关本原子类型的操作，必须在本条原子操作完成之后执行 memory_order_acquire 本线程中，所有后续的读操作必须在本条原子操作完成后执行 memory_order_release 本线程中，所有之前的写操作完成后才能执行本条原子操作 memory_order_acq_rel 同时包含memory_order_acquire和memory_order_release标记 memory_order_seq_cst 全部存取都按顺序执行 下面我们来举例一一说明，扒开内存模型的神秘面纱。 2.1 memory order releaxed relaxed 表示一种最为宽松的内存操作约定，Relaxed ordering 仅仅保证load()和store()是原子操作， 除此之外，不提供任何跨线程的同步[5]。 std :: atomic < int > x = 0 ; // global variable std :: atomic < int > y = 0 ; // global variable Thread - 1 : Thread - 2 : r1 = y . load ( memory_order_relaxed ); // A r2 = x . load ( memory_order_relaxed ); // C x . store ( r1 , memory_order_relaxed ); // B y . store ( 42 , memory_order_relaxed ); // D 上面的多线程模型执行的时候，可能出现r2 == r1 == 42。 要理解这一点并不难，因为CPU在执行的时候允许局部指令重排reorder，D可能在C前执行。 如果程序的执行顺序是 D -> A -> B -> C，那么就会出现r1 == r2 == 42。 如果某个操作只要求是原子操作，除此之外，不需要其它同步的保障，那么就可以使用 relaxed ordering。 程序计数器是一种典型的应用场景： #include <cassert> #include <vector> #include <iostream> #include <thread> #include <atomic> std :: atomic < int > cnt = { 0 }; void f () { for ( int n = 0 ; n < 1000 ; ++ n ) { cnt . fetch_add ( 1 , std :: memory_order_relaxed ); } } int main () { std :: vector < std :: thread > v ; for ( int n = 0 ; n < 10 ; ++ n ) { v . emplace_back ( f ); } for ( auto & t : v ) { t . join (); } assert ( cnt == 10000 ); // never failed return 0 ; } cnt 是共享的全局变量，多个线程并发地对 cnt 执行RMW（Read Modify Write）原子操作。 这里只保证 cnt 的原子性，其他有依赖 cnt 的地方不保证任何的同步。 2.2 memory order consume consume 要搭配 release 一起使用。很多时候， 线程间只想针对有依赖关系的操作进行同步 ， 除此之外线程中其他操作顺序如何不关心，这时候就适合用 consume 来完成这个操作。 例如： b = * a ; c = * b 第二行的变量c依赖于第一行的执行结果，因此这两行代码是\"Carries dependency\"关系。 显然，由于 consume 是针对有明确依赖关系的语句来限定其执行顺序的一种内存顺序， 而 releaxed 不提供任何顺序保证， 所以consume order要比releaxed order要更加地Strong。 #include <thread> #include <atomic> #include <cassert> #include <string> std :: atomic < std :: string *> ptr ; int data ; void producer () { std :: string * p = new std :: string ( \"Hello\" ); data = 42 ; ptr . store ( p , std :: memory_order_release ); } void consumer () { std :: string * p2 ; while ( ! ( p2 = ptr . load ( std :: memory_order_consume ))) ; assert ( * p2 == \"Hello\" ); // never fires: *p2 carries dependency from ptr assert ( data == 42 ); // may or may not fire: data does not carry dependency from ptr } int main () { std :: thread t1 ( producer ); std :: thread t2 ( consumer ); t1 . join (); t2 . join (); } assert(*p2 == \"Hello\")永远不会失败，但assert(data == 42)可能会。 原因是： p2和ptr直接有依赖关系，但data和ptr没有直接依赖关系， 尽管线程1中data赋值在ptr.store()之前，线程2看到的data的值还是不确定的。 2.3 memory order acquire acquire 和 release 也必须放到一起使用。 release 和 acquire 构成了synchronize-with关系，也就是同步关系。 在这个关系下： 线程A中所有发生在release x之前的值的写操作， 对线程B的acquire x之后的任何操作都可见 。 #include <thread> #include <atomic> #include <cassert> #include <string> #include <iostream> std :: atomic < bool > ready { false }; int data = 0 ; std :: atomic < int > var = { 0 }; void sender () { data = 42 ; // A var . store ( 100 , std :: memory_order_relaxed ); // B ready . store ( true , std :: memory_order_release ); // C } void receiver () { while ( ! ready . load ( std :: memory_order_acquire )) // D ; assert ( data == 42 ); // never failed // E assert ( var == 100 ); // never failed // F } int main () { std :: thread t1 ( sender ); std :: thread t2 ( receiver ); t1 . join (); t2 . join (); } 上面的例子中： sender线程中 data = 42 是sequence before原子变量ready的 sender和receiver在C和D处发生了同步 线程sender中C之前的所有读写对线程receiver都是可见的 显然， release 和 acquire 组合在一起比 release 和 consume 组合更加Strong！ 2.4 memory order release release order一般不单独使用，它和 acquire 和 consume 组成2种独立的内存顺序搭配。 这里就不用展开啰里啰嗦了。 2.5 memory order acq_rel acq_rel 是acquire和release的叠加。中文不知道该咋描述好： A read - modify - write operation with this memory order is both an acquire operation and a release operation . No memory reads or writes in the current thread can be reordered before or after this store . All writes in other threads that release the same atomic variable are visible before the modification and the modification is visible in other threads that acquire the same atomic variable . 大致意思是： memory_order_acq_rel适用于read-modify-write operation， 对于采用此内存序的read-modify-write operation，我们可以称为acq_rel operation， 既属于acquire operation 也是release operation. 设有一个原子变量M上的acq_rel operation： 自然的，该acq_rel operation之前的内存读写都不能重排到该acq_rel operation之后， 该acq_rel operation之后的内存读写都不能重排到该acq_rel operation之前. 其他线程中所有对M的release operation及其之前的写入都对当前线程从该acq_rel operation开始的操作可见， 并且截止到该acq_rel operation的所有内存写入都对另外线程对M的acquire operation以及之后的内存操作可见[13]。 这里是一个例子，关于为什么要有 acq_rel 可以参考一下： #include <thread> #include <atomic> #include <cassert> #include <vector> std :: vector < int > data ; std :: atomic < int > flag = { 0 }; void thread_1 () { data . push_back ( 42 ); flag . store ( 1 , std :: memory_order_release ); } void thread_2 () { int expected = 1 ; while ( ! flag . compare_exchange_strong ( expected , 2 , std :: memory_order_acq_rel )) { expected = 1 ; } } void thread_3 () { while ( flag . load ( std :: memory_order_acquire ) < 2 ) ; assert ( data . at ( 0 ) == 42 ); // will never fire } int main () { std :: thread a ( thread_1 ); std :: thread b ( thread_2 ); std :: thread c ( thread_3 ); a . join (); b . join (); c . join (); } 2.6 memory order seq_cst seq_cst 表示顺序一致性内存模型，在这个模型约束下 不仅同一个线程内的执行结果是和程序顺序一致的， 每个线程间互相看到的执行结果和程序顺序也保持顺序一致 。 显然， seq_cst 的约束是最强的，这意味着要牺牲性能为代价。 atomic int x ( 0 ); atomic int y ( 0 ); x . store ( 1 , seq cst ); || y . store ( 1 , seq cst ); int r1 = y . load ( seq cst ); || int r2 = x . load ( seq cst ); assert ( r1 == 1 || r2 == 1 ); 下面是一个seq_cst的实例： #include <thread> #include <atomic> #include <cassert> std :: atomic < bool > x = { false }; std :: atomic < bool > y = { false }; std :: atomic < int > z = { 0 }; void write_x () { x . store ( true , std :: memory_order_seq_cst ); } void write_y () { y . store ( true , std :: memory_order_seq_cst ); } void read_x_then_y () { while ( ! x . load ( std :: memory_order_seq_cst )) ; if ( y . load ( std :: memory_order_seq_cst )) { ++ z ; } } void read_y_then_x () { while ( ! y . load ( std :: memory_order_seq_cst )) ; if ( x . load ( std :: memory_order_seq_cst )) { ++ z ; } } int main () { std :: thread a ( write_x ); std :: thread b ( write_y ); std :: thread c ( read_x_then_y ); std :: thread d ( read_y_then_x ); a . join (); b . join (); c . join (); d . join (); assert ( z . load () != 0 ); // will never happen } 2.7 Relationship with volatile 人的一生总是充满了疑惑 。 可能你会思考，volatile关键字能够防止指令被编译器优化，那它能提供线程间(inter-thread)同步语义吗？ 答案是： 不能 ！！！ 尽管volatile能够防止单个线程内对volatile变量进行reorder，但多个线程同时访问同一个volatile变量，线程间是完全不提供同步保证。 而且，volatile不提供原子性！ 并发的读写volatile变量是会产生数据竞争的，同时non volatile操作可以在volatile操作附近自由地reorder。 看一个例子，执行下面的并发程序，不出意外的话，你不会得到一个为0的结果。 #include <thread> #include <iostream> volatile int count = 0 ; void increase () { for ( int i = 0 ; i < 1000000 ; i ++ ) { count ++ ; } } void decrease () { for ( int i = 0 ; i < 1000000 ; i ++ ) { count -- ; } } int main () { std :: thread t1 ( increase ); std :: thread t2 ( decrease ); t1 . join (); t2 . join (); std :: cout << count << std :: endl ; } 如果要进一步理解这个topic，建议可以听一下油管上的大拿视频，可以更好地理解memory model的含义。 3. Reference The C/C++ Memory Model: Overview and Formalization 知乎专栏：如何理解C++的6种memory order 理解 C++ 的 Memory Order 理解弱内存顺序模型 当我们在谈论 memory order 的时候，我们在谈论什么 https://en.cppreference.com/w/cpp/atomic/memory_order Youtube: Atomic's memory orders, what for? - Frank Birbacher [ACCU 2017] C++11中的内存模型下篇 - C++11支持的几种内存模型 memory ordering, Gavin's blog c++11 内存模型解读 memory barriers in c, MariaDB FOUNDATION, pdf C++ memory order循序渐进 Memory Models for C/C++ Programers Memory Consistency Models: A Tutorial","tags":"linux"},{"title":"Memory Hotplug & CPU Hotplug","url":"https://kernelgo.org/qemu-device-hotplug.html","loc":"https://kernelgo.org/qemu-device-hotplug.html","text":"在虚拟化场景下Guest的设备由软件进行模拟，所以可以比较方便地实现设备热插、热拔， 在不影响虚拟机正常运行的情况下，动态调整设备数目可以达到在线调整Guest算力资源的目的。 x86 platform上已经支持了Memory和CPU的hotplug/hotunplug等高级特性，aarch64现在也已经跟上。 这篇文章整理一下最近openEuler上合入的Memory和CPU hotplug特性。 1. Memory Hotplug 特性 QEMU 4.2版本给我们带来了期待已久的Memory Hotplug特性，相关实现patch可以patchworks上获取到： https://patchwork.kernel.org/cover/11150345/ 在ARM64平台上没有PIO空间（PIO是x86独有的），为了支持memory hotplug event signaling， 于是使用了GED(Generic Event Device)设备来完成事件通知机制。 有点好奇什么是GED设备？翻阅了ACPI Spec才知道， GED是ACPI 6.1规范在Hardware Reduced模式下定义的一个事件通知设备， ARM64上的内存热插使用这个设备通知Guest。 备注：这有个疑问是，为啥不像x86上一样用ACPI GPE（General-Purpose Event）来做这个事情呢? generic_event_device.c 中定义了GED设备类型，为了支持hotplug这个设备 实现了两个qom interface， TYPE_HOTPLUG_HANDLER 和 TYPE_ACPI_DEVICE_IF 。 前者定义了hotplug相关的hook，后者定义了acpi相关的hook，体现了OOB设计的思想。 static const TypeInfo acpi_ged_info = { . name = TYPE_ACPI_GED , . parent = TYPE_SYS_BUS_DEVICE , // 挂载到sysbus . instance_size = sizeof ( AcpiGedState ), // 设备状态 . instance_init = acpi_ged_initfn , // 设备实例化函数 . class_init = acpi_ged_class_init , // 类初始化函数 . interfaces = ( InterfaceInfo []) { // 实现接口列表 { TYPE_HOTPLUG_HANDLER }, { TYPE_ACPI_DEVICE_IF }, { } } }; 实现了2个相关interface之后，需要为Memory Hotplug定义自己的hook函数（但实际上只需实现一部分接口）。 所以，这里对应的实现是 acpi_ged_device_plug_cb 和 acpi_ged_send_event 。 static void acpi_ged_class_init ( ObjectClass * class , void * data ) { DeviceClass * dc = DEVICE_CLASS ( class ); HotplugHandlerClass * hc = HOTPLUG_HANDLER_CLASS ( class ); AcpiDeviceIfClass * adevc = ACPI_DEVICE_IF_CLASS ( class ); dc -> desc = \"ACPI Generic Event Device\" ; device_class_set_props ( dc , acpi_ged_properties ); // 定义了properties dc -> vmsd = & vmstate_acpi_ged ; // 热迁移状态保存和恢复 hc -> plug = acpi_ged_device_plug_cb ; // hotplug callback 回调 adevc -> send_event = acpi_ged_send_event ; // hotplug event事件注入 } GED设备状态里面维护了6个成员： parent_obj 用来将设备挂载到Sysbus下 memhp_state 用来描述MemoryHotplug状态 container_memhp 这个MR Container MMIO区域，长度24字节 ged_state 用来描述GED状态 ged_event_bitmap 事件bitmap irq 设备有一个GPIO中断，给谁用的？ typedef struct AcpiGedState { SysBusDevice parent_obj ; // 挂载到sysbus下 MemHotplugState memhp_state ; // MemoryHotplug状态 MemoryRegion container_memhp ; // MR GEDState ged_state ; // GED状态 uint32_t ged_event_bitmap ; // event bitmap qemu_irq irq ; // irq } AcpiGedState ; 这个GED设备存在的唯一意义就是帮助完成Memory Hotplug。 有必要了解一下这个GED设备，根据 ACPI Spec Chapter 5.6.9 Interrupt-signaled ACPI events 章节， GED设备使用_CRS（Current Resource Setting）描述中断，使用_EVT对象来映射ACPI事件。 例如：下面的ASL中定义了3个中断及其回调函数 Device ( \\ _SB . GED1 ) { Name ( HID , \" ACPI0013 \" ) Name ( _CRS , ResourceTemplate () { Interrupt ( ResourceConsumer , Level , ActiveHigh , Exclusive ) { 41 } Interrupt ( ResourceConsumer , Edge , ActiveHigh , Shared ) { 42 } Interrupt ( ResourceConsumer , Level , ActiveHigh , ExclusiveAndWake ) { 43 } } Method ( _EVT , 1 ) { // Handle all ACPI Events signaled by the Generic Event Device ( GED1 ) Switch ( Arg0 ) // Arg0 = GSIV of the interrupt { Case ( 41 ) { // interrupt 41 Store ( One , ISTS ) // clear interrupt status register at device X // which is mapped via an operation region Notify ( \\ _SB . DEVX , 0x0 ) // insertion request } Case ( 42 ) { // interrupt 42 Notify ( \\ _SB . DEVX , 0x3 ) // ejection request } Case ( 43 ) { // interrupt 43 Store ( One , ISTS ) // clear interrupt status register at device X // which is mapped via an operation region Notify ( \\ _SB . DEVX , 0x2 ) // wake event } } } // End of Method } // End of GED1 Scope Memory Hotplug实现的重点部分是ACPI表的那一部分。 为了支持Memory Hotplug需要提前做的事情有2个： ARM64上virt主板初始化的时候要创建GED设备 主板创建完成之后开始构建GED ACPI表，如果配置了numa要构建srat表 machvirt_init -> vms -> acpi_dev = create_acpi_ged ( vms ) // 创建ged设备 virt_machine_done -> virt_acpi_setup -> virt_acpi_build -> build_dsdt -> build_ged_aml // build GED ACPI table -> build_memory_hotplug_aml // build DIMM hotplug ACPI table -> build_srat // numa节点>0的时候要呈现SRAT 1.1 创建ged设备 GED设备创建的时候做哪些事情？ 创建ged设备 设置了属性 ged-event 当前只支持了2个事件：powerdown事件和hotplug事件 设置了ged设备和dimm设备的mmio基地址 连接了ged设备的GPIO 设备属性 ged-event 是32bit的，理论上可以支持很多事件。 而且可以看到，设备的ACPI和PCDIMM基地址是提前分配好的（静态的）。 static inline DeviceState * create_acpi_ged ( VirtMachineState * vms ) { DeviceState * dev ; MachineState * ms = MACHINE ( vms ); int irq = vms -> irqmap [ VIRT_ACPI_GED ]; uint32_t event = ACPI_GED_PWR_DOWN_EVT ; if ( ms -> ram_slots ) { event |= ACPI_GED_MEM_HOTPLUG_EVT ; } dev = qdev_create ( NULL , TYPE_ACPI_GED ); qdev_prop_set_uint32 ( dev , \"ged-event\" , event ); sysbus_mmio_map ( SYS_BUS_DEVICE ( dev ), 0 , vms -> memmap [ VIRT_ACPI_GED ]. base ); sysbus_mmio_map ( SYS_BUS_DEVICE ( dev ), 1 , vms -> memmap [ VIRT_PCDIMM_ACPI ]. base ); sysbus_connect_irq ( SYS_BUS_DEVICE ( dev ), 0 , qdev_get_gpio_in ( vms -> gic , irq )); qdev_init_nofail ( dev ); // 设备实例化，调用realize函数 return dev ; } 这里隐含了的是，ged设备在实例化的时候 acpi_ged_initfn 里调用了 acpi_memory_hotplug_init 。 此时，会为DIMM设备创建好一个名为 acpi-mem-hotplug 的MemoryRegion， 作为一段MMIO呈现给Guest OS，Guest可以读写这个MR来查询DIMM slot状态， 并支持Memory Hot-unplug这个高级特性。 1.2 构建ged的acpi表 ged设备的ACPI信息方在DSDT(Differentiated System Description Table)表中， 在函数 build_ged_aml 来构建AML信息： void build_ged_aml ( Aml * table , const char * name , HotplugHandler * hotplug_dev , uint32_t ged_irq , AmlRegionSpace rs , hwaddr ged_base ) { AcpiGedState * s = ACPI_GED ( hotplug_dev ); Aml * crs = aml_resource_template (); // 创建_CRS，描述中断信息 Aml * evt , * field ; Aml * dev = aml_device ( \"%s\" , name ); Aml * evt_sel = aml_local ( 0 ); Aml * esel = aml_name ( AML_GED_EVT_SEL ); /* _CRS interrupt */ // 定义了一个_CRS中断，编号irqmap[VIRT_ACPI_GED] + ARM_SPI_BASE aml_append ( crs , aml_interrupt ( AML_CONSUMER , AML_EDGE , AML_ACTIVE_HIGH , AML_EXCLUSIVE , & ged_irq , 1 )); aml_append ( dev , aml_name_decl ( \"_HID\" , aml_string ( \"ACPI0013\" ))); aml_append ( dev , aml_name_decl ( \"_UID\" , aml_string ( GED_DEVICE ))); aml_append ( dev , aml_name_decl ( \"_CRS\" , crs )); /* Append IO region */ // 添加EVT_REG和EVT_SEL寄存器 aml_append ( dev , aml_operation_region ( AML_GED_EVT_REG , rs , aml_int ( ged_base + ACPI_GED_EVT_SEL_OFFSET ), ACPI_GED_EVT_SEL_LEN )); field = aml_field ( AML_GED_EVT_REG , AML_DWORD_ACC , AML_NOLOCK , AML_WRITE_AS_ZEROS ); aml_append ( field , aml_named_field ( AML_GED_EVT_SEL , ACPI_GED_EVT_SEL_LEN * BITS_PER_BYTE )); aml_append ( dev , field ); /* * For each GED event we: * - Add a conditional block for each event, inside a loop. * - Call a method for each supported GED event type. * * The resulting ASL code looks like: * * Local0 = ESEL * If ((Local0 & One) == One) * { * MethodEvent0() * } * * If ((Local0 & 0x2) == 0x2) * { * MethodEvent1() * } * ... */ evt = aml_method ( \"_EVT\" , 1 , AML_SERIALIZED ); { Aml * if_ctx ; uint32_t i ; uint32_t ged_events = ctpop32 ( s -> ged_event_bitmap ); /* Local0 = ESEL */ aml_append ( evt , aml_store ( esel , evt_sel )); for ( i = 0 ; i < ARRAY_SIZE ( ged_supported_events ) && ged_events ; i ++ ) { uint32_t event = s -> ged_event_bitmap & ged_supported_events [ i ]; if ( ! event ) { continue ; } if_ctx = aml_if ( aml_equal ( aml_and ( evt_sel , aml_int ( event ), NULL ), aml_int ( event ))); switch ( event ) { case ACPI_GED_MEM_HOTPLUG_EVT : // 调用Memory Slot扫描函数 aml_append ( if_ctx , aml_call0 ( MEMORY_DEVICES_CONTAINER \".\" MEMORY_SLOT_SCAN_METHOD )); break ; case ACPI_GED_PWR_DOWN_EVT : aml_append ( if_ctx , // 调用回调函数，似乎是用来关机 aml_notify ( aml_name ( ACPI_POWER_BUTTON_DEVICE ), aml_int ( 0x80 ))); break ; default : /* * Please make sure all the events in ged_supported_events[] * are handled above. */ g_assert_not_reached (); } aml_append ( evt , if_ctx ); ged_events -- ; } if ( ged_events ) { error_report ( \"Unsupported events specified\" ); abort (); } } /* Append _EVT method */ aml_append ( dev , evt ); aml_append ( table , dev ); } 除此之外，还要构建DIMM支持Hotplug的ACPI表，它的实现函数是 build_memory_hotplug_aml 。 在这个表里面存放了当前主板上内存的插槽信息 max_slot_num ， 插槽状态信息，和插槽扫描函数( MEMORY_SLOT_SCAN_METHOD )等关键信息。 Guest在boot的时候会去解析这些表的信息，当事件到来时就调用对应的函数来处理 Memory Hotplug/unplug事件。 当配置了Guest Numa的时候，还要去呈现SRAT（System Resource Affinity Table）表。 这个过程是在 build_srat 中实现，目的是为了呈现资源亲和性。 1.3 Linux arm64 内核支持memory hotplug QEMU 4.1支持arm64 memory hotplug之后当然也需要Linux kernel进行支持， 相关的内核patch可以在patchwork上获取到（看pw历史记录有助于了解patch review的过程）： https://patchwork.kernel.org/patch/10724455/ 对应的内核git commit是: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=4ab215061554ae2a4b78744a5dd3b3c6639f16a7 不得不说的是，有点气人，尽管没几行代码但这个patch我看不懂！！！ 1.4 测试arm64虚拟机的Memory Hotplug功能 Launch qemu虚拟机进程，初始配置8G内存2个numa node，最大支持内存512G内存。 / usr / bin / qemu - system - aarch64 \\ - M virt , usb = off , accel = kvm , gic - version = 3 \\ - cpu host \\ - smp sockets = 1 , cores = 4 \\ - m size = 8 G , slots = 128 , maxmem = 512 G \\ - numa node , nodeid = 0 , cpus = 0 - 1 , mem = 4 G \\ - numa node , nodeid = 1 , cpus = 2 - 3 , mem = 4 G \\ - chardev socket , id = qmp , path =/ var / run / qemu - qmp . qmp , server , nowait \\ - mon chardev = qmp , mode = control \\ - nodefaults \\ - drive file =/ usr / share / edk2 / aarch64 / QEMU_EFI - pflash . raw , if = pflash , format = raw , unit = 0 , readonly = on \\ - drive file =/ var / lib / libvirt / qemu / nvram / fangying_openeuler_VARS . fd , if = pflash , format = raw , unit = 1 \\ - device pcie - root - port , port = 0x8 , chassis = 1 , id = pci . 1 , bus = pcie . 0 , multifunction = on , addr = 0x1 \\ - device pcie - root - port , port = 0x9 , chassis = 2 , id = pci . 2 , bus = pcie . 0 , addr = 0x1 . 0x1 \\ - device pcie - pci - bridge , id = pci . 3 , bus = pci . 1 , addr = 0x0 \\ - device pcie - root - port , port = 0xa , chassis = 4 , id = pci . 4 , bus = pcie . 0 , addr = 0x1 . 0x2 \\ - device pcie - root - port , port = 0xb , chassis = 5 , id = pci . 5 , bus = pcie . 0 , addr = 0x1 . 0x3 \\ - device pcie - root - port , port = 0xc , chassis = 6 , id = pci . 6 , bus = pcie . 0 , addr = 0x1 . 0x4 \\ - device pcie - root - port , port = 0xd , chassis = 7 , id = pci . 7 , bus = pcie . 0 , addr = 0x1 . 0x5 \\ - device pcie - root - port , port = 0xe , chassis = 8 , id = pci . 8 , bus = pcie . 0 , addr = 0x1 . 0x6 \\ - device pcie - root - port , port = 0xf , chassis = 9 , id = pci . 9 , bus = pcie . 0 , addr = 0x1 . 0x7 \\ - device virtio - blk - pci , drive = drive - virtio0 , id = virtio0 , bus = pci . 4 , addr = 0x0 \\ - drive file =./ openeuler - lts . qcow2 , if = none , id = drive - virtio0 , cache = none , aio = native \\ - device usb - ehci , id = usb , bus = pci . 3 , addr = 0x1 \\ - device usb - tablet , id = input0 , bus = usb . 0 , port = 1 \\ - device usb - kbd , id = input1 , bus = usb . 0 , port = 2 \\ - device virtio - gpu - pci , id = video0 , bus = pci . 3 , addr = 0x4 \\ - device virtio - balloon - pci , id = balloon0 , bus = pci . 3 , addr = 0x3 \\ - vnc : 99 \\ - monitor stdio 使用qemu monitor命令执行内存热插，这里为虚拟机添加1G内存： (qemu) object_add memory-backend-ram,id=mem1,size=1G (qemu) device_add pc-dimm,id=dimm1,memdev=mem1 然后到Guest OS内部去执行 free -g 或者 numactl -H 查询一下（注意Guest OS内核版本要支持memhp）， 应该已经生效了，但这个时候内存应该默认还没自动上线。 手动上线： for i in ` grep -l offline /sys/devices/system/memory/memory*/state ` do echo online > $i done udev设备自动上线。编辑udev rules创建文件 /etc/udev/rules.d/99-hotplug-memory.rules # automatically online hot-plugged memory ACTION==\"add\", SUBSYSTEM==\"memory\", ATTR{state}=\"online\" 2. CPU Hotplug 特性 QEMU在x86上很早就支持了CPU hotplug，但在aarch64上目前upstream还没有支持。 值得高兴地是，在openEuler上提前为我们带来了这个特性， 感谢HUAWEI Hisilicon和openEuler virtualization team的努力！ CPU Hotplug主要涉及的点有： MADT(Multiple APIC Description Table)表构构建 构建PPTT( Processor Properties Topology Table)表 扩展GED，支持CPU Hotplug Event 3. Reference Linux Kernel Memory Hotplug","tags":"virtualization"},{"title":"A Qemu Hang Problem Revelant to AIO","url":"https://kernelgo.org/qemu-aio-internal.html","loc":"https://kernelgo.org/qemu-aio-internal.html","text":"最近openEuler在HDC 2020上宣布开源了，采用开源社区的运作模式开发和发布版本。 openEuler对Huawei鲲鹏系列CPU（基于ARM架构）提供了良好的支持， 开始陆续被国内私有云厂商开始采用，希望openEuler能够成长起来成OS业界的明星。 在前期我们的测试环境中发现在使用qemu-img做镜像格式转换的时候偶然会发现 qemu主线程会出现hung的情况。奇怪的是这个现在只在aarch64平台上出现， 在x86平台上我们没有复现出来，当时也没太在意但后来被平安科技的技术人员测出来了反馈给我们开始引起了我们的注意。 通过我们的分析，再加上和社区的讨论发现这个问题其实是 隐藏在qemu AIO机制中的一个典型smp多线程共享变量直接的同步问题， 究其根因是一个不同体系结构上内存模型不同从而导致代码行为不一致的问题。 （注：一开始我们以为是编译器优化导致局部指令被重排了，后来发现并不是） 通过对这个问题的分析和探讨，可以有助于进一步理解了smp架构上内存模型和其的重要性。 这里分析和记录一下这个问题。 问题复现方法很简单，采用下面的测试命令即可： COUNT = 0 while true do qemu-img convert -f qcow2 origin.qcow2 -O output.qcow2 COUNT = $( expr $COUNT + 1 ) echo \"do image convert times = $COUNT \" done 我们向qemu社区提交了bugzilla，但qemu社区也没有就这个问题达成一个结论， 社区讨讨论连接是： https://lists.gnu.org/archive/html/qemu-arm/2019-10/msg00051.html 使用gdb调试出错现场发现IO请求已经完成（THREAD_DONE）， 但主线程却没有感知到，从而导致主线程一直处poll循环监听pipe事件中(AIO完成事件)， 造成了主线程hang住的假象，没有办法正常退出了。 ( gdb ) p * ( ThreadPoolElement * ) 0xaaab002707e0 $ 6 = { common = { aiocb_info = 0xaaaacfd6e250 < thread_pool_aiocb ) info > , bs = 0x0 , cb = 0xaaaacfcd128 < thread_pool_co_cb > , opaqueue = 0xffff90cef828 , refcnt = 1 , pool = 0xaaab002a4000 , func = 0xaaaacfc607e4 < aio_worker > , state = THREAD_DONE , } } 要定位这个文问题，必须先分析一下qemu aio机制的执行逻辑。 qemu aio基于glib2的mainloop事件循环机制， mainloop主要分为prepare, polling, check, disptch四个阶段状态，状态机如下图： Step1：首先是prepare阶段， 当上一次IO处理完成之后，qemu主线程会执行aio_ctx_prepare进入prepare阶段， 在prepare阶段会先把 ctx->notify_me 置1（表示我下面可能要睡眠了，有IO完成了请主动通知我）， 然后通过aio_compute_timeout计算下次poll的超时时间。 该函数会去检查所有的bottom half（包括定时器和监听的file descriptor） 综合判断是否有已经下发的IO（通过bh->flags BH_SCHEDULED的值来确定），是否有定时器事件到来， 是否有fd状态改变（可读、可写）从而算出一个timeout时间。 若有需要立刻下发的IO直接返回0，表示下次poll的超时时间为0，即不管eventfd的pipe中是否有事件都会直接退出polling阶段， 立刻dispatch一次（调用aio_ctx_check和dispatch阶段去下发IO）。 若有bh被schedule（flags|BH_SCHEDULED）但这是个idle任务则休眠10ms再poll。 若没有IO需要处理，则返回timeout的值为-1，这时候主线程会一直poll等待eventfd通知到来。 Step2：prepare后进入poll阶段 poll阶段会监听glib_pollfds_fill添加的那些fd，一直等到fd状态发生变化（可读、可写） 或者prepare阶段设定的超时时间到达就进入到check阶段。 Step3：check阶段调用 aio_ctx_check 将ctx->notify_me置成0， 并清除ctx->notifier标志，然后遍历整个bh列表如果发现有bh的flags|BH_SCHEDULED返回true， 或者aio_pending满足、定时器超时满足也返回true，表示可以下发一次IO执行操作了。 Step4：dispatch阶段给IO worker线程下发IO操作， 执行具体任务的函数是aio_dispatch。 这个阶段会调用bh的callback，执行最终的IO操作。 整个AIO的处理流程大概可以用下面的UML活动图表示： 针对这个问题我们要重点看下 ctx_aio_prepare 函数和 qemu_bh_schedule 这两个函数。 static gboolean aio_ctx_prepare ( GSource * source , gint * timeout ) { AioContext * ctx = ( AioContext * ) source ; atomic_or ( & ctx -> notify_me , 1 ); /* We assume there is no timeout already supplied */ * timeout = qemu_timeout_ns_to_ms ( aio_compute_timeout ( ctx )); if ( aio_prepare ( ctx )) { * timeout = 0 ; } return * timeout == 0 ; } int64_t aio_compute_timeout ( AioContext * ctx ) { BHListSlice * s ; int64_t deadline ; int timeout = -1 ; timeout = aio_compute_bh_timeout ( & ctx -> bh_list , timeout ); if ( timeout == 0 ) { return 0 ; } QSIMPLEQ_FOREACH ( s , & ctx -> bh_slice_list , next ) { timeout = aio_compute_bh_timeout ( & s -> bh_list , timeout ); if ( timeout == 0 ) { return 0 ; } } deadline = timerlistgroup_deadline_ns ( & ctx -> tlg ); if ( deadline == 0 ) { return 0 ; } else { return qemu_soonest_timeout ( timeout , deadline ); } } static int64_t aio_compute_bh_timeout ( BHList * head , int timeout ) { QEMUBH * bh ; QSLIST_FOREACH_RCU ( bh , head , next ) { if (( bh -> flags & ( BH_SCHEDULED | BH_DELETED )) == BH_SCHEDULED ) { if ( bh -> flags & BH_IDLE ) { /* idle bottom halves will be polled at least * every 10ms */ timeout = 10000000 ; } else { /* non-idle bottom halves will be executed * immediately */ return 0 ; } } } return timeout ; } 前面提到prepare阶段调用ctx_aio_prepare来计算下一次polling的超时时间， 实际调用了aio_compute_timeout计算超时时间，注意： 这里会先将notify_me置1然后再计算超时时间，而需要额外注意的是notfiy_me是一个共享变量(其实ctx都是共享的)。 aio_ctx_prepare执行在主线程的上下文，所以这就涉及到共享变量的barrier同步问题。 附：aio_ctx_prepare的执行callstack为： #0 aio_ctx_prepare ( source = 0 x555555780070 , timeout = 0 x7fffffffdbf4 ) at util / async . c : 250 #1 0 x00007ffff7eb6d1a in g_main_context_prepare () from / lib64 / libglib - 2 . 0 . so . 0 #2 0 x0000555555695d4e in glib_pollfds_fill ( cur_timeout = 0 x7fffffffdcc8 ) at util / main - loop . c : 191 #3 0 x0000555555695ec0 in os_host_main_loop_wait ( timeout =- 1 ) at util / main - loop . c : 232 #4 0 x000055555569600f in main_loop_wait ( nonblocking = 0 ) at util / main - loop . c : 518 #5 0 x0000555555577e89 in convert_do_copy ( s = 0 x7fffffffdea0 ) at qemu - img . c : 2043 #6 0 x0000555555579332 in img_convert ( argc = 7 , argv = 0 x7fffffffe1d0 ) at qemu - img . c : 2555 #7 0 x000055555557f3b8 in main ( argc = 7 , argv = 0 x7fffffffe1d0 ) at qemu - img . c : 5116 void qemu_bh_schedule ( QEMUBH * bh ) { aio_bh_enqueue ( bh , BH_SCHEDULED ); } static void aio_bh_enqueue ( QEMUBH * bh , unsigned new_flags ) { AioContext * ctx = bh -> ctx ; unsigned old_flags ; /* * The memory barrier implicit in atomic_fetch_or makes sure that: * 1. idle & any writes needed by the callback are done before the * locations are read in the aio_bh_poll. * 2. ctx is loaded before the callback has a chance to execute and bh * could be freed. */ old_flags = atomic_fetch_or ( & bh -> flags , BH_PENDING | new_flags ); if ( ! ( old_flags & BH_PENDING )) { QSLIST_INSERT_HEAD_ATOMIC ( & ctx -> bh_list , bh , next ); } aio_notify ( ctx ); } void aio_notify ( AioContext * ctx ) { /* Write e.g. bh->scheduled before reading ctx->notify_me. Pairs * with atomic_or in aio_ctx_prepare or atomic_add in aio_poll. */ smp_mb (); if ( ctx -> notify_me ) { event_notifier_set ( & ctx -> notifier ); atomic_mb_set ( & ctx -> notified , true ); } } 提示：bh是一个任务推迟执行的机制，IO线程里调用aio_bh_new创建一个bh，然后在要下发IO的时候调用qemu_bh_schedule， 将BH_SCHEDULE置1告诉下发IO者这个下半部可以执行了。 下一步分析qemu_bh_schedule流程，熟悉bh机制的都应该知道qemu_bh_schedule是在IO线程上下文。 其代码中首先通过原子操作xchq将BH_SCHEDULED标志位置为1， 告诉调度者（这里是主线程）这个bh可以开始下发执行了。 若之前BH_SCHEDULED为0则调用aio_notify通知主线程， 而aio_notify()中又会判断ctx->notify_me是否为0， 只有当其不为0时候才会写ctx->event_notifier。 值得注意的是： 在这里aio_notify的时候要通过判断notify_me来决定是否 发送ctx->event_notifer事件通知给主线程。 然而这里notify_me是个多线程之间的共享变量，作为判断条件会影响代码的分支执行！ 创建8个协程： #0 qemu_bh_schedule (bh=0x555555793b60) at util/async.c:181 181 aio_bh_enqueue ( bh, BH_SCHEDULED ) ; ( gdb ) bt #0 qemu_bh_schedule (bh=0x555555793b60) at util/async.c:181 #1 0x0000555555693b48 in spawn_thread (pool=0x555555780800) at util/thread-pool.c:158 #2 0x0000555555693f4c in thread_pool_submit_aio (pool=0x555555780800, func=0x5555555f9a7d <handle_aiocb_rw>, arg=0x7ffff70949d0, cb = 0x555555693fc2 <thread_pool_co_cb>, opaque = 0x7ffff7094920 ) at util/thread-pool.c:262 #3 0x0000555555694072 in thread_pool_submit_co (pool=0x555555780800, func=0x5555555f9a7d <handle_aiocb_rw>, arg=0x7ffff70949d0) at util/thread-pool.c:288 #4 0x00005555555fab23 in raw_thread_pool_submit (bs=0x55555578d3f0, func=0x5555555f9a7d <handle_aiocb_rw>, arg=0x7ffff70949d0) at block/file-posix.c:1894 #5 0x00005555555fac23 in raw_co_prw (bs=0x55555578d3f0, offset=0, bytes=104, qiov=0x7ffff7094e00, type=1) at block/file-posix.c:1941 #6 0x00005555555fac73 in raw_co_preadv (bs=0x55555578d3f0, offset=0, bytes=104, qiov=0x7ffff7094e00, flags=0) at block/file-posix.c:1948 #7 0x00005555556072e9 in bdrv_driver_preadv (bs=0x55555578d3f0, offset=0, bytes=104, qiov=0x7ffff7094e00, qiov_offset=0, flags=0) at block/io.c:1116 #8 0x0000555555608181 in bdrv_aligned_preadv (child=0x55555577f900, req=0x7ffff7094bd0, offset=0, bytes=104, align=1, qiov=0x7ffff7094e00, qiov_offset = 0 , flags = 0 ) at block/io.c:1492 #9 0x0000555555608a73 in bdrv_co_preadv_part (child=0x55555577f900, offset=0, bytes=104, qiov=0x7ffff7094e00, qiov_offset=0, flags=0) at block/io.c:1733 #10 0x00005555556088ab in bdrv_co_preadv (child=0x55555577f900, offset=0, bytes=104, qiov=0x7ffff7094e00, flags=0) at block/io.c:1691 #11 0x0000555555606b8b in bdrv_rw_co_entry (opaque=0x7ffff7094d50) at block/io.c:908 #12 0x0000555555606c62 in bdrv_prwv_co (child=0x55555577f900, offset=0, qiov=0x7ffff7094e00, is_write=false, flags=0) at block/io.c:938 #13 0x0000555555606f6d in bdrv_preadv (child=0x55555577f900, offset=0, qiov=0x7ffff7094e00) at block/io.c:1001 #14 0x0000555555606ffa in bdrv_pread (child=0x55555577f900, offset=0, buf=0x7ffff7094ee0, bytes=104) at block/io.c:1018 #15 0x00005555555b752d in qcow2_do_open (bs=0x555555785cb0, options=0x55555578b190, flags=0, errp=0x7fffffffda00) at block/qcow2.c:1258 #16 0x00005555555b8d24 in qcow2_open_entry (opaque=0x7fffffffd970) at block/qcow2.c:1796 #17 0x00005555556b6599 in coroutine_trampoline (i0=1434005424, i1=21845) at util/coroutine-ucontext.c:115 结合场景进一步分析： qemu-img 进行covert格式转换的时候创建了8个协程来并发处理转换操作， 协程创建好之后主线程执行 main_loop_wait ，协程负责处理镜像转换的具体工作（发起AIO请求）。 结合问题调试发现，通过aio_notify()和aio_compute_timeout()中tracelog， 我们发现io thread已经完成io请求，且调用了qemu_bh_schedule设置bh->flags BH_SCHEDULED为1， 但是主线程好像并没有看到BH_SCHEDULED被置1，或者还有一种情况是主线程在并发执行aio_ctx_prepare 的时候并没有先将notify_me置1而是直接调用了aio_ctx_compute_timeout此时却计算timeout值为-1， 导致直接即后续主线程会一直处于poll等待事件的fd从而导致主线程hang住的假象。 为了验证这个猜测，我们在aio_ctx_prepare里面加了个memory barrier发现问题就没有复现了， 于是发了一个patch到社区讨论： https://lists.gnu.org/archive/html/qemu-devel/2020-04/msg00204.html 问题的讨论点聚焦在qemu用共享变量进行线程间通信，问题场景为： When using C11 atomics , non - seqcst reads and writes do not participate in the total order of seqcst operations . In util / async . c and util / aio - posix . c , in particular , the pattern that we use write ctx -> notify_me write bh -> scheduled read bh -> scheduled read ctx -> notify_me if ! bh -> scheduled , sleep if ctx -> notify_me , notify needs to use seqcst operations for both the write and the read . In general this is something that we do not want , because there can be many sources that are polled in addition to bottom halves . The alternative is to place a seqcst memory barrier between the write and the read . This also comes with a disadvantage , in that the memory barrier is implicit on strongly - ordered architectures and it wastes a few dozen clock cycles . 根据Paolo Bonzini的回复和总结，这个问题可以简化为一个smp编程场景下 多线程之间共享变量的一个同步模型： aio_ctx_prepare里面：主线程写 notify_me ，然后又读取bh->flags | BH_SCHEDULE的值来判是休眠还是处理bh； io worker线程写bh->flags的BH_SCHEDULE标志位，然后又读取 notify_me 来判断是否唤醒主线程。 Yes , that ' s what I expected too when I wrote that code. :( But Torvald Riegel explained a while ago that seq - cst accesses are actually weaker than e . g . the Linux kernel atomics [ 1 ]. The difference basically only matters if you are writing the relatively common synchronization pattern write A write B smp_mb () smp_mb () read B read A if not B then sleep if A then wake up the other side do something because you must either use memory barriers as above , or use seq - cst writes * and * reads . You cannot rely on having a memory barrier implicit in the writes . Paolo [ 1 ] https : // lists . gnu . org / archive / html / qemu - arm / 2019 - 10 / msg00051 . html 在这个模型下面，主线程和io worker线程之间的执行逻辑去取决于共享变量 notify_me 和bh->flags | BH_SCHEDULE的值，两个线程之间又各自会写对方需要的分支判断条件， 这样就存在一种竞争关系，作者在写代码的时候认为 atomic_or 或者 atomic_and 这类API函数 在执行的时候会加barrier，保证代码执行的顺序逻辑（注意是从另外一个线程的角度去看）， 但不幸的是实际上并没有。为了验证这一点，我们在aarch64环境上对 aio_ctx_prepare 进行反汇编： 从反汇编可以看出，atomic_or这里对应一条orr指令，后面并没有添加任何barrier指令(dmb或者dsb指令)， 也就是说 aio_ctx_prepare 中并不能保证smp环境下多线程之间的指令顺序执行。 atomic_or ( & ctx -> notify_me , 1 ); /* We assume there is no timeout already supplied */ * timeout = qemu_timeout_ns_to_ms ( aio_compute_timeout ( ctx )); 所以这问题的根源在于： aarch64环境上的smp多线程场景下，线程间使用了共享变量 （这里为例提升qemu aio的性能，主线程和IO线程之间使用共享变量方式进行同步， 并没有采用互斥锁、条件变量、信号量等重量级同步方式来保证） 可能会导致一些意想不到的结果，单独的用户态原子操作并不能保证smp多线程之间 代码的执行顺序，只有在代码分支条件判断的关键点显式添加memory barrier才能 保证代码的执顺序！！！ 为什么这个bug在x86上不出现而在aarch64上出现可以高概率复现？ 猜测是aarch64和x86上内存模型不同导致的结果。 （注意：编译器barriers只能够防止代码被编译器优化，并不能防止指令在执行的时候被CPU重新排列） 从ARMv8的PG文档可以看到aarch46使用的是一种weakly-ordered model of memory内存模型。 通俗的讲就是处理器实际对内存访问（load and store）的执行序列和program order不一定保持严格的一致， 处理器可以对内存访问进行reorder（可以参考： http://www.wowotech.net/armv8a_arch/__cpu_setup.html ）。 所以，很明显aarch64和x86的内存模型是很大不同的。 ARMv8A PG Chapter 13. Memory Ordering The ARMv8 architecture employs a weakly - ordered model of memory . In general terms , this means that the order of memory accesses is not required to be the same as the program order for load and store operations . The processor is able to re - order memory read operations with respect to each other . Writes may also be re - ordered ( for example , write combining ) . As a result , hardware optimizations , such as the use of cache and write buffer , function in a way that improves the performance of the processor , which means that the required bandwidth between the processor and external memory can be reduced and the long latencies associated with such external memory accesses are hidden . 这里对x86环境上的 aio_ctx_prepare 进行反汇编得到的结果是： 可以看到x86上是一条lock orl指令，查询资料x86使用Total Store Order内存模型： lock prefix guarantee that result of instruction is immediately globaly visible. 发现lock前缀是锁总线，可以保证执行操作立刻全局可见，难道这里顺带这里可以起到内存屏障的作用？ 更新一下最新进展，Paolo Bonzini已经开始发patch了，针对weakly-ordered内存模型体系结构修改了一些内容： https://patchwork.kernel.org/cover/11476375/ 还有针对这个问题的补丁已经post，不得不说社区大牛速度真是快，日常膜拜： https://patchwork.kernel.org/patch/11476383/ SMP架构下的内存模型（Memory Model）与CPU Arch和编程语言息息相关， 为了深入理解这个Topic，这里列举了一些参考文档供学习。 参考文献 ： http://www.cs.jhu.edu/~gyn/publications/memorymodels/writeup.html https://developer.arm.com/docs/den0024/a/memory-ordering https://www.csm.ornl.gov/workshops/openshmem2018/presentations/mm-openshmem2018.pdf https://github.com/qemu/qemu/blob/master/docs/devel/atomics.rst http://www.inf.ed.ac.uk/teaching/courses/pa/Notes/lecture07-sc.pdf https://www.cs.cmu.edu/afs/cs/academic/class/15740-s17/www/lectures/07-multiprocessor-consistency-cache.pdf https://people.mpi-sws.org/~viktor/wmc/c11.pdf https://www.jianshu.com/p/a26763dd9b0e","tags":"virtualization"},{"title":"Article Archive 2020 Reading Plan","url":"https://kernelgo.org/reading2020.html","loc":"https://kernelgo.org/reading2020.html","text":"2020年学习计划 一转眼2019已经过了，制定的2019年阅读计划执行并不好。2019年上半年工作上太忙碌了，压得喘不过气来，所以阅读较少。 2019年下半年有执行一部分的阅读计划，但个人时间利用率并不高，欠了很多阅读债务。 作为一个在虚拟化领域工作快4年的人了，好多特性还没有系统的学习过，真的很汗颜。 2020年伊始告别了我的台式机入手了BP16寸，可以很方便的带到公众场合学习，所以2020年一定要恶补一下相关领域的知识， 不然出去混真的混不开。这里简单的制定一下2020年的学习计划，当然计划是活动的，所以随时更新。 学习dpdk和vhost-user方案的原理和代码； 学习openvswitch的原理和基本知识； 学习一些存储方面的知识，包括但不限于spdk等基本内容； 如果有时间要学习RSIC-V相关的知识，并补齐arm64的基本原理。 继续学习和使用Rust编程语言，Rust是系统编程和基础软件开发的不二之选。 2020好好学习，天天向上，加油！ 阅读笔记摘要 1. ACPI基础知识学习 ACPI的学习资料最好的还是ACPI Spec文档，但阅读文档总是让人觉得卷帙浩繁。这次主要看了下ACPI Spec Chapter 4和ACPI Chapter 5，这两个章节分别描述了ACPI 硬件接口和软件编程模型，让我们明白ACPI标准是如何在为OS体统更好的电源管理能力的同时 又为硬件厂商提供高度的灵活性。尤其是Chapter5，非常适合OS设计人员阅读，该章节介绍了ACPI System Description Table Achitecture， 方便我们理解ACPI里面的系统描述表的具体结构和作用。 总体上，ACPI固件定义两种数据结构：data tables 和 defination blocks，data tables里提供了给设备驱动程序的元数据， defination blocks里面则包含了OS可以执行的Byte Code，defination blocks可以用一种叫做ASL语言来定义和描述。 ASL(ACPI Source language)是ACPI中用来描述硬件信息以及硬件操作给OSPM的表达式语言（相当于源代码）， 通过ASL Compiler编译后生成了AML（ACPI Machine Language）格式，不同的AML模块组合在一起构成了ACPI Firmware， 是一种与OS平台无关的格式。详情可以看ACPI Spec，也可以看这里一个关于ASL的Tutorial， 可以快速了解ASL的基本语法: https://acpica.org/sites/acpica/files/asl_tutorial_v20190625.pdf 另外还有一个文章带你快速了解ACPI的架构： https://acpica.org/sites/acpica/files/ACPI-Introduction.pdf QEMU里面的i440fx主板对应的南桥芯片组是piix4，它包含了一个IDE Controller，一个USB Controller，一个PCI-ISA bridge和 RTC, PIT, PICs等设备。其中piix4里面包含了一个PM模块（本质是一个PCI设备），这个PM设备便是芯片组支持ACPI的接口。 同理，Q35主板上我们拥有一个叫做ich9的芯片组，它除了集成了AHCI controller，USB controller, network adapter, audio adapter, PCI-E and PCI bus LPC bus for the SuperIO devices等一堆设备之外，也具备一个Power Management模块，负责ACPI规范的支持。具体可以参考：https://wiki.qemu.org/Features/Q35。 x86平台上QEMU初始化主板的最后阶段，pc_machine_done -> acpi_setup 初始化x86的ACPI子系统。 arm64平台上virt主板最后初始化阶段，virt_machine_done -> virt_acpi_setup 初始化aarch64的ACPI子系统。 ACPI提供了CPU热插拔、内存热插拔以及pci/pcie 热插拔相关接口给操作系统。 2.0 QEMU FwCfg机制 QEMU使用fw_cfg设备向OS呈现系统固件(firmware)信息， 其中主要包括：device bootorder，ACPI和SMBIOS tables，VM UUID，SMP/NUMA， kenerl/initrd images for direct (Linux) kernel booting等（fw_cfg直接把硬件信息报告给OS）。 fw_cfg设备对外提供了3个关键寄存器用来完成设备操作，x86上使用PIO方式，arm上使用MMIO方式访问寄存器。 === x86, x86_64 Register Locations === Selector Register IOport: 0x510 Data Register IOport: 0x511 DMA Address IOport: 0x514 === ARM Register Locations === Selector Register address: Base + 8 (2 bytes) Data Register address: Base + 0 (8 bytes) DMA Address address: Base + 16 (8 bytes) Select Register是Write Only的，通过写这个寄存器来选择要操作的Firmware。 Data Register是WR的，通过读Data Register内容可以将Select Register选中的固件内容按Byte读取出来。 DMA Address寄存器为fw_cfg提供了DMA方式访问固件的能力，相对于按Byte读取固件，可以提升固件读取的速率。 除此之外，fw_cfg还提供了ACPI接口给OS访问固件使用，其ACPI描述表信息中包含了固件目录信息（FW_CFG_FILE_DIR）， 具体格式可以参考一下： fw_cfg ACPI数据结构 除了能由ACPI，DTB进行fw_cfg实例化之外，也可以直接配置内核引导参数使用fw_cfg， 很显然这种方法是为了适用于QEMU直接引导内核的场景。 * The fw_cfg device may be instantiated via either an ACPI node ( on x86 * and select subsets of aarch64 ), a Device Tree node ( on arm ), or using * a kernel module ( or command line ) parameter with the following syntax : * * [ qemu_fw_cfg .] ioport =< size > @ < base > [ :< ctrl_off >:< data_off > [ :< dma_off > ]] * or * [ qemu_fw_cfg .] mmio =< size > @ < base > [ :< ctrl_off >:< data_off > [ :< dma_off > ]] * * where : * < size > := size of ioport or mmio range * < base > := physical base address of ioport or mmio range * < ctrl_off > := ( optional ) offset of control register * < data_off > := ( optional ) offset of data register * < dma_off > := ( optional ) offset of dma register * * e . g . : * qemu_fw_cfg . ioport = 12 @0x510 : 0 : 1 : 4 ( the default on x86 ) * or * qemu_fw_cfg . mmio = 16 @0x9020000 : 8 : 0 : 16 ( the default on arm ) */ 这里可以参考： https://wiki.osdev.org/QEMU_fw_cfg https://git.qemu.org/?p=qemu.git;a=blob;f=docs/specs/fw_cfg.txt https://github.com/torvalds/linux/blob/master/drivers/firmware/qemu_fw_cfg.c https://www.kernel.org/doc/Documentation/devicetree/bindings/arm/fw-cfg.txt 3. FDT、ACPI和SMBIOS的区别与联系 TODO SMBIOS Mappings https://gist.github.com/smoser/290f74c256c89cb3f3bd434a27b9f64c 4. QEMU Direct Boot引导内核 TODO glue load_elf elf_ops.h 5.内核大锁的讨论 https://kernelnewbies.org/BigKernelLock 6. PVH Boot引导方式 https://patchwork.kernel.org/cover/10722233/ PVH特性的目的是为了让QEMU支持内核的快速启动，可以跳过固件直接引导一个未经压缩的内核， 听上去是不是有点酷。在KVM之前，Xen上就已经有支持Linux和FreeBSD的PVH guests和ABI。 For a Microkernel, a BIG Lock is fine ! 阅读清单 perf event原理和使用方法 perf 原理和代码分析","tags":"utils"},{"title":"Live Migration with Passthroughed Device (Draft)","url":"https://kernelgo.org/live-migration-with-passthroughed-device.html","loc":"https://kernelgo.org/live-migration-with-passthroughed-device.html","text":"异构计算已经成为当前云计算的一个主流应用场景， 各种硬件加速器例如：GPGPU，FPGA，ASIC等都被用在异构计算场景来提供额外的算力加速。 为了将算力以更低的成本提供给客户，需要设计专门的加速器虚拟化方案。 就GPU虚拟化而言，目前比较著名的方案有： Nvidia的GRID vGPU虚拟化方案（支持将一个物理GPU虚拟出多个vGPU实例提供给不同的虚拟机使用）， AMD的MxGPU方案(AMD FirePro系列显卡，走到是SRIOV方案，被AWS AppStream2采用)， 以及Intel的GVT-g方案（Intel 集成显卡，目前有一个demo）。 异构计算场景基本上都是用设备直通的方式将设备呈现给虚拟机， 而直通设备对于虚拟机热迁移不太友好，阻碍了热迁移的应用。 0. Live Migration With Passthourghed Device 下图是一个典型的Pre-Copy场景热迁移基本流程图: +----------+ +---------------+ +-----------------+ | Pre - copy | | Stop and Copy | |Resume/Post - Copy | +----------+ +---------------+ +-----------------+ | | Service Downtime | | | | < ------------------ > | | | | | < ---------------- Total Migration Time ---------------- > | 为了获得最优的IO性能，在公有云或者企业虚拟化场景下，会选择设备直通的方式将硬件设备直接呈现给虚拟机使用。 但直通带来的较大问题是对虚拟机的热迁移和热升级支持都非常不友好， 现阶段并没有一个很好的统一的框架能够完成直通虚拟机热迁移任务，其主要原因有以下几点： 挑战1：当前很多物理硬件设备还不持支持设备状态的暂停和运行（Pause & Resume） 在热迁移vCPU停机之后，设备端可能还存在已提交但尚未处理完成的任务，任务的结果还没写回处于一个中间的不确定状态。 很显然，这个时间点是不适合做设备状态快照的，因为状态都是中间态。 所以要么硬件设备在硬件层面就支持暂停、恢复，或者就要等到设备正在执行的任务执行完成，进入空闲状态。 挑战2：当前很多物理硬件设备还不支持硬件状态的保存和恢复（Save & Restore） 在一个典型的Pre-Copy迁移场景，当算法判断脏页已经收敛，最后一轮脏页拷贝后进入Checkpoint点， 源端需要将硬件\"设备状态\"保存到数据结构中再发送到目的端，目的端需要从接收到的数据结构中恢复出源端设备状态。 由于硬件设计缺陷，部分legacy设备的寄存器可能存在只读、只写的情况，这就导致状态无法100%复制， 而且由于一些软件框架上的设计缺陷，很多硬件设别不支持\"上下文切换\"（包括MMIO等，DMA buffer等资源）， 这也最终会导致硬件状态无法从源端拷贝到目的端。 挑战3：当前几乎所有的硬件平台还不支持DMA脏页跟踪 在Pre-Copy阶段，物理设备发起的DMA操作是通过IOMMU/SMMU页表进行翻译的， 然而当下IOMMU/SMMU的DMA操作还不持支持脏页跟踪，也就是页表并没有Acces Bit和Dirty Bit等硬件支持。 缺乏DMA脏页硬件标脏，会导致很大的问题，以为在内存迭代拷贝阶段， 设备会通过DMA发起写虚拟机物理内存操作，而我们在查询脏页的时候又无法察觉哪些页被写了， 这就导致这部分脏页无法同步过去，无法保证源端和目的端内存的一致性，最终会导致虚拟机异常。 (备注：从Intel最新的VT-d spec来看，新的Intel平台上的IOMMU已经开始支持IOMMU页表Access Bit和Dirty Bit， 估计是Intel已经看到了DMA脏页硬件标脏的重要性，但目前该功能在哪一代CPU上开始支持还不明确，至少截至目前还是不支持的) 上述几个难点，导致直通虚拟机在热迁移和热升级场景成为特例，比较难以处理。 如何搞定直通热迁移，目前应该是业界比较热点的一个努力方向。 从目前来看，直通设备热迁移方案就两种，即mdev方案和SRIOV方案， 对于mdev方案本文选取了Intel GVT-g热迁移方案作为介绍， 对于SRIOV方案本文选取了Intel X710网卡的直通热迁移方案作为介绍。 1. Intel GVT-g 直通热迁移方案 Intel GVT-g是业界第一个GPU全虚拟化方案，也是目前唯一开源（Nvidia不开源）且较为稳定的GPU全虚拟化方案。 Intel GVT-g基于mdev实现了受控的直通，在具备良好性能的同时保证了可扩展性和隔离性。 在Intel GVT-g中Intel GPU的GGTT和PGTT都通过影子页表方式实现，GGTT页表则在虚拟机之间通过切换分时共享。 GVT-g中还实现了一个 指令扫描器 来扫描所有GGTT中缓存中的GPU指令，确保不会访问非法地址。 同时，最牛叉的地方在于Intel GVT-g中实现了GPU调度和CPU调度相互分离机制， 这是一种时间片轮转和按需调度相互结合的方式。 不得不说，Intel还是掌握了核心科技！ 为了分析Intel GVT-g直通热迁移方案，我们分析了一下整个Intel GVT-g框架的流程。 Intel GVT-g要去硬件平台是5代酷睿以上（桌面版）和至强系列（服务器）V4以上的芯片架构。 首先，简单看下Intel显卡驱动加载流程，这里是一堆框架相关的初始化操作，驱动加载的入口是 i915_driver_hw_probe ： i915_driver_hw_probe 显卡驱动加载入口 --> intel_device_info_runtime_init --> i915_ggtt_probe_hw 初始化 GGTT相关信息 ，包括 ggtt基地址 ，大小等信息 --> gen8_gmch_probe ggtt存放在BAR2里面 --> i915_kick_out_firmware_fb 没看太懂，好像是是删除了 firmware的framebuffer --> vga_remove_vgacon 这里将默认的 vga console给关掉了 --> i915_ggtt_init_hw --> ggtt_init_hw ( i915_address_space_init , arch_phys_wc_add ) --> intel_gvt_init 初始化 GVT 组件， drm_i915_private --> intel_gvt_init_device --> init_device_info --> intel_gvt_setup_mmio_info --> intel_gvt_init_engine_mmio_context --> intel_gvt_load_firmware 加载固件 --> intel_gvt_init_irq 初始化 GVT - g 中断模拟子系统，注意： vGPU的中断是模拟的 ？ --> intel_gvt_init_gtt 初始化全局地址图形地址翻译表 --> intel_gvt_init_workload_scheduler 调度器初始化，注意 GPU有多个渲染引擎 ，都支持上下文切换 --> intel_gvt_init_sched_policy 调度器初始化 --> intel_gvt_init_cmd_parser 这个不简单，初始化 GPU渲染器的命令解析器 （有点复杂，要好好看看） --> init_service_thread --> intel_gvt_init_vgpu_types // 初始化vGPU类型，根据SKU芯片资源不同，最多有4种规格的vGPU，最多支持8个vGPU --> intel_gvt_init_vgpu_type_groups 创建不同 vGPU Type的属性组 --> intel_gvt_create_idle_vgpu 空闲的 vGPU实例 --> intel_gvt_debugfs_init create debugfs ，方便调试 --> intel_opregion_setup 没看懂这里啥意思？ 从上面的分析我们可以看到，Intel GPU初始化的时候获取了GGTT的相关信息并对GGTT进行了初始化， 然后初始化了vGPU相关的东东，其中包括设置MMIO，初始化MMIO上下文，初始化vGPU的模拟中断， 初始化vGPU的GGTT，初始化调度器和参数，初始化cmdparser（GPU指令解析器），初始化server线程， 初始化vGPU类型信息，初始化了一个idle状态的的vCPU实例，以及调试相关的debugfs。 接着来看下Intel vGPU的mdev设备模型，我们知道mdev设备模型中要求提供Userspace API接口， mdev_parent_ops 作为mdev sysfs回调，负责mdev实例的创建和销毁等操作，是vGPU对用户态的接口。 加载kvmgt.ko的时候，会调用 kvmgt_host_init 注册设备。 static struct mdev_parent_ops intel_vgpu_ops = { . mdev_attr_groups = intel_vgpu_groups , . create = intel_vgpu_create , . remove = intel_vgpu_remove , . open = intel_vgpu_open , . release = intel_vgpu_release , . read = intel_vgpu_read , . write = intel_vgpu_write , . mmap = intel_vgpu_mmap , . ioctl = intel_vgpu_ioctl , }; insmod kvmgt . ko kvmgt_init --> intel_gvt_register_hypervisor --> intel_gvt_hypervisor_host_init --> kvmgt_host_init --> mdev_register_device ( dev , & intel_vgpu_ops ) 通过sysfs创建vCPU的mdev实例，用户态程序echo \" \\(UUID\" > /sys/class/mdev_bus/\\) type_id/create里面， 这里会调用到 intel_vgpu_create 来创建一个vGPU实例。 intel_vgpu_create --> intel_gvt_ops -> gvt_find_vgpu_type -> intel_gvt_find_vgpu_type 根据 echo进来的字符串 ，找到对应的 vGPU Type --> intel_gvt_ops -> vgpu_create -> intel_gvt_create_vgpu （创建 intel_vgpu实例 ） --> mdev_set_drvdata ( mdev , vgpu ) // 将driver_data赋值为创建的intel_vgpu对象 重点分析 intel_gvt_create_vgpu intel_gvt_create_vgpu --> vgpu = vzalloc ( sizeof ( * vgpu )) // 分配一个vGPU结构体 --> vgpu -> id = idr_alloc () // 分配一个唯一的id --> intel_vgpu_init_cfg_space // 初始化配置空间 --> intel_vgpu_init_mmio // 初始化MMIO，intel_gvt_init_device里面给mmio_size = 2MB，给MMIO赋默认值 --> intel_vgpu_alloc_resource // 好复杂的，vGPU各种资源分配 --> intel_vgpu_init_display --> intel_vgpu_setup_submission --> intel_vgpu_init_sched_policy --> intel_gvt_hypervisor_set_opregion 2.0 SRIOV直通热迁移框架 https://patchwork.kernel.org/cover/11274097/ TO BE CONTINUED Refs Intel GVT-g Reference https://patchwork.kernel.org/cover/11274097/","tags":"virtualization"},{"title":"VFIO Mediated Devices Introduction","url":"https://kernelgo.org/vfio-mdev.html","loc":"https://kernelgo.org/vfio-mdev.html","text":"0. VFIO Mdev Introduction VFIO框架解决了设备直通的问题，使得用户态可以直接使用设备的DMA能力。 VFIO使用IOMMU页表来对设备访问进行保护，将用户态对设备的访问限定在一个安全的域内。 利用这个框架，我们可以将GPU、网卡适配器、一些计算加速器（例如：AI芯片，FPGA加速器）直接呈现给虚拟机。 VFIO直通的缺点是单个PCIe设备只能直通给一台虚拟机,这样设备的利用率较低. 为了能够提升设备资源的利用率就出现了SRIOV（Single Root I/O Virtualization）技术。 SRIOV技术就像孙悟空一样，从本体是上拔出一根猴毛一吹，生成了多个分身，每个分身都有独自的战斗力。 SRIOV技术将设备的本体称为PF（Physical Function），可以从PF上生成多个VF（Virtual Function）, 多个VF之间使用PCIe ACS(Access Control Services)进行隔离，确保互相之间不会访问到对方的数据以确保安全。 那么对于一些不具备SR_IOV能力的设备，如何来做到同时向多个用户态进程提供服务呢？ Nvidia提出了基于VFIO的 VFIO Mdev框架 来解决这个问题。 在mdev模型的核心在于mdev会对硬件设备的状态进行抽象，将硬件设备的\"状态\"保存在mdev device数据结构中， 设备驱动层面要求实现一个调度器，将多个mdev设备在硬件设备上进行调度（分时复用）， 从而实现把一个物理硬件设备分享给多个虚拟机实例进行使用。 1. VFIO mdev框架Overview 下图是vfio mdev框架的示意图，这个框架对外提供了一组管理接口，其中包括： 创建和销毁一个mdev 从mdev bus driver上添加和删除一个mdev设备 从IOMMU group上增加和删除一个mdev设备 从框架来看Top Level是mdev bus driver，是为mdev虚拟的总线类型驱动， 这里我们可以绑定到vfio上，于是就有了vfio mdev，不过目前好像就只设计了vfio mdev。 可以通过调用 mdev_register_driver 向mdev bus添加 vfio_device_ops 的驱动。 下面的一层是和物理设备相关的接口，可以调用 mdev_register_device 来注册某一个物理设备到mdev上， mdev_register_device 传入了 mdev_parent_ops 参数， mdev_parent_ops 是跟物理设备相关的驱动集合，例如它可以是nvidia的GPU设备驱动(nvidia.ko)的callback， 可以是Intel集成显卡(i915.ko)的callback，还可以是s390设备驱动(ccw_device.ko)的callback。 mdev在vfio的基础上通过增加一个mdev bus，然后在这个bus下挂载mdev设备， 通过mdev的抽象，将和物理设备相关的驱动实现细节做了剥离，不同的厂商利用mdev框架去 实现自家设备的vendor driver hook（可以是开源的，可以是闭源的）。 Nvidia就是利用这个mdev框架去设计了Nvidia vGPU设备驱动，将自己的驱动实现细节做了闭源，这是Nvidia常用的套路。 2. VFIO mdev框架代码分析 从代码框架来看，vfio mdev首先提供了2个关键的注册接口， mdev_register_driver 和 mdev_register_device 。 先看mdev bus driver ，它提供了一个 mdev_register_driver 接口api来注册一个新的mdev driver， 在 vfio_mdev_init 的时候注册了 mdev_driver 类型的 vfio_mdev_driver 。 struct bus_type mdev_bus_type = { . name = \"mdev\" , . probe = mdev_probe , //设备加载 . remove = mdev_remove , //设备卸载 }; EXPORT_SYMBOL_GPL ( mdev_bus_type ); ( mdev . ko ) mdev_init -> mdev_bus_register -> bus_register ( & mdev_bus_type ); //将mdev_bus注册到系统上 struct mdev_driver { const char * name ; int ( * probe )( struct device * dev ); void ( * remove )( struct device * dev ); struct device_driver driver ; //定义结构体的时候，加上struct device_driver driver就表示driver类似 }; int mdev_register_driver ( struct mdev_driver * drv , struct module * owner ) { /* initialize common driver fields */ drv -> driver . name = drv -> name ; drv -> driver . bus = & mdev_bus_type ; //driver的bus指定为mdev_bus drv -> driver . owner = owner ; /* register with core */ return driver_register ( & drv -> driver ); //注册设备驱动 } EXPORT_SYMBOL ( mdev_register_driver ); static struct mdev_driver vfio_mdev_driver = { . name = \"vfio_mdev\" , . probe = vfio_mdev_probe , . remove = vfio_mdev_remove , }; static int __init vfio_mdev_init ( void ) { return mdev_register_driver ( & vfio_mdev_driver , THIS_MODULE ); } 当mdev设备在加载的时候，mdev bus driver负责将设备绑定到指定的iommu group上，具体流程是： mdev create -> mdev_probe -> mdev_attach_iommu, drv->probe(dev) -> vfio_mdev_probe -> vfio_add_group_dev mdev_probe 的时候会创建一个iommu group，然后将设备添加到这个iommu group， 再调用 vfio_mdev_probe 将设备添加到一个vfio group当中。 再看下mdev device driver ，这里提供了 mdev_register_device api来注册一个新的物理设备驱动（Physical Device Driver Interface），注意这儿有个概念叫做 parent_device ，其实指的就是一个特定的物理设备，而mdev device则是物理设备的一个\"实例\"。 struct mdev_parent_ops { struct module * owner ; const struct attribute_group ** dev_attr_groups ; const struct attribute_group ** mdev_attr_groups ; struct attribute_group ** supported_type_groups ; int ( * create )( struct kobject * kobj , struct mdev_device * mdev ); int ( * remove )( struct mdev_device * mdev ); int ( * open )( struct mdev_device * mdev ); void ( * release )( struct mdev_device * mdev ); ssize_t ( * read )( struct mdev_device * mdev , char __user * buf , size_t count , loff_t * ppos ); ssize_t ( * write )( struct mdev_device * mdev , const char __user * buf , size_t count , loff_t * ppos ); long ( * ioctl )( struct mdev_device * mdev , unsigned int cmd , unsigned long arg ); int ( * mmap )( struct mdev_device * mdev , struct vm_area_struct * vma ); }; mdev_register_device 所做的事情是创建一个 parent device 并对其进行初始化 (很奇怪，取名为 mdev_register_parent_device 似乎更合适啊）。 mdev parent 设备创建的时候，会在sysfs路径 /sys/class/mdev_bus/ 下面创建一堆目录结构， 用户态通过sysfs接口可以完成mdev设备创建，删除，查询等操作。 /* * mdev_register_device : Register a device * @dev: device structure representing parent device. * @ops: Parent device operation structure to be registered. * * Add device to list of registered parent devices. * Returns a negative value on error, otherwise 0. */ int mdev_register_device ( struct device * dev , const struct mdev_parent_ops * ops ) { struct mdev_parent * parent ; // 创建 mdev_parent设备 parent = kzalloc ( sizeof ( * parent ), GFP_KERNEL ); // 初始化mdev parent设备 kref_init ( & parent -> ref ); init_rwsem ( & parent -> unreg_sem ); parent -> dev = dev ; parent -> ops = ops ; // 给mdev_parent_ops赋值 // 创建sysfs接口 ret = parent_create_sysfs_files ( parent ); } struct mdev_parent_ops { struct module * owner ; const struct attribute_group ** dev_attr_groups ; const struct attribute_group ** mdev_attr_groups ; struct attribute_group ** supported_type_groups ; int ( * create )( struct kobject * kobj , struct mdev_device * mdev ); int ( * remove )( struct mdev_device * mdev ); int ( * open )( struct mdev_device * mdev ); void ( * release )( struct mdev_device * mdev ); ssize_t ( * read )( struct mdev_device * mdev , char __user * buf , size_t count , loff_t * ppos ); ssize_t ( * write )( struct mdev_device * mdev , const char __user * buf , size_t count , loff_t * ppos ); long ( * ioctl )( struct mdev_device * mdev , unsigned int cmd , unsigned long arg ); int ( * mmap )( struct mdev_device * mdev , struct vm_area_struct * vma ); }; mdev_parent_ops 定义了一些属性，这些属性给QEMU/Libvirt等管理接口提供了一些查询和配置单个mdev设备实例的接口， 例如： dev_attr_groups , mdev_attr_groups 和 supported_config 等。 dev_attr_groups: attributes of the parent device mdev_attr_groups: attributes of the mediated device supported_config: attributes to define supported configuration Nvidia vGPU方案中使用mdev框架对GPU进行分时复用，可以将一个GPU实例化成多个mdev vGPU设备， 然后将不同的vGPU直通给不同的虚拟机，提升了GPU的使用效率，降低了GPU实例的成本。 同时，mdev的作者为mdev框架提供了一些示例程序，阅读一个示例程序对我们理解mdev的工作原理有一定的帮助。 3. mdev based hardware virtio offloading https://patchwork.freedesktop.org/series/66987/ 这个是最近Jason Wang在搞的一个东东，目的是为了搞一个基于mdev的virtio卸载技术方案， 这个方案还是有点意思。我们知道virtio半虚拟化通过前后端内存共享的方式，减少了IO过程中的数据拷贝流程， 提升了模拟设备的性能，virtio和mdev结合则可以进一步提升性能，因为我们可以直接将virtio virtqueue里面的 io请求地址提供硬件，让硬件直接通过DMA从virtqueue里面做IO请求，是不是很赞？ 简单地分析了一下他的这组补丁，补丁为mdev device新增了一个 class id 的概念（设备在创建的时候要指定自己的class id）， 通过这个 class id 去区分设备类，然后还向mdev device里面塞进了一个 vfio_mdev_device_ops 指针， 这使得vfio-mdev设备和virtio-mdev设备可以拥有不同的callback函数组。 为此还对 mdev_parent_ops 做了改动，将设备驱动callback单独挪到了 vfio_mdev_device_ops 里面， 这样以后 mdev_parent_ops 专职负责mdev device的创建、销毁和sysfs接口处理了。 struct mdev_device { struct device dev ; struct mdev_parent * parent ; guid_t uuid ; void * driver_data ; struct list_head next ; struct kobject * type_kobj ; struct device * iommu_device ; bool active ; + u16 class_id ; + union { + const struct vfio_mdev_device_ops * vfio_ops ; + }; }; + static const struct vfio_mdev_device_ops vfio_mdev_ops = { + . open = vfio_ap_mdev_open , + . release = vfio_ap_mdev_release , + . ioctl = vfio_ap_mdev_ioctl , + }; static const struct mdev_parent_ops vfio_ap_matrix_ops = { . owner = THIS_MODULE , . supported_type_groups = vfio_ap_mdev_type_groups , . mdev_attr_groups = vfio_ap_mdev_attr_groups , . create = vfio_ap_mdev_create , . remove = vfio_ap_mdev_remove , - . open = vfio_ap_mdev_open , - . release = vfio_ap_mdev_release , - . ioctl = vfio_ap_mdev_ioctl , }; 4. Refs VGPU ON KVM VFIO BASED MEDIATED DEVICE FRAMEWORK Virtual Function I/O (VFIO) Mediated device Mediated Device For Ethernet 持续助力数据中心虚拟化：KVM里的虚拟GPU Add migration support for VFIO device","tags":"virtualization"},{"title":"Discuss On X86 APIC Virtualization","url":"https://kernelgo.org/x86_apicv.html","loc":"https://kernelgo.org/x86_apicv.html","text":"关于x86的LAPIC虚拟化，这里再记录讨论讨论。 APIC-Access Page 和 Virtual-APIC Page的区别是啥？ VT-x Posted Interrupt的实现原理是什么？（可以对比一下VT-d Posted Interrupt） 0. APIC Virtualizaiton 原理 在看LAPIC虚拟化章节(Intel SDM Chatper 29)的时候，发现和LAPIC虚拟化相关的有2个重要的Physcal Page， 即APIC-Access Page和Virtual-APIC Page，那么这两个物理页在vCPU APIC虚拟化中的作用是什么？ 在回答这个问题之前，需要先了解一下APIC的基本知识。 在x86上，APIC寄存器在系统初始化的时候默认被映射到在0xFEE00000H为起始位置的一个4K物理页上。 在MP系统上，APIC寄存器被映射到物理地址空间上，软件可以选择改变APIC寄存器的页基地址， 每个逻辑CPU可以选择将自己的APIC寄存器reload到自己设定的地方。 The Pentium 4 , Intel Xeon , and P6 family processors permit the starting address of the APIC registers to be relocated from FEE00000H to another physical address by modifying the value in the 24 - bit base address field of the IA32_APIC_BASE MSR . This extension of the APIC architecture is provided to help resolve conflicts with memory maps of existing systems and to allow individual processors in an MP system to map their APIC registers to different locations in physical memory . 在X86上软件程序可以通过 3种方式 访问逻辑CPU的LAPIC寄存器： 如果APIC工作在xAPIC模式下，可以通过MMIO方式访问基地的为IA32_APIC_BASE MSR的一个4-KByte物理页方式访问。 如果APIC工作在x2APIC模式下，可以通过使用RDMSR和WRMSR指令访问APIC寄存器。 在64-bit模式下，可以通过使用MOV CR8指令访问APIC的TPR寄存器（Task Priority Register）。 那么在虚拟化场景下我们也要支持上面的3种访问模式，为此VMCS提供了若干个APIC虚拟化相关的控制域： APIC-Access address ：如果\"virtualize APIC accessess\"控制域为1， 那么VMCS会使用一个4-KByte的物理页(APIC-access page)来辅助APIC虚拟化， 当vCPU访问这个page的时候会产生VM exits。 注意：只有处理器支持设置\"virtualize APIC accessess\"特性时APIC-access page才会存在。 Virtual-APIC address ：处理器使用这个物理虚拟化 某些 APIC寄存器和管理虚拟中断。virtual-APIC page可以通过下面的操作访问: MOV CR8指令； 访问APIC-access页，前提是\"virtualize APIC accesses\"是enable的； 使用RDMSR和WRMSR指令，前提是ECX范围是800H-8FFH（表示APIC MSR范围）并且\"virtualize x2APIC mode\"模式是enable的。 注意：只有\"use TPR shadow\"使能的条件下才会存在\"virtual-APIC page\", \"virtual-APIC page\"存在的唯一目的是为了对TPR寄存器进行shadow。 TPR Threshold ： 这个域的Bits 3:0 定义了VTPR的Bits 7:4 fall上限。 如果\"virtual-interrupt delivery\"为0，那么VM exit（例如：mov CR8）会减小TPR threshold的值。 只有处理器使能了\"use TPR shadow\"，TPR Threshold才会存在。 EOI-exit bitmap ：这个域用来控制哪些操作写APIC EOI寄存器后触发VM exits： EOI_EXIT0：包含bit掩码控制vector0(bit0)-vector63(bit63)的EOI； EOI_EXIT1：包含bit掩码控制vector64(bit64)-vector127(bit127)的EOI； EOI-EXIT2: 包含bit掩码控制vector128(bit128)-vector191(bit191)的EOI； EOI-EXIT3: 包含bit掩码控制vector192(bit192)-vector255(bit255)的EOI。 Posted-interrupt notification vector : 对于支持VT-x Posted Interrupt的处理器， 该域的lower 8bit包含了一个用来通知vCPU中断已投递的中断向量。 Posted-interrupt descriptor address : 对于支持VT-x Posted Interrupt的处理器， 该域包含了vCPU的Posted Interrupt Descriptor数据结构的物理地址。 看到这里就开始觉得：APIC的虚拟化也是挺复杂的，VMCS提供了非常细粒度的控制策略来辅助APIC的虚拟化工作。 SDM Chapter 29将APIC虚拟化的要点总结了一下： virtual-interrupt delivery : VMM直接写Virtual-APIC Page的VIRR寄存器就能给vCPU投递中断。 use TPR shadow ：主要是shadow TPR寄存器，当guest执行mov CR8给TPR赋值时，TPR寄存器的值会自动映射到Virtual-APIC Page上。 virtualize APIC accessess ：vCPU可以通过MMIO方式访问APIC寄存器； virtualize x2APIC mode ：使能基于MSR方式对APIC寄存器的访问（x2API虚拟化）； APIC register virtualization ： 控制APIC寄存器的访问方式（MMIO/MSR BASED）将MMIO写APIC-access page操作重定向到virtual-APIC page上。 Process posted interrupts ：VT-x Posted Interrupt 特性，配置Posted Interrupt Descriptor Adress和 Notification Vector， 当目标处理器接受到中断后，硬件自动将中断请求信息copy到virtual-APIC page上。 因此，我们可以看出： APIC-Access Page的作用主要是让vCPU能过通过MMIO方式访问到特定的APIC寄存器， Virtual-APIC Page的作用是虚拟中断投递，shadow TPR和部分关键APIC寄存器的虚拟化（VTPR,VPPR,VEOI,VISR,VIRR,VICR）。 当使能了\"APIC-register virtualization\"时，Reads form the APIC-access Page时将会被虚拟化， 并且当vCPU访问APIC-access Page的时候硬件会自动返回virtual-APIC Page对应offset处的内容（相当于重定向）。 对于APIC-write操作，除了写APIC的一些关键寄存器（例如：vTPR，vEOI，vICR等）不需要VM exit（有硬件辅助加速）， 其他的大部分page offset写操作都会触发 APIC-write VM exits然后由VMM进行模拟（具体细节参考：SMD Chapter 29.4.2和29.4.3）。 注意 ：当Guest直接通过GPA方式访问APIC-access Page的时候必然会触发APIC-access exit。 尼玛，真的被搞晕了！！！ 1. Even when addresses are translated using EPT ( see Section 28.2 ), the determination of whether an APIC - access VM exit occurs depends on an access ' s physical address , not its guest - physical address 1. APIC虚拟化代码分析 The secret is APIC-access page and virtual-APIC page. if ( cpu_has_vmx_tpr_shadow () && ! init_event ) { vmcs_write64 ( VIRTUAL_APIC_PAGE_ADDR , 0 ); if ( cpu_need_tpr_shadow ( vcpu )) vmcs_write64 ( VIRTUAL_APIC_PAGE_ADDR , ¦ ¦ __pa ( vcpu -> arch . apic -> regs )); vmcs_write32 ( TPR_THRESHOLD , 0 ); } Here is a reply I received from a colleague: It is correct that the only function of the virtual-APIC page is to shadow the TPR. There are three address of interest, all of which are physical addresses (meaning that they are not subject to any kind of translation). IA32_APIC_BASE. This address is contained in an MSR. This is the address at which that actual hardware APIC is mapped. Accesses to this physical address (e.g., if this physical address is the output of paging) will access the memory-mapped registers of the actual hardware APIC. It is expected that a VMM will not map this physical address into the address space of any guest. That means the following: (1) if EPT is in use, no EPT PTE should contain this address; (2) if EPT is not in use, no ordinary PTE should have this address while a guest is running. (See #2 below for an exception.) It is also expected that the VMM will not allow any guest software to access the IA32_APIC_BASE MSR. APIC-access address. This is the address of the APIC-access page and is programmed via a field in the VMCS. The CPU will treat specially guest accesses to physical addresses on this page. For most cases, such accesses cause VM exits. The only exceptions are reads and writes of offset 080H (TPR) on the page. See item #3 for how they are treated. The relevant accesses are defined as follows: (1) if EPT is in use, accesses that use an EPT PTE that contains the APIC-address; (2) if EPT is not in use, accesses that use an ordinary PTE that contains the APIC-access address. NOTE: the APIC-access address take priority over the address in IA32_APIC_BASE. If both have the same value and are programmed into a PTE, accesses through that PTE are virtualized (cause VM exits in most cases) and do not access the actual hardware APIC. This is an exception to statements in #1 (that the address in IA32_APIC_BASE not appear in PTEs while a guest is running). Virtual-APIC address. This is the address of the virtual-APIC page and is programmed via a field in the VMCS. The CPU uses this field in three situations: (1) MOV to/from CR8; (2) RDMSR/WRMSR to MSR 808H; and (3) for accesses to offset 080H (TPR) on the APIC-access page. If the CPU detects an access to offset 080H on the APIC-access page (see above), it will redirect the access to offset 080H on the virtual-APIC page. It is expected that a VMM will not map this physical address into the address space of any guest, except guests for which the virtual-APIC address is identical to the APIC-access address. The APIC-access address and the virtual-APIC address were made distinct from each other to support guests with multiple virtual processors. In such a situation, the virtual processors could all be supported with a single hierarchy of EPT paging structures. This hierarchy would include an EPT PTE with an address that is the APIC-access address for all the virtual processors. That is, the VMCS of each virtual processor would include this address as its APIC-access address. But these VMCS's would each have its own virtual-APIC address. In this way, the virtual processors of a single guest can be supported by a single hierarchy of EPT paging structures while each having its own virtual APIC. David Ott 3. Refs https://www.linux-kvm.org/images/7/70/2012-forum-nakajima_apicv.pdf https://software.intel.com/en-us/forums/virtualization-software-development/topic/284386 https://www.spinics.net/lists/kvm/msg85565.html https://cloud.tencent.com/developer/column/75113 https://blog.csdn.net/wanthelping/article/details/47069077","tags":"virtualization"},{"title":"Virtio Spec Overview","url":"https://kernelgo.org/virtio-overview.html","loc":"https://kernelgo.org/virtio-overview.html","text":"摘要 半虚拟化设备(Virtio Device)在当前云计算虚拟化场景下已经得到了非常广泛的应用， 并且现在也有越来越多的物理设备也开始支持Virtio协议，即所谓的 Virtio Offload ， 通过将virtio协议卸载到硬件上（例如virtio-net网卡卸载，virtio-scsi卸载）让物理机和虚拟机都能够获得加速体验。 本文中我们来重点了解一下virtio技术中的一些关键点，方便我们加深对半虚拟化的理解。 本文适合对IO虚拟化有一定了解的人群阅读，本文的目的是对想要了解virtio内部机制的读者提供帮助。 在开始了解virtio之前，我们先思考一下几个相关问题： virtio设备有哪几种呈现方式? virtio-pci设备的配置空间都有哪些内容？ virtio前端和后端基于共享内存机制进行通信，它是凭什么可以做到无锁的？ virtio机制中有那几个关键的数据结构？virtio配置接口存放在哪里？virtio是如何工作的？ virtio前后端是如何进行通信的？irqfd和ioeventfd是什么回事儿？在virtio前后端通信中是怎么用到的？ virtio设备支持MSIx，在qemu/kvm中具体是怎么实现对MSIx的模拟呢？ virtio modern相对于virtio legay多了哪些新特性？ 0. 简单了解一下Virtio Spec协议 virtio协议标准最早由IBM提出，virtio作为一套标准协议现在有专门的技术委员会进行管理， 具体的标准可以访问 virtio 官网 ， 开发者可以向技术委员会提供新的virtio设备提案（ RFC ），经过委员会通过后可以增加新的virtio设备类型。 组成一个virtio设备的四要素包括： 设备状态域， feature bits ，设备配置空间，一个或者多个 virtqueue 。 其中设备状态域包含6种状态： ACKNOWLEDGE（1）：GuestOS发现了这个设备，并且认为这是一个有效的virtio设备； DRIVER (2) : GuestOS知道该如何驱动这个设备； FAILED (128) : GuestOS无法正常驱动这个设备，Something is wriong； FEATURES_OK (8) : GuestOS认识所有的feature，并且feature协商一完成； DRIVER_OK (4) : 驱动加载完成，设备可以投入使用了； DEVICE_NEEDS_RESET (64) ：设备触发了错误，需要重置才能继续工作。 feature bits 用来标志设备支持那个特性，其中bit0-bit23是特定设备可以使用的 feature bits ， bit24-bit37预给队列和feature协商机制，bit38以上保留给未来其他用途。 例如：对于virtio-net设备而言，feature bit0表示网卡设备支持checksum校验。 VIRTIO_F_VERSION_1 这个feature bit用来表示设备是否支持virtio 1.0 spec标准。 在virtio协议中，所有的设备都使用virtqueue来进行数据的传输。 每个设备可以有0个或者多个virtqueue，每个virtqueue占用2个或者更多个4K的物理页 。 virtqueue有 Split Virtqueues 和 Packed Virtqueues 两种模式， 在 Split virtqueues 模式下virtqueue被分成若干个部分， 每个部分都是前端驱动或者后端单向可写的（不能两端同时写）。 每个virtqueue都有一个16bit的queue size参数，表示队列的总长度。 每个virtqueue由3个部分组成： +-------------------+--------------------------------+-----------------------+ | Descriptor Table | Available Ring (padding) | Used Ring | +-------------------+--------------------------------+-----------------------+ Descriptor Table：存放IO传输请求信息； Available Ring：记录了Descriptor Table表中的哪些项被更新了，前端Driver可写但后端只读； Used Ring：记录Descriptor Table表中哪些请求已经被提交到硬件，前端Driver只读但后端可写。 整个virtio协议中设备IO请求的工作机制可以简单地概括为： 前端驱动将IO请求放到 Descriptor Table 中，然后将索引更新到 Available Ring 中，然后kick后端去取数据； 后端取出IO请求进行处理，然后结果刷新到 Descriptor Table 中再更新 Using Ring ，然后发送中断notify前端。 从virtio协议可以了解到 virtio设备支持3种设备呈现模式 ： Virtio Over PCI BUS，依旧遵循PCI规范，挂在到PCI总线上，作为virtio-pci设备呈现； Virtio Over MMIO，部分不支持PCI协议的虚拟化平台可以使用这种工作模式，直接挂载到系统总线上； Virtio Over Channel I/O：主要用在s390平台上，virtio-ccw使用这种基于channel I/O的机制。 其中，Virtio Over PCI BUS的使用比较广泛，作为PCI设备需按照规范要通过PCI配置空间来向操作系统报告设备支持的特性集合， 这样操作系统才知道这是一个什么类型的virtio设备，并调用对应的前端驱动和这个设备进行握手，进而将设备驱动起来。 QEMU会给virtio设备模拟PCI配置空间，对于virtio设备来说PCI Vendor ID固定为0x1AF4， PCI Device ID 为 0x1000到0x107F之间的是virtio设备。 同时，在不支持PCI协议的虚拟化平台上，virtio设备也可以直接通过MMIO进行呈现， virtio-spec 4.2 Virtio Over MMIO 有针对virtio-mmio设备呈现方式的详细描述，mmio相关信息可以直接通过内核参数报告给Linux操作系统。 本文主要基于virtio-pci展开讨论。 前面提到virtio设备有 feature bits ， virtqueue 等四要素，那么在virtio-pci模式下是如何呈现的呢？ 从virtio spec来看，老的virtio协议和新的virtio协议在这一块有很大改动。 virtio legacy（virtio 0.95）协议规定，对应的配置数据结构（virtio common configuration structure） 应该存放在设备的BAR0里面，我们称之为 virtio legay interface ，其结构如下： virtio legacy == > Mapped into PCI BAR0 +------------------------------------------------------------------+ | Host Feature Bits [ 0:31 ] | +------------------------------------------------------------------+ | Guest Feature Bits [ 0:31 ] | +------------------------------------------------------------------+ | Virtqueue Address PFN | +---------------------------------+--------------------------------+ | Queue Select | Queue Size | +----------------+----------------+--------------------------------+ | ISR Status | Device Stat | Queue Notify | +----------------+----------------+--------------------------------+ | MSI Config Vector | MSI Queue Vector | +---------------------------------+--------------------------------+ 对于新的 virtio modern ，协议将配置结构划分为5种类型： /* Common configuration */ # define VIRTIO_PCI_CAP_COMMON_CFG 1 /* Notifications */ # define VIRTIO_PCI_CAP_NOTIFY_CFG 2 /* ISR Status */ # define VIRTIO_PCI_CAP_ISR_CFG 3 /* Device specific configuration */ # define VIRTIO_PCI_CAP_DEVICE_CFG 4 /* PCI configuration access */ # define VIRTIO_PCI_CAP_PCI_CFG 5 以上的每种配置结构是直接映射到virtio设备的BAR空间内，那么如何指定每种配置结构的位置呢？ 答案是通过 PCI Capability list 方式去指定，这和物理PCI设备是一样的，体现了virtio-pci的协议兼容性。 struct virtio_pci_cap { u8 cap_vndr ; /* Generic PCI field: PCI_CAP_ID_VNDR */ u8 cap_next ; /* Generic PCI field: next ptr. */ u8 cap_len ; /* Generic PCI field: capability length */ u8 cfg_type ; /* Identifies the structure. */ u8 bar ; /* Where to find it. */ u8 padding [ 3 ] ; /* Pad to full dword. */ le32 offset ; /* Offset within bar. */ le32 length ; /* Length of the structure, in bytes. */ } ; 只是略微不同的是，virtio-pci的Capability有一个统一的结构， 其中 cfg_type 表示Cap的类型，bar表示这个配置结构被映射到的BAR空间号。 这样每个配置结构都可以通过BAR空间直接访问，或者通过PCI配置空间的 VIRTIO_PCI_CAP_PCI_CFG 域进行访问。 每个Cap的具体结构定义可以参考virtio spec 4.1.4.3小节。 1. 前后端数据共享 传统的纯模拟设备在工作的时候，会触发频繁的陷入陷出， 而且IO请求的内容要进行多次拷贝传递，严重影响了设备的IO性能。 virtio为了提升设备的IO性能，采用了共享内存机制， 前端驱动会提前申请好一段物理地址空间用来存放IO请求，然后将这段地址的GPA告诉QEMU 。 前端驱动在下发IO请求后，QEMU可以直接从共享内存中取出请求，然后将完成后的结果又直接写到虚拟机对应地址上去。 整个过程中可以做到直投直取，省去了不必要的数据拷贝开销 。 Virtqueue 是整个virtio方案的灵魂所在 。每个virtqueue都包含3张表， Descriptor Table 存放了IO请求描述符， Available Ring 记录了当前哪些描述符是可用的， Used Ring 记录了哪些描述符已经被后端使用了。 +------------------------------------+ | virtio guest driver | +-----------------+------------------+ / | &#94; / | \\ put update get / | \\ V V \\ +----------+ +------------+ +----------+ | | | | | | +----------+ +------------+ +----------+ | available| | descriptor | | used | | ring | | table | | ring | +----------+ +------------+ +----------+ | | | | | | +----------+ +------------+ +----------+ | | | | | | +----------+ +------------+ +----------+ \\ &#94; &#94; \\ | / get update put \\ | / V | / +----------------+-------------------+ | virtio host backend | +------------------------------------+ Desriptor Table 中存放的是一个一个的 virtq_desc 元素，每个 virq_desc 元素占用16个字节。 +-----------------------------------------------------------+ | addr/gpa [ 0:63 ] | +-------------------------+-----------------+---------------+ | len [ 0:31 ] | flags [ 0:15 ] | next [ 0:15 ] | +-------------------------+-----------------+---------------+ 其中，addr占用64bit存放了单个IO请求的GPA地址信息，例如addr可能表示某个DMA buffer的起始地址。 len占用32bit表示IO请求的长度，flags的取值有3种， VIRTQ_DESC_F_NEXT 表示这个IO请求和下一个 virtq_desc 描述的是连续的， IRTQ_DESC_F_WRITE 表示这段buffer是write only的， VIRTQ_DESC_F_INDIRECT 表示这段buffer里面放的内容是另外一组buffer的 virtq_desc （相当于重定向）， next是指向下一个 virtq_desc 的索引号（前提是 VIRTQ_DESC_F_NEXT & flags）。 Available Ring 是前端驱动用来告知后端那些IO buffer是的请求需要处理，每个Ring中包含一个 virtq_avail 占用8个字节。 其中，flags取值为 VIRTQ_AVAIL_F_NO_INTERRUPT 时表示前端驱动告诉后端： \"当你消耗完一个IO buffer的时候，不要立刻给我发中断\"（防止中断过多影响效率）。 idx表示下次前端驱动要放置 Descriptor Entry 的地方。 +--------------+-------------+--------------+---------------------+ | flags [ 0:15 ] | idx [ 0:15 ] | ring [ 0:15 ] | used_event [ 0:15 ] | +--------------+-------------+--------------+---------------------+ Used Ring结构稍微不一样，flags的值如果为 VIRTIO_F_EVENT_IDX 并且前后端协商 VIRTIO_F_EVENT_IDX feature成功, 那么Guest会将used ring index放在available ring的末尾，告诉后端说： \"Hi 小老弟，当你处理完这个请求的时候，给我发个中断通知我一下\"， 同时host也会将avail_event index放到used ring的末尾，告诉guest说： \"Hi 老兄，记得把这个idx的请求kick给我哈\"。 VIRTIO_F_EVENT_IDX 对virtio通知/中断有一定的优化，在某些场景下能够提升IO性能。 /* The Guest publishes the used index for which it expects an interrupt * at the end of the avail ring. Host should ignore the avail->flags field. */ /* The Host publishes the avail index for which it expects a kick * at the end of the used ring. Guest should ignore the used->flags field. */ struct virtq_used { #define VIRTQ_USED_F_NO_NOTIFY 1 le16 flags ; le16 idx ; struct virtq_used_elem ring [ /* Queue Size */ ]; le16 avail_event ; /* Only if VIRTIO_F_EVENT_IDX */ }; /* le32 is used here for ids for padding reasons. */ struct virtq_used_elem { /* Index of start of used descriptor chain. */ le32 id ; /* Total length of the descriptor chain which was used (written to) */ le32 len ; }; 原理就到这里，后面会以virtio网卡为例进行详细流程说明。 2. 前后端通信机制（irqfd 与 ioeventfd） 共享内存方式解决了传统设备IO过程中内存拷贝带来的性能损耗问题，除此之外前端驱动和后端驱动的通信问题也是有可以改进的地方。 Virtio前后端通信概括起来只有两个方向，即GuestOS通知QEMU和QEMU通知GuestOS。 当前端驱动准备好IO buffer之后，需要通知后端（QEMU），告诉后端： \"小老弟，我有一波IO请求已经准备好了，你帮我处理一下\"。 前端通知出去后，就可以等待IO结果了（操作系统可以进行一次调度），这时候vCPU可以去干点其他的事情。 后端收到消息后开始处理IO请求，当IO请求处理完成之后，后端就通过中断机制通知GuestOS： \"老哥，你的IO给你处理好了，你来取一下\"。 前后端通信机制如下图所示： +-------------+ +-------------+ | | | | | | | | | GuestOS | | QEMU | | | | | | | | | +---+---------+ +----+--------+ | &#94; | &#94; | | | | +---|-----|-------------------------|----|---+ | | | irqfd | | | | | +-------------------------+ | | | | ioeventfd | | | +------------------------------------+ | | KVM | +--------------------------------------------+ 前端驱动通知后端比较简单，QEMU设置一段特定的MMIO地址空间，前端驱动访问这段MMIO触发VMExit， 退出到KVM后利用 ioeventfd 机制通知到用户态的QEMU，QEMU主循环（main_loop poll） 检测到ioeventfd事件后调用callback进行处理。 前端驱动通知后端： 内核流程 mark一下 ， PCI设备驱动流程这个后面可以学习一下 ，先扫描 PCI bus发现是virtio设备再扫描virtio - bus 。 worker_thread --> process_one_work --> pciehp_power_thread --> pciehp_enable_slot --> pciehp_configure_device --> pci_bus_add_devices --> pci_bus_add_device --> device_attach --> __device_attach --> bus_for_each_drv --> __device_attach_driver --> driver_probe_device --> pci_device_probe --> local_pci_probe --> virtio_pci_probe --> register_virtio_device --> device_register --> device_add --> bus_probe_device --> device_initial_probe --> __device_attach --> bus_for_each_drv --> __device_attach_driver --> driver_probe_device --> virtio_dev_probe --> virtnet_probe ( 网卡设备驱动加载的入口 ) static int virtnet_probe ( struct virtio_device * vdev ) { ...... virtio_device_ready ( vdev ); } /** * virtio_device_ready - enable vq use in probe function * @vdev: the device * * Driver must call this to use vqs in the probe function. * * Note: vqs are enabled automatically after probe returns. */ static inline void virtio_device_ready ( struct virtio_device * dev ) { unsigned status = dev -> config -> get_status ( dev ); BUG_ON ( status & VIRTIO_CONFIG_S_DRIVER_OK ); dev -> config -> set_status ( dev , status | VIRTIO_CONFIG_S_DRIVER_OK ); } # QEMU/KVM后端的处理流程如下： # 前端驱动写Status位，val & VIRTIO_CONFIG_S_DRIVER_OK，这时候前端驱动已经ready virtio_pci_config_write --> virtio_ioport_write --> virtio_pci_start_ioeventfd --> virtio_bus_set_host_notifier --> virtio_bus_start_ioeventfd --> virtio_device_start_ioeventfd_impl --> virtio_bus_set_host_notifier --> virtio_pci_ioeventfd_assign --> memory_region_add_eventfd --> memory_region_transaction_commit --> address_space_update_ioeventfds --> address_space_add_del_ioeventfds --> kvm_io_ioeventfd_add / vhost_eventfd_add --> kvm_set_ioeventfd_pio --> kvm_vm_ioctl ( kvm_state , KVM_IOEVENTFD , & kick ) 其实，这就是QEMU的 Fast MMIO 实现机制。 我们可以看到，QEMU会为每个设备MMIO对应的MemoryRegion注册一个ioeventfd。 最后调用了一个KVM_IOEVENTFD ioctl到KVM内核里面，而在KVM内核中会将MMIO对应的（gpa,len,eventfd）信息会注册到KVM_FAST_MMIO_BUS上。 这样当Guest访问MMIO地址范围退出后（触发 EPT Misconfig ），KVM会查询一下访问的GPA是否落在某段MMIO地址空间range内部， 如果是的话就直接写eventfd告知QEMU，QEMU就会从coalesced mmio ring page中取MMIO请求 （注：pio page和 mmio page是QEMU和KVM内核之间的共享内存页，已经提前mmap好了）。 #kvm内核代码virt/kvm/eventfd.c中 kvm_vm_ioctl ( KVM_IOEVENTFD ) --> kvm_ioeventfd --> kvm_assign_ioeventfd --> kvm_assign_ioeventfd_idx # MMIO处理流程中（handle_ept_misconfig）最后会调用到ioeventfd_write通知QEMU。 /* MMIO/PIO writes trigger an event if the addr/val match */ static int ioeventfd_write ( struct kvm_vcpu * vcpu , struct kvm_io_device * this , gpa_t addr , int len , const void * val ) { struct _ioeventfd * p = to_ioeventfd ( this ); if ( ! ioeventfd_in_range ( p , addr , len , val )) return - EOPNOTSUPP ; eventfd_signal ( p -> eventfd , 1 ); return 0 ; } 不了解 MMIO 是如何模拟的童鞋，可以结合本站的文章 MMIO 模拟实现分析 去了解一下， 如果还是不懂的可以在文章下面评论。 后端通知前端，是通过中断的方式 ，QEMU/KVM中有一套完整的中断模拟实现框架， 如果对QEMU/KVM中断模拟不熟悉的童鞋， 建议阅读一下这篇文章： QEMU学习笔记-中断 。 对于virtio-pci设备，可以通过Cap呈现MSIx给虚拟机，这样在前端驱动加载的时候就会尝试去使能MSIx中断， 后端在这个时候建立起MSIx通道。 前端驱动加载(probe)的过程中，会去初始化 virtqueue ，这个时候会去申请MSIx中断并注册中断处理函数： virtnet_probe --> init_vqs --> virtnet_find_vqs --> vi -> vdev -> config -> find_vqs [ vp_modern_find_vqs ] --> vp_find_vqs --> vp_find_vqs_msix // 为每virtqueue申请一个MSIx中断，通常收发各一个队列 --> vp_request_msix_vectors // 主要的MSIx中断申请逻辑都在这个函数里面 --> pci_alloc_irq_vectors_affinity // 申请MSIx中断描述符(__pci_enable_msix_range) --> request_irq // 注册中断处理函数 // virtio-net网卡至少申请了3个MSIx中断： // 一个是configuration change中断（配置空间发生变化后，QEMU通知前端） // 发送队列1个MSIx中断，接收队列1MSIx中断 在QEMU/KVM这一侧，开始模拟MSIx中断，具体流程大致如下： virtio_pci_config_write --> virtio_ioport_write --> virtio_set_status --> virtio_net_vhost_status --> vhost_net_start --> virtio_pci_set_guest_notifiers --> kvm_virtio_pci_vector_use |--> kvm_irqchip_add_msi_route //更新中断路由表 |--> kvm_virtio_pci_irqfd_use //使能MSI中断 --> kvm_irqchip_add_irqfd_notifier_gsi --> kvm_irqchip_assign_irqfd # 申请MSIx中断的时候，会为MSIx分配一个gsi，并为这个gsi绑定一个irqfd，然后调用ioctl KVM_IRQFD注册到内核中。 static int kvm_irqchip_assign_irqfd ( KVMState * s , int fd , int rfd , int virq , bool assign ) { struct kvm_irqfd irqfd = { . fd = fd , . gsi = virq , . flags = assign ? 0 : KVM_IRQFD_FLAG_DEASSIGN , }; if ( rfd != -1 ) { irqfd . flags |= KVM_IRQFD_FLAG_RESAMPLE ; irqfd . resamplefd = rfd ; } if ( ! kvm_irqfds_enabled ()) { return - ENOSYS ; } return kvm_vm_ioctl ( s , KVM_IRQFD , & irqfd ); } # KVM内核代码virt/kvm/eventfd.c kvm_vm_ioctl ( s , KVM_IRQFD , & irqfd ) --> kvm_irqfd_assign --> vfs_poll ( f . file , & irqfd -> pt ) // 在内核中poll这个irqfd 从上面的流程可以看出， QEMU/KVM使用 irqfd 机制来模拟MSIx中断 ， 即设备申请MSIx中断的时候会为MSIx分配一个gsi（这个时候会刷新irq routing table）， 并为这个gsi绑定一个 irqfd ，最后在内核中去 poll 这个 irqfd 。 当QEMU处理完IO之后，就写MSIx对应的irqfd，给前端注入一个MSIx中断，告知前端我已经处理好IO了你可以来取结果了。 例如，virtio-scsi从前端取出IO请求后会取做DMA操作（DMA是异步的，QEMU协程中负责处理）。 当DMA完成后QEMU需要告知前端IO请求已完成（Complete），那么怎么去投递这个MSIx中断呢？ 答案是调用 virtio_notify_irqfd 注入一个MSIx中断。 #0 0x00005604798d569b in virtio_notify_irqfd (vdev=0x56047d12d670, vq=0x7fab10006110) at hw/virtio/virtio.c:1684 #1 0x00005604798adea4 in virtio_scsi_complete_req (req=0x56047d09fa70) at hw/scsi/virtio-scsi.c:76 #2 0x00005604798aecfb in virtio_scsi_complete_cmd_req (req=0x56047d09fa70) at hw/scsi/virtio-scsi.c:468 #3 0x00005604798aee9d in virtio_scsi_command_complete (r=0x56047ccb0be0, status=0, resid=0) at hw/scsi/virtio-scsi.c:495 #4 0x0000560479b397cf in scsi_req_complete (req=0x56047ccb0be0, status=0) at hw/scsi/scsi-bus.c:1404 #5 0x0000560479b2b503 in scsi_dma_complete_noio (r=0x56047ccb0be0, ret=0) at hw/scsi/scsi-disk.c:279 #6 0x0000560479b2b610 in scsi_dma_complete (opaque=0x56047ccb0be0, ret=0) at hw/scsi/scsi-disk.c:300 #7 0x00005604799b89e3 in dma_complete (dbs=0x56047c6e9ab0, ret=0) at dma-helpers.c:118 #8 0x00005604799b8a90 in dma_blk_cb (opaque=0x56047c6e9ab0, ret=0) at dma-helpers.c:136 #9 0x0000560479cf5220 in blk_aio_complete (acb=0x56047cd77d40) at block/block-backend.c:1327 #10 0x0000560479cf5470 in blk_aio_read_entry (opaque=0x56047cd77d40) at block/block-backend.c:1387 #11 0x0000560479df49c4 in coroutine_trampoline (i0=2095821104, i1=22020) at util/coroutine-ucontext.c:115 #12 0x00007fab214d82c0 in __start_context () at /usr/lib64/libc.so.6 在 virtio_notify_irqfd 函数中，会去写 irqfd ，给内核发送一个信号。 void virtio_notify_irqfd ( VirtIODevice * vdev , VirtQueue * vq ) { ... /* * virtio spec 1.0 says ISR bit 0 should be ignored with MSI, but * windows drivers included in virtio-win 1.8.0 (circa 2015) are * incorrectly polling this bit during crashdump and hibernation * in MSI mode, causing a hang if this bit is never updated. * Recent releases of Windows do not really shut down, but rather * log out and hibernate to make the next startup faster. Hence, * this manifested as a more serious hang during shutdown with * * Next driver release from 2016 fixed this problem, so working around it * is not a must, but it's easy to do so let's do it here. * * Note: it's safe to update ISR from any thread as it was switched * to an atomic operation. */ virtio_set_isr ( vq -> vdev , 0x1 ); event_notifier_set ( & vq -> guest_notifier ); //写vq->guest_notifier，即irqfd } QEMU写了这个 irqfd 后，KVM内核模块中的irqfd poll就收到一个 POLL_IN 事件，然后将MSIx中断自动投递给对应的LAPIC。 大致流程是： POLL_IN -> kvm_arch_set_irq_inatomic -> kvm_set_msi_irq , kvm_irq_delivery_to_apic_fast static int irqfd_wakeup ( wait_queue_entry_t * wait , unsigned mode , int sync , void * key ) { if ( flags & EPOLLIN ) { idx = srcu_read_lock ( & kvm -> irq_srcu ); do { seq = read_seqcount_begin ( & irqfd -> irq_entry_sc ); irq = irqfd -> irq_entry ; } while ( read_seqcount_retry ( & irqfd -> irq_entry_sc , seq )); /* An event has been signaled, inject an interrupt */ if ( kvm_arch_set_irq_inatomic ( & irq , kvm , KVM_USERSPACE_IRQ_SOURCE_ID , 1 , false ) == - EWOULDBLOCK ) schedule_work ( & irqfd -> inject ); srcu_read_unlock ( & kvm -> irq_srcu , idx ); } 这里还有一点没有想明白，结合代码和调试来看，virtio-blk/virtio-scsi的msi中断走irqfd机制， 但是virtio-net（不开启vhost的情况下）不走irqfd，而是直接调用 virtio_notify / virtio_pci_notify ， 最后通过KVM的ioctl投递中断？ 从代码路径上来看，后者明显路径更长，谁知道原因告诉我一下!!!。 https://patchwork.kernel.org/patch/9531577/ Once in virtio_notify_irqfd , once in virtio_queue_guest_notifier_read . Unfortunately , for virtio - blk + MSI + KVM + old Windows drivers we need the one in virtio_notify_irqfd . For virtio - net + vhost + INTx we need the one in virtio_queue_guest_notifier_read . 这显然路径更长啊。 Ok，到这里virtio前后端通信机制已经明了，最后一个小节我们以virtio-net为例，梳理一下virtio中的部分核心代码流程。 3. virtio核心代码分析，以virtio-net为例 这里我们已virtio-net网卡为例，在没有使用vhost的情况下（网卡后端收发包都走QEMU处理）， 后端收发包走vhost的情况下有些不同，后面单独分析。 3.1 前后端握手流程 QEM模拟PCI设备对GuestOS进行呈现，设备驱动加载的时候尝试去初始化设备。 # 先在PCI总线上调用probe设备，调用了virtio_pci_probe，然后再virtio-bus上调用virtio_dev_probe # virtio_dev_probe最后调用到virtnet_probe pci_device_probe --> local_pci_probe --> virtio_pci_probe --> register_virtio_device --> device_register --> device_add --> bus_probe_device --> device_initial_probe --> __device_attach --> bus_for_each_drv --> __device_attach_driver --> driver_probe_device --> virtio_dev_probe --> virtnet_probe # 在virtio_pci_probe里先尝试以virtio modern方式读取设备配置数据结构，如果失败则尝试virio legacy方式。 # 对于virtio legacy，我们前面提到了virtio legacy协议规定设备的配置数据结构放在PCI BAR0里面。 /* the PCI probing function */ int virtio_pci_legacy_probe ( struct virtio_pci_device * vp_dev ) { rc = pci_request_region ( pci_dev , 0 , \"virtio-pci-legacy\" ); //将设备的BAR0映射到物理地址空间 vp_dev -> ioaddr = pci_iomap ( pci_dev , 0 , 0 ); //获得BAR0的内核地址 } #对于virtio modern，通过capability方式报告配置数据结构的位置，配置数据结构有5种类型。 int virtio_pci_modern_probe ( struct virtio_pci_device * vp_dev ) { /* check for a common config: if not, use legacy mode (bar 0). */ common = virtio_pci_find_capability ( pci_dev , VIRTIO_PCI_CAP_COMMON_CFG , IORESOURCE_IO | IORESOURCE_MEM , & vp_dev -> modern_bars ); /* If common is there, these should be too... */ isr = virtio_pci_find_capability ( pci_dev , VIRTIO_PCI_CAP_ISR_CFG , IORESOURCE_IO | IORESOURCE_MEM , & vp_dev -> modern_bars ); notify = virtio_pci_find_capability ( pci_dev , VIRTIO_PCI_CAP_NOTIFY_CFG , IORESOURCE_IO | IORESOURCE_MEM , & vp_dev -> modern_bars ); /* Device capability is only mandatory for devices that have * device-specific configuration. */ device = virtio_pci_find_capability ( pci_dev , VIRTIO_PCI_CAP_DEVICE_CFG , IORESOURCE_IO | IORESOURCE_MEM , & vp_dev -> modern_bars ); err = pci_request_selected_regions ( pci_dev , vp_dev -> modern_bars , \"virtio-pci-modern\" ); sizeof ( struct virtio_pci_common_cfg ), 4 , 0 , sizeof ( struct virtio_pci_common_cfg ), NULL ); // 将配virtio置结构所在的BAR空间MAP到内核地址空间里 vp_dev -> common = map_capability ( pci_dev , common , sizeof ( struct virtio_pci_common_cfg ), 4 , 0 , sizeof ( struct virtio_pci_common_cfg ), NULL ); ...... } # 接着来到virtio_dev_probe里面看下： static int virtio_dev_probe ( struct device * _d ) { /* We have a driver! */ virtio_add_status ( dev , VIRTIO_CONFIG_S_DRIVER ); // 更新status bit，这里要写配置数据结构 /* Figure out what features the device supports. */ device_features = dev -> config -> get_features ( dev ); // 查询后端支持哪些feature bits // feature set协商，取交集 err = virtio_finalize_features ( dev ); // 调用特定virtio设备的驱动程序probe，例如: virtnet_probe, virtblk_probe err = drv -> probe ( dev ); } 再看下 virtnet_probe 里面的一些关键的流程，这里包含了virtio-net网卡前端初始化的主要逻辑。 static int virtnet_probe ( struct virtio_device * vdev ) { // check后端是否支持多队列，并按情况创建队列 /* Allocate ourselves a network device with room for our info */ dev = alloc_etherdev_mq ( sizeof ( struct virtnet_info ), max_queue_pairs ); // 定义一个网络设备并配置一些属性，例如MAC地址 dev -> ethtool_ops = & virtnet_ethtool_ops ; SET_NETDEV_DEV ( dev , & vdev -> dev ); // 初始化virtqueue err = init_vqs ( vi ); // 注册一个网络设备 err = register_netdev ( dev ); // 写状态位DRIVER_OK，告诉后端，前端已经ready virtio_device_ready ( vdev ); // 将网卡up起来 netif_carrier_on ( dev ); } 其中关键的流程是 init_vqs ，在 vp_find_vqs_msix 流程中会尝试去申请MSIx中断，这里前面已经有分析过了。 其中，\"configuration changed\" 中断服务程序 vp_config_changed ， virtqueue队列的中断服务程序是 vp_vring_interrupt 。 init_vqs --> virtnet_find_vqs --> vi -> vdev -> config -> find_vqs --> vp_modern_find_vqs --> vp_find_vqs --> vp_find_vqs_msix static int vp_find_vqs_msix ( struct virtio_device * vdev , unsigned nvqs , struct virtqueue * vqs [], vq_callback_t * callbacks [], const char * const names [], bool per_vq_vectors , const bool * ctx , struct irq_affinity * desc ) { /* 为configuration change申请MSIx中断 */ err = vp_request_msix_vectors ( vdev , nvectors , per_vq_vectors , per_vq_vectors ? desc : NULL ); for ( i = 0 ; i < nvqs ; ++ i ) { // 创建队列 --> vring_create_virtqueue --> vring_create_virtqueue_split --> vring_alloc_queue vqs [ i ] = vp_setup_vq ( vdev , queue_idx ++ , callbacks [ i ], names [ i ], ctx ? ctx [ i ] : false , msix_vec ); // 每个队列申请一个MSIx中断 err = request_irq ( pci_irq_vector ( vp_dev -> pci_dev , msix_vec ), vring_interrupt , 0 , vp_dev -> msix_names [ msix_vec ], vqs [ i ]); } vp_setup_vq 流程再往下走就开始分配共享内存页，至此建立起共享内存通信通道。 值得注意的是一路传下来的callbacks参数其实传入了发送队列和接收队列的回调处理函数， 好家伙，从 virtnet_find_vqs 一路传递到了 __vring_new_virtqueue 中最终赋值给了 vq->vq.callback 。 static struct virtqueue * vring_create_virtqueue_split ( unsigned int index , unsigned int num , unsigned int vring_align , struct virtio_device * vdev , bool weak_barriers , bool may_reduce_num , bool context , bool ( * notify )( struct virtqueue * ), void ( * callback )( struct virtqueue * ), const char * name ) { /* TODO : allocate each queue chunk individually */ for (; num && vring_size ( num , vring_align ) > PAGE_SIZE ; num /= 2 ) { // 申请物理页，地址赋值给 queue queue = vring_alloc_queue ( vdev , vring_size ( num , vring_align ), & dma_addr , GFP_KERNEL | __GFP_NOWARN | __GFP_ZERO ); } queue_size_in_bytes = vring_size ( num , vring_align ); vring_init ( & vring , num , queue , vring_align ); // 确定 descriptor table , available ring , used ring的位置 。 } 我们看下如果 virtqueue 队列如果收到MSIx中断消息后，会调用哪个 hook 来处理？ irqreturn_t vring_interrupt ( int irq , void * _vq ) { struct vring_virtqueue * vq = to_vvq ( _vq ); if ( ! more_used ( vq )) { pr_debug ( \"virtqueue interrupt with no work for %p \\n \" , vq ); return IRQ_NONE ; } if ( unlikely ( vq -> broken )) return IRQ_HANDLED ; pr_debug ( \"virtqueue callback for %p (%p) \\n \" , vq , vq -> vq . callback ); if ( vq -> vq . callback ) vq -> vq . callback ( & vq -> vq ); return IRQ_HANDLED ; } EXPORT_SYMBOL_GPL ( vring_interrupt ); 不难想到中断服务程序里面会调用队列上的callback。 我们再回过头来看下 virtnet_find_vqs ，原来接受队列的回调函数是 skb_recv_done ，发送队列的回调函数是 skb_xmit_done 。 static int virtnet_find_vqs ( struct virtnet_info * vi ) { /* Allocate/initialize parameters for send/receive virtqueues */ for ( i = 0 ; i < vi -> max_queue_pairs ; i ++ ) { callbacks [ rxq2vq ( i )] = skb_recv_done ; callbacks [ txq2vq ( i )] = skb_xmit_done ; } } OK，这个小节就到这里。Are you clear ? 3.2 virtio-net网卡收发在virtqueue上的实现 这里以virtio-net为例（非vhost-net模式）来分析一下网卡收发报文在virtio协议上的具体实现。 virtio-net模式下网卡收发包的流程为： 收包：Hardware => Host Kernel => Qemu => Guest 发包：Guest => Host Kernel => Qemu => Host Kernel => Hardware 3.2.1 virtio-net网卡发包 前面我们看到virtio-net设备初始化的时候会创建一个 net_device 设备： virtnet_probe -> alloc_etherdev_mq 注册了 netdev_ops = &virtnet_netdev ， 这里 virtnet_netdev 是网卡驱动的回调函数集合（收发包和参数设置）。 static const struct net_device_ops netdev_ops = { . ndo_open = rio_open , . ndo_start_xmit = start_xmit , . ndo_stop = rio_close , . ndo_get_stats = get_stats , . ndo_validate_addr = eth_validate_addr , . ndo_set_mac_address = eth_mac_addr , . ndo_set_rx_mode = set_multicast , . ndo_do_ioctl = rio_ioctl , . ndo_tx_timeout = rio_tx_timeout , }; 网卡发包的时候调用 ndo_start_xmit ，将TCP/IP上层协议栈扔下来的数据发送出去。 对应到virtio网卡的回调函数就是 start_xmit ，从代码看就是将skb发送到virtqueue中， 然后调用virtqueue_kick通知qemu后端将数据包发送出去。 Guest内核里面的virtio-net驱动发包： 内核驱动 virtio_net . c start_xmit // 将skb放到virtqueue队列中 -> xmit_skb -> sg_init_table , virtqueue_add_outbuf -> virtqueue_add // kick通知qemu后端去取 virtqueue_kick_prepare && virtqueue_notify // kick次数加1 sq -> stats . kicks ++ Guest Kick后端从KVM中VMExit出来退出到Qemu用户态（走的是ioeventfd）由Qemu去将数据发送出去。 大致调用的流程是： virtio_queue_host_notifier_read -> virtio_net_handle_tx_bh -> virtio_net_flush_tx -> virtqueue_pop 拿到发包(skb) -> qemu_sendv_packet_async Qemu代码virtio - net相关代码 : virtio_queue_host_notifier_read -> virtio_queue_notify_vq -> vq -> handle_output -> virtio_net_handle_tx_bh 队列注册的时候，回注册回调函数 -> qemu_bh_schedule -> virtio_net_tx_bh -> virtio_net_flush_tx -> virtqueue_pop -> qemu_sendv_packet_async // 报文放到发送队列上，写tap设备的fd去发包 -> tap_receive_iov -> tap_write_packet // 最后调用 tap_write_packet 把数据包发给tap设备投递出去 3.2.2 virtio-net网卡收包 网卡收包的时候，tap设备先收到报文，对应的virtio-net网卡tap设备fd变为可读， Qemu主循环收到POLL_IN事件调用回调函数收包。 tap_send -> qemu_send_packet_async -> qemu_send_packet_async_with_flags -> qemu_net_queue_send -> qemu_net_queue_deliver -> qemu_deliver_packet_iov -> nc_sendv_compat -> virtio_net_receive -> virtio_net_receive_rcu virtio-net网卡收报最终调用了 virtio_net_receive_rcu ， 和发包类似都是调用 virtqueue_pop 从前端获取virtqueue element， 将报文数据填充到vring中然后 virtio_notify 注入中断通知前端驱动取结果。 这里不得不吐槽一下，为啥收包函数取名要叫 tap_send 。 4. 参考文献 virtio spec v1.1 Towards a De-Facto Standard For Virtual https://github.com/qemu/qemu/blob/master/hw/net/virtio-net.c https://github.com/torvalds/linux/blob/master/drivers/net/virtio_net.c","tags":"virtualization"},{"title":"ARMv8 Virtualization Overview","url":"https://kernelgo.org/armv8-virt-guide.html","loc":"https://kernelgo.org/armv8-virt-guide.html","text":"摘要： ARM处理器在移动领域已经大放异彩占据了绝对优势，但在服务器领域当前主要还是X86的天下。 为了能够和X86在服务器领域展开竞争，ARM也逐渐对虚拟化扩展有了较为完善的支持。 本文的目的是介绍一下ARMv8 AArch64处理器的虚拟化扩展中的一些相关知识点， 将主要从ARM体系结构、内存虚拟化、中断虚拟化、I/O虚拟化等几个方面做一些概括总结。 本文将尽可能的在特性层面和X86做一些对比以加深我们对于ARM Virtualizaiton Extension的映像。 0. ARMv8 System Architecture 在进入正题之前先回顾一下ARMv8体系结构的一些基本概念。 ARMv8支持两种执行状态：AArch64和AArch32。 AArch64 64-bit执行状态： 提供31个64bit的通用处理器，其中X30是Procedure link register 提供一个64bit的程序寄存器PC，堆栈指针(SPs)和Exception link registers（ELRs） 提供了32个128bit的寄存器以支持SIMD矢量和标量浮点运算 定义了4个Exception Level （EL0-EL3） 支持64bit虚拟机地址(virtual address) 定义了一些PSTATE eletems来存储PE的状态 过后ELn缀来表示不同Exception Level下可以操作的系统寄存器 AArch32 32-bit执行状态： 提供了13个32bit通用寄存器，1个32bit的PC，SP和Link Register（LR） 为Hyper Mode下的异常返回值提供给了一个单一的ELR 提供32个64bit的寄存器来支持SIMD矢量和标量浮点运算支持 提供了2个指令集，A32和T32, 支持ARMv7-A Exception Mode，基于PE modes并且可以对应到ARMv8的Exception model中 使用32bit的虚拟地址 使用一个单一的CPSR来保存PE的状态 ARM 内存模型： 非对齐的内存访问将产生一个异常 限制应用程序访问指定的内存区域 程序执行中的虚拟地址将被翻译成物理地址 内存访问顺序受控 控制cache和地址翻译的结构 多个PE之间共享内存的访问同步 ARM 内存管理（ 参考ARM Address Translation ）： ARMv8-A架构支持的最大物理内存地址宽度是48bit，支持4KB、16KB、或者64KB的页面大小 使用虚拟内存管理机制，VA的最高有效位（MSB）为0时MMU使用TTBR0的转换表来翻译，VA的最高有效位为1时MMU使用TTBR1的转换表来翻译 EL2和EL3有TTBR0，但是没有TTBR1，这意味着EL2和EL3下只能使用0x0~0x0000FFFF_FFFFFFFF范围的虚拟地址空间 ------------------------------------------------------------------------- AArch64 Linux memory layout with 4KB pages + 4 levels:: Start End Size Use 0000000000000000 0000ffffffffffff 256TB user ffff000000000000 ffffffffffffffff 256TB kernel ------------------------------------------------------------------------- OK，假装我们现在的ARMv8-A已经有了一个初步的了解，下面再从几个大的维度去看下ARMv8对虚拟化是怎么支持的。 1. ARMv8 Virtualization Extension Overview ARM为了支持虚拟化扩展在CPU的运行级别上引入了Exception Level的概念，AArch64对应的Exception Level视图如下图： EL0：用户态程序的运行级别，Guest内部的App也运行在这个级别 EL1：内核的运行级别，Guest的内核也运行在这个级别 EL2：Hypervisor的运行级别，Guest在运行的过程中会触发特权指令后陷入到EL2级别，将控制权交给Hypervisor EL3：Monitor Mode，CPU在Secure World和 Normal World直接切换的时候会先进入EL3，然后发生World切换 注：当CPU的Virtualization Extension被disable的时候，软件就运行在EL0和EL1上，这时候EL1有权限访问所有的硬件。 与ARMv8不同的是，在X86为支持CPU虚拟化引入了Root Mode和None-Root Mode的概念和一套特殊的VMX指令集， 其中非根模式是Guest CPU的执行环境，根模式是Host CPU的执行环境。 根模式、非根模式与CPU的特权级别是两个完全独立的概念，二者完全正交， 也就是说非根模式下支持和根模式下一样的用户态（Ring 3）、内核态（Ring 0）特权级。 而这和ARM是不同的，ARM CPU是依靠在不同的EL之间切换来支持虚拟化模式切换。 但二者都有一个相同点:那就是ARM和X86在虚拟化模式下如果执行了敏感指令会分别退出到EL2和Root Mode之间。 同时，X86上为了更好地支持Root/Non-root Mode在内存中实现了一个叫做VMCS的数据结构， 用来保存和恢复Root/None-root模式切换过程中的寄存器信息，VMX指令集则专门用来操作VMCS数据结构。 但在RISC-style的ARM处理器上，则没有类似的实现，而是让Hypervisor软件自己来决定哪些信息需要保存和恢复， 这在一定程度上带来了一些灵活性[ Ref1 ]。 2. Memory Virtualization 在ARMv8-A上，每个tarnslation regime可能包括1个stage，也可能包括2个sate。 每个Exception Level都有自己的地址翻译机制，使用不同的页表基地址寄存器，地址翻译可以细分到stage， 大部分的EL包括一个stage的地址翻译过程， Non-Secure EL1&0包括了2个stage的地址翻译过程。 每个stage都有自己独立的一系列Translation tables，每个stage都能独立的enable或者disable。 每个stage都是将输入地址（IA）翻译成输出地址（OA）[ Ref2 ]。 所以在虚拟化场景下，ARM和X86上的方案是类似的，都是采用两阶段地址翻译实现GPA -> HPA的地址翻译过程。 虚拟机运行在None-secure EL1&0，当虚拟机内的进程访问GVA的时候MMU会将GVA翻译成IPA（intermediate physical address，中间物理地址：GPA）， 这就是所谓的stage 1地址翻译。然后MMU会再次将IPA翻译成HPA，这就是所谓的stage 2地址翻译。 在不同的Eexception Level下有不同的Address Space，那么如何去控制不同地址空间的翻译呢？ ARMv8-A上有一个TCR（Translation Control Register）寄存器来控制地址翻译。 例如：对于EL1&0来说，由于在该运行模式下VA存在2个独立的映射空间（User Space和Kernel Space）， 所以需要两套页表来完成地址翻译，这2个页表的及地址分别放在TTBR0_EL1和TTBR1_EL1中。 对于每一个地址翻译阶段: 有一个system control register bit来使能该阶段的地址翻译 有一个system control register bit来决定翻译的时候使用的大小端策略 有一个TCR寄存器来控制整个阶段的地址翻译过程 如果某个地址翻译阶段支持将VA映射到两个subranges，那么该阶段的地址翻译需要为每个VA subrange提供不同的TTBR寄存器 内存虚拟化也没有太多可以说道，理解了原理之后就可以去梳理KVM相关代码，相关代码实现主要在arch/arm/mm/mmu.c里面。 3. I/O Virtualization 设备直通的目的是能够让虚拟机直接访问到物理设备，从而提升IO性能。 在X86上使用VT-d技术就能够实现设备直通，这一切都得益于VFIO驱动和Intel IOMMU的加持。 那么在ARMv8-A上为了支持设备直通，又有哪些不同和改进呢？ 同X86上一样，ARM上的设备直通关键也是要解决DMA重映射和直通设备中断投递的问题。 但和X86上不一样的是，ARMv8-A上使用的是SMMU v3.1来处理设备的DMA重映射， 中断则是使用GICv3中断控制器来完成的，SMMUv3和GICv3在设计的时候考虑了更多跟虚拟化相关的实现， 针对虚拟化场景有一定的改进和优化。 先看下SMMUv3.1的在ARMv8-A中的使用情况以及它为ARM设备直通上做了哪些改进[ Ref3 ]。 SMMUv3规定必须实现的特性有： SMMU支持2阶段地址翻译，这和内存虚拟化场景下MMU支持2阶段地址翻译类似， 第一阶段的地址翻译被用做进程（software entity）之间的隔离或者OS内的DMA隔离， 第二阶段的地址翻译被用来做DMA重映射，即将Guest发起的DMA映射到Guest的地址空间内。 支持16bit的ASIDs 支持16bit的VMIDs 支持SMMU页表共享，允许软件选择一个已经创建好的共享SMMU页表或者创建一个私有的SMMU页表 支持49bit虚拟地址 (matching ARMv8-A's 2×48-bit translation table input sizes)，SMMUv3.1支持52bit VA，IPA，PA SMMUv3支持的可选特性有： Stage1和Stage2同时支持AArch32(LPAE: Large Page Address Extension)和AArch64地址翻译表格式（兼容性考虑） 支持Secure Stream （安全的DMA流传输） 支持SMMU TLB Invalidation广播 支持HTTU(Hardware Translation Table Update)硬件自动刷新页表的Access/Dirty标志位 支持PCIE ATS和PRI（PRI特性非常厉害，后面单独介绍） 支持16K或者64K页表粒度 我们知道，一个平台上可以有多个SMMU设备，每个SMMU设备下面可能连接着多个Endpoint， 多个设备互相之间可能不会复用同一个页表，需要加以区分，SMMU用StreamID来做这个区分， 通过StreamID去索引Stream Table中的STE（Stream Table Entry）。 同样x86上也有类似的区分机制，不同的是x86是使用Request ID来区分的，Request ID默认是PCI设备分配到的BDF号。 不过看SMMUv3 Spec，又有说明：对于PCI设备StreamID就是PCI设备的RequestID， 好吧，两个名词其实表示同一个东西，只是一个是从SMMU的角度去看就成为StreamID，从PCIe的角度去看就称之为RequestID。 同时，一个设备可能被多个进程使用，多个进程有多个页表，设备需要对其进行区分，SMMU使用SubstreamID来对其进行表示。 SubstreamID的概念和PCIe PASID是等效的，这只不过又是在ARM上的另外一种称呼而已。 SubstreamID最大支持20bit和PCIe PASID的最大宽度是一致的。 STE里面都有啥呢？Spec里面有说明： STE里面包含一个指向stage2地址翻译表的指针，并且同时还包含一个指向CD（Context Descriptor）的指针 CD是一个特定格式的数据结构，包含了指向stage1地址翻译表的基地址指针 理论上，多个设备可以关联到一个虚拟机上，所以多个STE可以共享一个stage2的翻译表。 类似的，多个设备(stream)可以共享一个stage1的配置，因此多个STE可以共享同一个CD。 Stream Table是存在内存中的一张表，在SMMU设备初始化的时候由驱动程序创建好。 Stream Table支持2种格式，Linear Stream Table 和 2-level Stream Table， Linear Stream Table就是将整个Stream Table在内存中线性展开为一个数组，优点是索引方便快捷，缺点是当平台上外设较少的时候浪费连续的内存空间。 2-level Stream Table则是将Stream Table拆成2级去索引，优点是更加节省内存。 在使能SMMU两阶段地址翻译的情况下，stage1负责将设备DMA请求发出的VA翻译为IPA并作为stage2的输入， stage2则利用stage1输出的IPA再次进行翻译得到PA，从而DMA请求正确地访问到Guest的要操作的地址空间上。 在stage1地址翻译阶段：硬件先通过StreamID索引到STE，然后用SubstreamID索引到CD， CD里面包含了stage1地址翻译（把进程的GVA/IOVA翻译成IPA）过程中需要的页表基地址信息、per-stream的配置信息以及ASID。 在stage1翻译的过程中，多个CD对应着多个stage1的地址翻译，通过Substream去确定对应的stage1地址翻译页表。 所以，Stage1地址翻译其实是一个（RequestID, PASID） => GPA的映射查找过程。 注意：只有在使能了stage1地址翻译的情况下，SubstreamID才有意义，否则该DMA请求会被丢弃。 在stage2地址翻译阶段：STE里面包含了stage2地址翻译的页表基地址（IPA->HPA）和VMID信息。 如果多个设备被直通给同一个虚拟机，那么意味着他们共享同一个stage2地址翻译页表[ Ref4 ]。 值得注意的是：CD中包含一个ASID，STE中包含了VMID，CD和VMID存在的目的是作为地址翻译过程中的TLB Tag，用来加速地址翻译的过程。 系统软件通过Command Queue和Event Queue来和SMMU打交道，这2个Queue都是循环队列。 系统软件将Command放到队列中SMMU从队列中读取命令来执行，同时设备在进行DMA传输或者配置发生错误的时候会上报事件， 这些事件就存放在Event Queue当中，系统软件要及时从Event Queue中读取事件以防止队列溢出。 SMMU支持两阶段地址翻译的目的只有1个，那就是为了支持虚拟化场景下的SVM特性（Shared Virtual Memory）。 SVM特性允许虚拟机内的进程都能够独立的访问直通给虚拟机的直通设备,在进程自己的地址空间内向设备发起DMA。 SVM使得虚拟机里面的每个进程都能够独立使用某个直通设备，这能够降低应用编程的复杂度，并提升安全性。 为了实现虚拟化场景下的SVM，QEMU需要模拟一个vSMMU（或者叫vIOMMU）的设备。 虚拟机内部进程要访问直通设备的时候，会调用Guest驱动创建PASID Table（虚拟化场景下这个表在Guest内部）， 在这个场景下PASID将作为虚拟机内进程地址空间的一个标志，设备在发起DMA请求的时候会带上PASID Prefix，这样SMMU就知道如何区分了。 创建PASID Table的时候会访问vSMMU，这样Guest就将PASID Table的地址（GPA）传给了QEMU， 然后QEMU再通过VFIO的IOCTL调用（VFIO_DEVICE_BIND_TASK）将表的信息传给SMMU， 这样SMMU就获得了Guest内部进程的PASID Table的shadow信息，它就知道该如何建立Stage1地址翻译表了。 所以，在两阶段地址翻译场景下，Guest内部DMA请求的处理步骤 Step1 : Guest驱动发起DMA请求 ，这个 DMA请求包含GVA + PASID Prefix Step2 ： DMA请求到达SMMU ， SMMU提取DMA请求中的RequestID就知道这个请求是哪个设备发来的 ，然后去 StreamTable索引对应的STE Step3 : 从对应的 STE表中查找到对应的CD ，然后用 PASID到CD中进行索引找到对应的S1 Page Table Step4 ： IOMMU进行S1 Page Table Walk ，将 GVA翻译成GPA （ IPA ）并作为 S2的输入 Step5 ： IOMMU执行S2 Page Table Walk ，将 GPA翻译成HPA ， done ！ 纵观SMMUv3，从设计上来和Intel IOMMU的设计和功能基本类似，毕竟这里没有太多可以创新的地方。 但ARM SMMUv3有2个比较有意思的改进点： 一个是支持Page Request Interface（PRI），PRI是对ATS的进一步改进。当设备支持PRI特性的时候， 设备发送DMA请求的时候可以缺页IOPF(IO Page Fault)，这就意味着直通虚拟机可以不需要进行内存预占， DMA缺页的时候SMMU会向CPU发送一个缺页请求，CPU建立好页表之后对SMMU进行回复，SMMU这时候再将内容写到DMA Buffer中。 另外一个改进就是，DMA写内存之后产生脏页可以由硬件自动更新Access/Dirty Bit， 这样就对直通设备热迁移比较友好，但这个功能是需要厂商选择性支持的， 而且在这种场景下如何解决SMMU和MMU的Cache一致性是最大的挑战。 4. Interrupt Virtualization ARM的中断系统和x86区别比较大，x86用的是IOAPIC/LAPIC中断系统，ARM则使用的是GIC中断控制器， 并且随着ARM的演进陆续出现了GICv2,GICv3,GICv4等不同版本， 看了GICv3手册感觉着玩儿设计得有点复杂，并不像x86上那样结构清晰。 GICv1和GICv2最大只支持8个PE，这放在现在显然不够用了。 所以，GICv3对这里进行改进，提出了 affinity routing 机制以支持更多的PE。 GICv3定义了以下中断类型[ Ref5 ]： ARM上的中断类型： LPI(Locality-specific Peripheral Interrupt) LPI始终是基于消息的中断，边缘触发、经过ITS路由，它们的配置保存在表中而不是寄存器，比如PCIe的MSI/MSI-x中断，GITS_TRANSLATER控制中断 SGI (Software Generated Interrupt) 软件触发的中断，软件可以通过写GICD_SGIR寄存器来触发一个中断事件，一般用于核间通信（对应x86 IPI中断） PPI(Private Peripheral Interrupt) 私有外设中断，这是每个核心私有的中断，PPI太冗长会送达到指定的CPU上，边缘触发或者电平触发、有Active转态，应用场景有CPU本地时钟，类似于x86上的LAPIC Timer Interrupt SPI(Shared Peripheral Interrupt) 公用的外部设备中断，也定义为共享中断，边缘触发或者电平触发、有Active转态，可以多个CPU或者说Core处理，不限定特定的CPU，SPI支持Message格式（GICv3），GICD_SETSPI_NSR设置中断，GICD_CLRSPI_NSR清除中断 ARM上的中断又可以分为两类： 一类中断要通过Distributor分发的，例如SPI中断。 另一类中断不通过Distributor的，例如LPI中断，直接经过ITS翻译后投递给某个Redistributor。 INTID Interrupt Type Notes 0-15 SGI Banked per PE 16-31 PPI Banked per PE 32-1019 SPI 1020-1023 Special Interrupt Number Used to signal special cases 1024-8191 Reserved 8192- LPI ARM上又搞出来一个 Affinity Routing 的概念，GICv3使用 Affinity Routing 来标志一个特定的PE或者是一组特定的PE， 有点类似于x86上的APICID/X2APIC ID机制。ARM使用4个8bit的域来表示affinity，格式如： <affinity level 3>.<affinity level 2>.<affinity level 1>.<affinity level 0> 例如，现在有个ARM Big.Little架构的移动处理器SOC，拥有2个Cluster，小核心拥有4个Cortex-A53大核心拥有2个A72，那么可以表示为： 0.0.0. [ 0 : 3 ] Cores 0 to 3 of a Cortex - A53 processor 0.0.1. [ 0 : 1 ] Cores 0 to 1 of a Cortex - A72 processor GICv3的设计上和x86的IOAPIC/LAPIC架构差异甚远，GICv3的设计架构如下图所示： GICv3中断控制器由Distributor，Redistributor和CPU Interface三个部分组成。 Distributor负责SPI中断管理并将中断发送给Redistributor，Redistributor管理PPI，SGI，LPI中断，并将中断投递给CPU Interface， CPU Interface负责将中断注入到Core里面（CPU Interface本身就在Core内部）。 Distributor的主要功能有： 中断优先级管理和中断分发 启用和禁用SPI 为每个SPI设置设置中断优先级 为每个SPI设置路由信息 设置每个SPI的中断触发属性：边沿触发或者电平触发 生成消息格式的SPI中断 控制SPI中断的active状态和pending状态 每个PE都对应有一个Redistributor与之相连，Distributor的寄存器是memory-mapped， 并且它的配置是全局生效的，直接影响所有的PE。Redistributor的主要功能有： 使能和禁用SGI和PPI 设置SGI和PPI的中断优先级 设置PPI的触发属性：电平触发或者边沿触发 为每个SGI和PPI分配中断组 控制SGI和PPI的状态 控制LPI中断相关数据结构的基地址 对PE的电源管理的支持 每个Redistributor都和一个CPU Interface相连， 在GICv3中CPU Interface的寄存器是是通过System registers(ICC_*ELn)来访问的。 在使用这些寄存器之前软件必须使能系统寄存器，CPU Interface的主要功能： 控制和使能CPU的中断处理。如果中断disable了，即使Distributor分发了一个中断事件到CPU Interface也会被Core屏蔽掉。 应答中断 进行中断优先级检测和中断deassert 为PE设置一个中断优先级mask标志，可以选择屏蔽中断 为PE定义抢占策略 为PE断定当前pending中断的最高优先级（优先级仲裁） GICv3中为了处理LPI中断，专门引入了ITS（Interrupt Translation Service）组件。 外设想发送LPI中断时（比如PCI设备的MSI中断），就去写ITS的寄存器GITS_TRANSLATER，这个写操作就会触发一个LPI中断。 ITS接收到LPI中断后，对其进行解析然后发送给对应的redistributor，然后再由redistributor发送给CPU Interface。 那么这个写操作里面包含了哪些内容呢？主要是2个关键域。 EventID：这个是写入到GITS_TRANSLATER的值，EventID定义了外设要触发的中断号，EventID可以和INTID一样，或者经过ITS翻译后得到一个INTID DeviceID：这个是外设的标志，实现是自定义的，例如可以使用AXI的user信号传递。 ITS使用3种类型的表来完成LPI的翻译和路由： Device Table： 将DeviceID映射到Interrupt Translation Table中 Interrupt Translation Table：包含了EventID到INTID映射关系之间和DeviceID相关的信息，同时也包含了INTID Collection Collection Table：将collections映射到Redistributor上 整个流程大概是： Step1： 外设写GITS_TRANSLATER，ITS使用DeviceID从Device Table中索引出这个外设该使用哪个Interrupt Translation Table Step2： 使用EventID去选中的Interrupt Translation Table中索引出INTID和对应的Collection ID Step3： 使用Collection ID从Collection Table中选择对应的Collection和路由信息 Step4： 把中断送给目标Redistributor 看来看去总觉得GICv3中断控制器设计比较复杂，不如x86上那样结构清晰，目前只是理了个大概，要深入理解再到代码级熟悉还得花不少时间。 上面说了这么多，还是在将GICv3控制器的逻辑，具体QEMU/KVM上是怎么实现的还得去看代码，为了提升中断的性能， GICv3的模拟是直接放到KVM里面实现的。比如说virtio设备的MSI中断，那肯定类型上是LPI中断，QEMU模拟的时候机制上还是使用irqfd方式来实现的， 前面也有从代码角度去分析过，后面再单独从代码层级去分析具体的实现方案。 5. Overview ARM体系结构和x86存在不少差异，其中差异最大的还是中断控制器这块，这里需要投入事件好好分析一下． 内存虚拟化和I/O虚拟化这块二者可能细节上有些不同，但背后的原理还是近似的． 例如：SMMUv3在设计上和Intel IOMMU都支持了二次地址翻译，但SMMU有针对性的改进点． 后面继续努力，慢慢入门学习ARM虚拟化的知识体系． 6. References ARMv8 Architecture Reference Manual ARMv8-A Address Translation Version 1.0 ARM64 Address Translation SMMU architecture version 3.0 and version 3.1 GICv3 Software Overview Official Release SVM on ARM SMMUv3 SVM and PASID","tags":"virtualization"},{"title":"Lightweight Micro Virtual Machines","url":"https://kernelgo.org/microVM.html","loc":"https://kernelgo.org/microVM.html","text":"轻量级虚拟化技术 云计算领域经过近13年的发展后，整个云软件栈已经变得大而全了。 例如：Openstack + KVM解决方案，这套IaaS解决方案已经比较完善了。 以AWS为首的云计算服务提供商为了更细粒度的划分计算资源，提出了Serverless模型。 为了更好地服务Serverless模型，涌现了若干个轻量级虚拟化方案。 这里简单介绍一下当前已有的４种解决方案．它们分别是Firecracker, gVisor, Rust-VMM 和NEMU。 轻量级虚拟化方案的设计理念是围绕着：安全、快速、轻量、高并高密度，这几个共同点展开的。 下面对这几个轻量级虚拟机化方案进行简要的对比介绍。 AWS Firecracker Firecracker由AWS发布并将firecracker开源， 它的定位是面向Serverless计算业务场景。 Firecracker本质上是基于KVM的轻量级的microVM， 可以同时支持多租户容器和FaaS场景。 Security和Fast是firecracker的首要设计目标。 它的设计理念可以概括为： 基于KVM 精简的设备集（极简主义） 基于Rust语言（Builtin Safety） 定制的guest kernel（快速启动） 优化内存开销（使用musl c） Firecracker使用了极为精简的设备模型（仅有几个关键的模拟设备），目的是减少攻击面已提升安全性。 同时这irecracker使用了一个精简的内核（基于Apline Linux），这使得Firecracker可以做在125ms内拉起一个虚拟机。 Firecracker使用musl libc而不是gnu libc，能够将虚拟机的最低内存开销小到5MB。 注：一个Standard MicroVM的规格是1U 128M Fircracker的架构图如下： google gVisor gVisor由Google出品，目的是为了加强Container的隔离性但走的是另外一条路子。 gVisor是Container Runtime Sandbox，本质是一种进程级虚拟化技术，走的是沙箱的路子。 它的设计理念可以概括为： 基于ptrace syscall截获模拟 runsc直接对接Docker & Kubernates 不需要模拟Devices、Interrupts和IO 文件系统隔离机制 gVisor的架构如下图： gVisor由3个组件构成，Runsc、Sentry和Gopher。 Runsc向上提供Docker & Kubernates的接口(OCI)，Sentry截获syscall并进行模拟，Gopher用来做9p将文件系统呈现给沙箱内部App。 gVisor当前的设计模式中带来的问题有： 不适用于intensive syscall场景 对sysfs、procfs支持不完整 目前仅支持240个syscall的模拟 对ARM架构支持还不友好 对KVM的支持尚处于experimental阶段 更多的信息可以参考： https://lwn.net/Articles/754433/ Kubecon gVisor Talk Rust-VMM Rust-VMM是一个新项目，目前还处于开发阶段。它的愿景是帮助社区构建自定义的VMM和Hypervisor。 使得我们可以能够像搭积木一样去构建一个适用于我们自己应用场景的VMM，而不用去重复造轮子。 社区的参与者包括了Alibaba, AWS, Cloud Base, Crowdstrike, Intel, Google, Red Hat等。 它的设计理念是： 合并CrosVM项目和Firecracker项目 提供更安全、更精简的代码集（Build with Rust Language） 采用基于Rust的crates组件开发模式 高度可定制 项目目前在开发阶段，更多的信息见： https://github.com/rust-vmm NEMU NEMU项目由Intel领导开发，重点是面向IaaS云化场景。 NEMU是基于QEMU 4.0进行开发的，有可能会被QEMU社区接收到upstream中。 NEMU的涉及理念可以概括为： 基于QEMU/KVM 精简的设备集（no legacy devices) 新的virt类型主板 优化内存开销 定制的Guest OS(基于Clear Linux） 编译时可裁剪 个人认为NEMU项目如果用来面向Serverless场景是不够精简的， 它的完备设备集还是很大（包含约20种Moderm设备）。 NEMU选择支持seabios和UEFI目的就是要兼容IaaS方案，所以它的代码集还是很大的。 NEMU的做法更像是对QEMU做了一个减法，让QEMU更加聚焦X86和ARM的云OS场景。 NEMU的好处是原生支持设备直通、热迁移等高级特性。","tags":"virtualization"},{"title":"The Rust Programming Language","url":"https://kernelgo.org/rust-lang.html","loc":"https://kernelgo.org/rust-lang.html","text":"1. Rust 编程语言 Rust是一个聚焦安全性的多范式的系统编程语言，尤其是并发安全。 Rust的语法类似c++但在设计的时候考虑了更多的内存安全并且保持了高性能。 Rust语言的精髓深藏在各种语言规则中，这里记录一些Rust语言学习笔记，供自己体会和学习。 2.Rust 语言教程 https://doc.rust-lang.org/book/ https://doc.rust-lang.org/std/index.html#the-rust-standard-library Rust编程之道 张汉东 电子工业出版社 3.Rust 语言特性 所有权准则： Rust中的每个值都有一个与之对应的变量叫做它的所有权； 同一时刻只能有一个所有权； 当所有权走出作用域的时候，值就会被自动丢弃。 可变借用: 在特定的作用域内，任何数据只能有且仅有1个可变引用。 借用准则： 在任何时候对于一个变量，你只能有一个可变的引用 或者 任意数量的不可变引用。 引用都必须是有效的。 当数据被不可变地借用时，它还会冻结（freeze）。已冻结的数据无法通过原始 对象来修改，直到对这些数据的所有引用离开作用域为止。 引用和借用的区别： 引用, 简单而言就是指向一个值所在内存的指针, 这个跟 c++ 里是一致的。如果一个引用作为函数的参数, 它又称为对一个值的借用。 参考 生命周期省略规则： 每个引用类型的参数都有自己的生命周期参数； 如果只有一个生命周期输入参数，那么所有的输出参数都使用这个入参的生命周期； 如果有多个参数的生命周期，那么只能有一个参数是 &self 或者 &mut self 类型，因为这是一个方法。 说明：由于Rust会自动释放作用域内的变量，所以就需要用生命周期来告诉编译器去检查所有的借用都是有效的。 闭包（Lambda 表达式/匿名函数）准则： 使用 || 而不是 ()来包围输入变量； 对于单个表达式的执行体， {} 不是必须要求的； 能够使用外部环境中的变量； 可以自动推断输入和返回值的类型。 匿名函数默认会实现3个特定Trait类型的其中一个：FnOnce/FnMut/Fn 默认情况下它们是交给rust编译器去推理的, 大致的推理原则是: FnOnce: 当指定这个Trait时, 匿名函数内访问的外部变量必须拥有所有权. FnMut: 当指定这个Trait时, 匿名函数可以改变外部变量的值. Fn: 当指定这个Trait时, 匿名函数只能读取(borrow value immutably)变量值. 注意：若使用闭包作为函数参数，由于这个结构体的类型未知，任何的用法都要求是泛型的 Rust核心库： Rust核心库（Rust Core Lib）是标准库（Rust Standard Lib）的基础，不依赖于操作系统，没有堆内存分配。 可以用来构建OS和物联网等嵌入式应用程序。 基础的trait: Copy，Debug，Display，Option等 基本的原始类型：bool，char，i8/u8，i16/u16，i32/u32，i64/u64，isize/uszie，f32/f64，str，array，slice，tuple，pointer 常用的功能数据类型，入String，Vec，HashMap，Rc，Arc，Box等 常用的宏定义：println!，assert！，panic！，vec！等 Rust结构体： 具名结构体（Named-Field Struct） 元组结构体（Tuple-Like Struct） 单元结构体（Unit-Like Struct） Trait类型： trait是Rust唯一的接口抽象方式 可以静态生成，也可以动态调用 可以当做标记类型拥有某些特定行为的\"标签\"来使用 extern crate和use的区别： Rust2018之后不需要显式extern引入crate，编译器会自动引入。 只需要在Cargo.toml中的Dependency加入crate name，然后在源码中use即可。 这里是Rust关于废弃extern crate的声明： https://doc.rust-lang.org/nightly/edition-guide/rust-2018/module-system/path-clarity.html Rust Debug姿势： rust-gdb使用方法： https://bitshifter.github.io/rr+rust/index.html#1 函数入参的生命周期： 任何引用都必须拥有标注好的生命周期。 任何被返回的引用都必须有和某个输入量相同的生命周期或是静态类型（static）。 结构体中标注的生命周期和函数中一致。 语法糖： map如果有值Some(T)会执行f，反之直接返回None。 unwrap_or提供了一个默认值default，当值为None时返回default。 看起来and_then和map差不多，不过map只是把值为Some(t)重新映射了一遍，and_then则会返回另一个Option。 这些个语法糖经常用在错误处理的方面。 参考 关于MutexGard类型： Arc 类型执行了lock.unwrap得到MutexGuard类型，需要解引用或者借用才能取出包装类型； 例如: let mytype: &MyType = &x.lock().unwrap() //借用 或者: let mytype = *x.lock().unwrap() //解引用 Ref Rust Programming Language (wikipedia) Rust closure FnOne/FnMut/Fn 理解Rust生命周期","tags":"Programe Language"},{"title":"Article Archive 2019 Reading Plan","url":"https://kernelgo.org/reading2019.html","loc":"https://kernelgo.org/reading2019.html","text":"计划阅读的文章 mmap详解-1 网络高并发服务器之epoll接口 ARM v8 Architecture: pptx ARM v8A Architecture Reference Manual: pdf VFIO mdev Userspace NVME Driver In QEMU The Raft Consensus Algorithm Linux Page Cache 机制 ARM SMMU Architecture Spec Share Virtual Memory PCI SIG PASID ARM® Generic Interrupt Controller Architecture Specification ARM Virtualization: Performance and Architectural Implications Writing an OS in Rust CPU Cache Coherency Understanding virtualization facilities in the ARMv8 processor architecture ACPI 6.3 Spec ARM CoreLink CCI550 总线协议 GICv3 Software Overview Memory Hot-Add and Hot-remove RAS ARM64 ARM64 PSCI Spec Obstacle & Solution for livepatch on ARM64 XPC: Architectural Support for Secure and Efficient Cross Process Call Intel KVMGT Talk1 ARCN GVT-g Doc KVM for ARM pdf Linux Kernel知识点记录 VGPU ON KVM VFIO BASED MEDIATED DEVICE FRAMEWORK A scala based framework for developing acceleration systems with FPGAs HPCC: High Precision Congestion Control 阿里巴巴 NVME Storage Virtualization Solution with Mediated Pass-Through 已阅读的文章 浅谈可重入函数和不可重入函数 Rust编程之道-张汉东-电子工业出版社 NETLINK_KOBJECT_UEVENT侦听设备热插拔事件的原理 Sandboxing: Seccomp & AppAmour Mastering the DMA and IOMMU SRVM: Hypervisor Support for Live Migration with Passthrough SR-IOV Network Devices, Xin Xu, Bhavesh Davda, VMware Inc","tags":"utils"},{"title":"Virt NMI Emulation","url":"https://kernelgo.org/x86-nmi.html","loc":"https://kernelgo.org/x86-nmi.html","text":"X86 NMI中断 NMI（Nonmaskable Interrupt）中断之所以称之为NMI的原因是：这种类型的中断不能被CPU的EFLAGS寄存器的IF标志位所屏蔽。 而对于可屏蔽中断而言只要IF标志位被清理（例如：CPU执行了cli指令），那么处理器就会禁止INTR Pin和Local APIC上接收到的内部中断请求。 NMI中断有两种触发方式： 外部硬件通过CPU的 NMI Pin 去触发（硬件触发） 软件向CPU系统总线上投递一个NMI类型中断（软件触发） 当CPU从上述两种中断源接收到NMI中断后就立刻调用vector=2（中断向量为2）的中断处理函数来处理NMI中断。 Intel SDM, Volume 3, Chapter 6.7 Nonmaskable Interrupt章节指出： 当一个NMI中断处理函数正在执行的时候，处理器会block后续的NMI直到中断处理函数执行IRET返回。 1.NMI中断的用途 NMI中断的主要用途有两个： 用来告知操作系统有硬件错误（Hardware Failure） 用来做看门狗定时器，检测CPU死锁 除了用来，看门狗定时器在Linux内核中被用来进行死锁检测（Hard Lockup），当CPU长时间不喂狗的时候会触发看门狗超时， 这时候向操作系统注入NMI中断，告知系统异常。 2.NMI中断虚拟化 我们可以通过virsh inject-nmi VMname命令给虚拟机注入NMI中断。 QEMU这边的调用栈为： qmp_inject_nmi => nmi_monitor_handle => nmi_children //传入了struct do_nmi_s ns => do_nmi => nc -> nmi_monitor_handler => x86_nmi => apic_deliver_nmi => kvm_apic_external_nmi => do_inject_external_nmi => kvm_vcpu_ioctl ( cpu , KVM_NMI ) 其中 nmi_children 的设计比较特别，它调用了一个object_child_foreach函数， 会沿着QOM对象树往下遍历，遍历的时候调用 do_nmi 函数。 值得注意的是这里 NMIClass 被设计为一个 interface 类型，而主板类 MachineClass 实现了这个接口。 static const TypeInfo pc_machine_info = { . name = TYPE_PC_MACHINE , . parent = TYPE_MACHINE , . abstract = true , . instance_size = sizeof ( PCMachineState ), . instance_init = pc_machine_initfn , . class_size = sizeof ( PCMachineClass ), . class_init = pc_machine_class_init , . interfaces = ( InterfaceInfo []) { { TYPE_HOTPLUG_HANDLER }, { TYPE_NMI }, { } }, }; 主板类是一个抽象类，实现了 TYPE_HOTPLUG_HANDLER 和 TYPE_NMI 接口，有点Java面向对象的意思。 static int do_nmi ( Object * o , void * opaque ) { struct do_nmi_s * ns = opaque ; NMIState * n = ( NMIState * ) object_dynamic_cast ( o , TYPE_NMI ); // 对象动态转换 if ( n ) { // 如果能够成功转换，说明这个对象实现了 NMI 接口，那么可以调用这个对象的处理函数 NMIClass * nc = NMI_GET_CLASS ( n ); ns -> handled = true ; nc -> nmi_monitor_handler ( n , ns -> cpu_index , & ns -> err ); // nmi_monitor_handler 是NMI接口的方法 if ( ns -> err ) { return - 1 ; } } nmi_children ( o , ns ); return 0 ; } nmi_monitor_handle 函数中调用了nmi_children(object_get_root(), &ns)，从Root Object对象开始向下遍历， 在对象上调用 do_nmi 方法，而 do_nmi 里面会检测这个对象是否实现了 TYPE_NMI 类型的接口， 如果这个对象实现了这个接口，那么调用 mi_monitor_handler 方法来发送NMI中断。这里充分体现了QOM面向对象思想。 在看代码的时候，我们可以找到 pc_machine_class_init 里面注册了 mi_monitor_handler 。 这里还不太理解的是x86_nmi里面会遍历所有的CPU，对每个CPU都注了NMI，有这个必要吗？ static void pc_machine_class_init ( ObjectClass * oc , void * data ) { NMIClass * nc = NMI_CLASS ( oc ); // 把对象转换为NMIClass类型对象 nc -> nmi_monitor_handler = x86_nmi ; // 实现接口方法 } QEMU调用完kvm_vcpu_ioctl(cpu, KVM_NMI)之后就开始进入KVM内核进行NMI中断注入， 毕竟LAPIC和IOAPIC现在都放到KVM模拟来提升中断注入的实时性。 KVM x86 . c kvm_arch_vcpu_ioctl => kvm_vcpu_ioctl_nmi => kvm_inject_nmi kvm_inject_nmi 里面将nmi_queued加1，然后make KVM_REQ_NMI request。 为了防止中断嵌套KVM做了一些额外的处理。 void kvm_inject_nmi ( struct kvm_vcpu * vcpu ) { atomic_inc ( & vcpu -> arch . nmi_queued ); kvm_make_request ( KVM_REQ_NMI , vcpu ); } 这样VCPU在下次VM Exit的时候会check标志位，进行NMI注入。 static int vcpu_enter_guest ( struct kvm_vcpu * vcpu ) { if ( kvm_check_request ( KVM_REQ_NMI , vcpu )) process_nmi ( vcpu ); } // 由于NMI中断不能嵌套，这里做了防呆，第一process_nmi的时候limit=2， static void process_nmi ( struct kvm_vcpu * vcpu ) { unsigned limit = 2 ; /* * x86 is limited to one NMI running, and one NMI pending after it. * If an NMI is already in progress, limit further NMIs to just one. * Otherwise, allow two (and we'll inject the first one immediately). */ if ( kvm_x86_ops -> get_nmi_mask ( vcpu ) || vcpu -> arch . nmi_injected ) limit = 1 ; vcpu -> arch . nmi_pending += atomic_xchg ( & vcpu -> arch . nmi_queued , 0 ); vcpu -> arch . nmi_pending = min ( vcpu -> arch . nmi_pending , limit ); kvm_make_request ( KVM_REQ_EVENT , vcpu ); } static int inject_pending_event ( struct kvm_vcpu * vcpu , bool req_int_win ) { kvm_x86_ops -> set_nmi ( vcpu ); // call vmx_inject_nmi } 最后调用 vmx_inject_nmi 函数注入NMI中断给虚拟机（也是通过写VMCS VM_ENTRY_INTR_INFO_FIELD域来实现）。 3.参考文献 Intel SDM Volume 3, Chapter 6","tags":"virtualization"},{"title":"VIM8 Customized Configuration","url":"https://kernelgo.org/vim8.html","loc":"https://kernelgo.org/vim8.html","text":"是时候秀一下我的VIM8自定义配置了！ 我个人是位忠实的vim用户，在浏览开源软件源代码的时候和在写作代码的时候我大部分时间都在使用vim， 它的很多优点使我很享受软件开发的乐趣，我很喜欢折腾一个自己喜欢的vim配置文件然后用它来工作． VIM8已经发布一两年了，已逐渐趋于稳定，所以是时候舍弃VIM7转投功能强大的VIM8了. VIM8最强大的地方是给我们带来了期待已久的异步任务机制． 异步任务机制的强大之处在于它允许各种插件创建异步任务而不会阻塞当前的编辑， 例如:允许ctags再后台为我们生成和更新符号表，允许ale再后台自动给我们编辑的文件做语法校验。 由于这些过程是异步的，用户感知不到。 折腾了一天之后我终于配置好了一个我心仪的版本，是时候秀一下我的VIM8配置文件了! 1.安装vim8 我们选择从源码安装vim8，因为我们需要让vim8支持python解释器和ruby解释器． git clone https://github.com/vim/vim.git cd vim ./configure --with-features = huge \\ --enable-multibyte \\ --enable-rubyinterp = yes \\ --enable-pythoninterp = yes \\ --with-python-config-dir = /usr/lib/python2.7/config \\ --enable-python3interp = yes \\ --with-python3-config-dir = /usr/lib/python3.5/config \\ --enable-luainterp = yes \\ --enable-gui = gtk2 \\ --with-ruby-command = $( which ruby ) \\ --enable-cscope make -j && sudo make install 2.配置vim8前的准备工作 配置VIM8之前，我们需要先安装一下新的插件管理器 plug.vim ． curl -fLo ~/.vim/autoload/plug.vim --create-dirs \\ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim 或者 mkdir -pv ~/.vim/autoload wget https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim -O ~/.vim/autoload/plug.vim 安装 GNU Global和 Universal ctags git clone https: //github.com/universal-ctags/ctags.git cd ctags sh autogen . sh . / configure make - j sudo make install wget http: //tamacom.com/global/global-6.6.3.tar.gz --no-check-certificate tar xf global - 6.6.3 . tar . gz cd global - 6.6.3 sed - i \"s/(int i = 0;/(i = 0;/g\" gtags - cscope / find . c sed - i \"/regex_t reg;/a\\int i;\" gtags - cscope / find . c . / configure make - j sudo make install pip install pygments # 这个一定要安装！！！ 3.配置vim8的插件系统 先配置一下 plug.vim ，然后让 plug.vim 帮助我们自动管理插件，编辑~/.vimrc文件． \" Specify a directory for plugins \" - For Neovim: ~/.local/share/nvim/plugged \" - Avoid using standard Vim directory names like 'plugin' call plug # begin ( ' ~/.vim/plugged ' ) \" All of your Plugs must be added before the following line call plug # end () \" required plug#begin到plug#end之间的内容是受 plug.vim 管理的，我们大部分的配置文件都放在这里． 下面开始安装插件： if exists ( '&colorcolumn' ) set colorcolumn = 80 endif set paste syntax on \" syntax highlight set nu set nocompatible \" be iMproved , required filetype off \" required set t_Co=256 set backspace=indent,eol,start call plug#begin('~/.vim/plugged') \" basic plug Plug 'tpope/vim-fugitive' \" git support Plug 'vim-scripts/L9' Plug 'rstacruz/sparkup', {'rtp': 'vim/'} \" utils Plug 'vim-scripts/DrawIt' Plug 'mbriggs/mark.vim' Plug 'vim-scripts/tabbar' Plug 'wesleyche/Trinity' Plug 'vim-scripts/Smart-Tabs' Plug 'nvie/vim-togglemouse' Plug 'tpope/vim-unimpaired' Plug 'scrooloose/syntastic' Plug 'bronson/vim-trailing-whitespace' Plug 'tpope/vim-surround' Plug 'junegunn/vim-easy-align' Plug 'Lokaltog/vim-easymotion' Plug 'Yggdroot/indentLine' Plug 'itchyny/lightline.vim' \" YCM Plug 'Valloric/YouCompleteMe' \" file lookup Plug 'vim-scripts/command-t' Plug 'Yggdroot/LeaderF' , { 'do': './install.sh' } \" async grama check \" Plug 'w0rp/ale' Plug 'skywind3000/asyncrun.vim' \" language specific enhance Plug 'vim-scripts/c.vim' Plug 'vim-scripts/a.vim' Plug 'octol/vim-cpp-enhanced-highlight' Plug 'jnwhiteh/vim-golang' Plug 'rust-lang/rust.vim' Plug 'pangloss/vim-javascript' \" color \"Plug 'sunuslee/vim-plugin-random-colorscheme-picker' Plug 'altercation/vim-colors-solarized' Plug 'crusoexia/vim-monokai' Plug 'flazz/vim-colorschemes' \" vim colorschemes Plug 'rafi/awesome-vim-colorschemes' \" vim colorschemes Plug 'lifepillar/vim-solarized8' \" solarized8 \" gtags and gnu global support Plug 'vim-scripts/gtags.vim' Plug 'vim-scripts/autopreview' Plug 'vim-scripts/genutils' Plug 'ludovicchabant/vim-gutentags' Plug 'skywind3000/gutentags_plus' \" gutentags config set cscopeprg='gtags-cscope' set tags=./.tags ;. tags let $ GTAGSLABEL = 'native' let $ GTAGSCONF = '/usr/local/share/gtags/gtags.conf' let g : gutentags_project_root = [ '.git' , '.root' , '.svn' , '.hg' , '.project' ] let g : gutentags_ctags_tagfile = '.tags' let g : gutentags_modules = [] if executable ( 'gtags-cscope' ) && executable ( 'gtags' ) let g : gutentags_modules += [ 'gtags_cscope' ] endif if executable ( 'ctags' ) let g : gutentags_modules += [ 'ctags' ] endif let g : gutentags_cache_dir = expand ( '~/.cache/tags' ) let g : gutentags_ctags_extra_args = [] let g : gutentags_ctags_extra_args = [ '--fields=+niazS' , '--extra=+q' ] let g : gutentags_ctags_extra_args += [ '--c++-kinds=+px' ] let g : gutentags_ctags_extra_args += [ '--c-kinds=+px' ] let g : gutentags_ctags_extra_args += [ '--output-format=e-ctags' ] let g : gutentags_auto_add_gtags_cscope = 0 let g : gutentags_plus_switch = 1 let g : asyncrun_bell = 1 let g : gutentags_define_advanced_commands = 1 let g : gutentags_generate_on_empty_buffer = 1 \" open database \" let g : gutentags_trace = 1 Plug 'skywind3000/vim-preview' \"press shift + p to Preview, press p to close autocmd FileType qf nnoremap <silent><buffer> p :PreviewQuickfix<cr> autocmd FileType qf nnoremap <silent><buffer> P :PreviewClose<cr> noremap <Leader>u :PreviewScroll -1<cr> \" pageup noremap < leader > d : PreviewScroll + 1 < cr > \" pagedown noremap <silent> <leader>gs :GscopeFind s <C-R><C-W><cr> noremap <silent> <leader>gg :GscopeFind g <C-R><C-W><cr> noremap <silent> <leader>gc :GscopeFind c <C-R><C-W><cr> noremap <silent> <leader>gt :GscopeFind t <C-R><C-W><cr> noremap <silent> <leader>ge :GscopeFind e <C-R><C-W><cr> noremap <silent> <leader>gf :GscopeFind f <C-R>=expand(\" < cfile> \")<cr><cr> noremap <silent> <leader>gi :GscopeFind i <C-R>=expand(\" < cfile> \")<cr><cr> noremap <silent> <leader>gd :GscopeFind d <C-R><C-W><cr> noremap <silent> <leader>ga :GscopeFind a <C-R><C-W><cr> \" LeaderF let g : Lf_ShortcutF = '<c-p>' noremap < Leader > ff : LeaderfFunction < cr > noremap < Leader > fb : LeaderfBuffer < cr > noremap < Leader > ft : LeaderfTag < cr > noremap < Leader > fm : LeaderfMru < cr > noremap < Leader > fl : LeaderfLine < cr > let g : Lf_StlSeparator = { 'left': '' , 'right': '' , 'font': '' } let g : Lf_RootMarkers = [ '.project' , '.root' , '.svn' , '.git' ] let g : Lf_WorkingDirectoryMode = 'Ac' let g : Lf_WindowHeight = 0.30 let g : Lf_CacheDirectory = expand ( '~/.vim/cache' ) let g : Lf_ShowRelativePath = 0 let g : Lf_HideHelp = 1 let g : Lf_StlColorscheme = 'powerline' let g : Lf_PreviewResult = { 'Function': 0 , 'BufTag': 0 } let g : Lf_NormalMap = { \\ \"File\" : [[ \"<ESC>\" , ':exec g:Lf_py \"fileExplManager.quit()\"<CR>' ]], \\ \"Buffer\" : [[ \"<ESC>\" , ':exec g:Lf_py \"bufExplManager.quit()\"<CR>' ]], \\ \"Mru\" : [[ \"<ESC>\" , ':exec g:Lf_py \"mruExplManager.quit()\"<CR>' ]], \\ \"Tag\" : [[ \"<ESC>\" , ':exec g:Lf_py \"tagExplManager.quit()\"<CR>' ]], \\ \"Function\" : [[ \"<ESC>\" , ':exec g:Lf_py \"functionExplManager.quit()\"<CR>' ]], \\ \"Colorscheme\" : [[ \"<ESC>\" , ':exec g:Lf_py \"colorschemeExplManager.quit()\"<CR>' ]], \\ } \" latex support Plug 'lervag/vimtex' let g:tex_flavor='latex' let g:vimtex_view_method='zathura' let g:vimtex_quickfix_mode=0 set conceallevel=1 let g:tex_conceal='abdmg' \" UltiSnips Plug 'sirver/ultisnips' let g : UltiSnipsExpandTrigger = '<tab>' let g : UltiSnipsJumpForwardTrigger = '<tab>' let g : UltiSnipsJumpBackwardTrigger = '<s-tab>' \" All of your Plugs must be added before the following line call plug#end() \" required filetype plugin indent on \" required \" To ignore plugin indent changes , instead use : \"filetype plugin on \" Put your non - Plug stuff after this line set tabstop = 8 set softtabstop = 8 set shiftwidth = 4 \"set expandtab set hls set encoding=utf-8 set listchars=tab:>-,trail:- \" set F5 , F6 to find function and symbol nnoremap < F5 > : GscopeFind gs nnoremap < F9 > : GscopeFind gg nnoremap < F4 > :ccl < CR > nnoremap < F2 > :let g : gutentags_trace = 1 < CR > nnoremap < F3 > :let g : gutentags_trace = 0 < CR > \" color desert color Tomorrow - Night - Bright 更新配置文件之后调用 vim +PlugInstall +qall 安装全部插件，执行这个命令Plug会自动下载全部的插件到vim插件目录下。 vim +PlugInstall +qall 这里最重要的一组插件是 vim-gutentags.vim 和 gutentags_plus.vim ， 通过这两个插件配合安装GNU Global和 universal ctags工具， 可以为我们自动异步生成项目的符号表， 可以很方便地查找符号定义和调用关系， 再也不用我们去手动生成和维护tags更新了． 具体配置方式参考: https://zhuanlan.zhihu.com/p/36279445 YCM插件需要自己去编译一下，具体步骤见： https://github.com/ycm-core/YouCompleteMe#installation 其他插件的搭配也非常合理和高效，具体的使用方式参考该插件的help doc. 不早了，洗洗睡了！","tags":"utils"},{"title":"VT-d Interrupt Posting Code Analysis","url":"https://kernelgo.org/vtd-posted-interrupt-code-analysis.html","loc":"https://kernelgo.org/vtd-posted-interrupt-code-analysis.html","text":"VT-d Posted Interrupt 代码分析 Posted Interrupt是基于Interrupt Remapping机制实现的，关于VT-d Posted Interrupt的原理可以参考 VT-d Posted Interrupt ，建议先了解原理再来看代码分析。 分析VT-d Posted Interrupt代码的代码需要从vCPU调度入手，为了实现中断的直接投递和中断迁移， 在vCPU调度时候VMM需要为Posted Interrupt做一些额外的工作，但这些额外的工作带来的中断实时性提升是可观的。 per-vCPU Posted Interrupt Descriptor 为了支持VT-d Posted Interrup Inter为vCPU引入了Posted Interrupt Descriptor数据结构，其中有pir，on,sn,nv,ndst等几个关键域。 PIR域记录了要给虚拟机vCPU投递的vector号（由硬件自动写入并由VMM软件读取）， 当中断到来时ON标志位自动置位告知guest我有中断要投递给你了， SN标志位是VMM软件用来告知VT-d硬件当前vCPU不在Running状态你不要给我投中断了我收不到， NV是主机上配合Poste Interrupt工作的一个中断vector（它的值只能是wakeup_vector或者notification vector）， NDST存放当前vCPU所在PCPU的apicid（由VMM负责刷新，确保中断可以自动迁移到目的pCPU上）。 /* Posted-Interrupt Descriptor */ struct pi_desc { u32 pir [ 8 ]; /* Posted interrupt requested */ union { struct { /* bit 256 - Outstanding Notification */ u16 on : 1 , /* bit 257 - Suppress Notification */ sn : 1 , /* bit 271:258 - Reserved */ rsvd_1 : 14 ; /* bit 279:272 - Notification Vector */ u8 nv ; /* bit 287:280 - Reserved */ u8 rsvd_2 ; /* bit 319:288 - Notification Destination */ u32 ndst ; }; u64 control ; }; u32 rsvd [ 6 ]; } __aligned ( 64 ); 首先要明确 pi_desc 是percpu的，所以在struct vcpu_vmx 里面会包含一个 pi_desc 数据结构。 struct vcpu_vmx { /* Posted interrupt descriptor */ struct pi_desc pi_desc ; } vCPU创建的时候会将NV置成POSTED_INTR_VECTOR也就是notification event的中断号，同时把SN置1（因为这时候vCPU还没有运行）。 kvm_vm_ioctl_create_vcpu => kvm_arch_vcpu_create => vmx_vcpu_create ，这里会注册vCPU的preempt notifier， 当调度器选中vCPU线程的时候VMM会收到通知，VMM调用回调函数进行处理。 static struct kvm_vcpu * vmx_create_vcpu ( struct kvm * kvm , unsigned int id ) { preempt_notifier_init ( & vcpu -> preempt_notifier , & kvm_preempt_ops ); #注册 vcpu的preempt notifier /* * Enforce invariant: pi_desc.nv is always either POSTED_INTR_VECTOR * or POSTED_INTR_WAKEUP_VECTOR. */ vmx -> pi_desc . nv = POSTED_INTR_VECTOR ; vmx -> pi_desc . sn = 1 ; } 同时 kvm_vm_ioctl_create_vcpu => kvm_arch_vcpu_setup => vcpu_load , vcpu_put 会对pi_desc做一些修改， 后面结合虚拟机vCPU调度进行代码分析。 vCPU调度与VT-d Posted Interrupt vCPU的运行状态主要有3种： Running 状态：vCPU正处于非根模式下运行 Runnable 状态：vCPU线程被抢占或者时间片到期，等待OS的下一次调度 Blocked 状态： vCPU执行hlt指令后从非根模式block出来准备休眠的状态 vCPU调度就是指在VMM的管理下虚拟机的vCPU线程在这几种状态之间切换的场景， 针对不同的状态转变VMM会干预进来为Posted Interrupt做一些额外的工作以确保中断自动迁移可以顺利进行。 vCPU 从 Runnable => Running 当vCPU被调度器选中运行之前会调用VMM的回调函数，在kvm中这个函数时 kvm_sched_in 。 static void kvm_sched_in ( struct preempt_notifier * pn , int cpu ) { struct kvm_vcpu * vcpu = preempt_notifier_to_vcpu ( pn ); if ( vcpu -> preempted ) vcpu -> preempted = false ; #将 vcpu被抢占的标志位清零 kvm_arch_sched_in ( vcpu , cpu ); #调整一下 ple window #将VMCS加载到pCPU上准备运行了（这里可能是调度到其他pCPU上运行，也可能是继续在原来pCPU上运行） kvm_arch_vcpu_load ( vcpu , cpu ); } kvm_sched_in => kvm_arch_vcpu_load => vmx_vcpu_load => vmx_vcpu_pi_load ， vCPU要从Runnable状态切换到Running状态了， 这时候要:刷新NDST为vCPU要运行到的pCPU的apic id，并设置SN=0（告知硬件我现在可以接收Posted Interrupt了）。 static void vmx_vcpu_pi_load ( struct kvm_vcpu * vcpu , int cpu ) { struct pi_desc * pi_desc = vcpu_to_pi_desc ( vcpu ); struct pi_desc old , new ; unsigned int dest ; /* * In case of hot-plug or hot-unplug, we may have to undo * vmx_vcpu_pi_put even if there is no assigned device. And we * always keep PI.NDST up to date for simplicity: it makes the * code easier, and CPU migration is not a fast path. */ if ( ! pi_test_sn ( pi_desc ) && vcpu -> cpu == cpu ) return ; /* * First handle the simple case where no cmpxchg is necessary; just * allow posting non-urgent interrupts. * * If the 'nv' field is POSTED_INTR_WAKEUP_VECTOR, do not change * PI.NDST: pi_post_block will do it for us and the wakeup_handler * expects the VCPU to be on the blocked_vcpu_list that matches * PI.NDST. */ if ( pi_desc -> nv == POSTED_INTR_WAKEUP_VECTOR || vcpu -> cpu == cpu ) { pi_clear_sn ( pi_desc ); return ; } /* The full case. */ do { old . control = new . control = pi_desc -> control ; dest = cpu_physical_id ( cpu ); if ( x2apic_enabled ()) new . ndst = dest ; else new . ndst = ( dest << 8 ) & 0xFF00 ; new . sn = 0 ; } while ( cmpxchg64 ( & pi_desc -> control , old . control , new . control ) != old . control ); } vCPU 从 Running => Runnable 当vCPU被抢占或者时间片到期的时候vCPU被调度出来，这时候会触发回调函数 kvm_sched_out 。 static void kvm_sched_out ( struct preempt_notifier * pn , struct task_struct * next ) { struct kvm_vcpu * vcpu = preempt_notifier_to_vcpu ( pn ); if ( current -> state == TASK_RUNNING ) vcpu -> preempted = true ; #置上 vcpu被抢占标志位 #将vCPU的VMCS从当前pCPU上拿下来，并且保存一下vCPU的相关信息到VMCS中 kvm_arch_vcpu_put ( vcpu ); } kvm_sched_out => vmx_vcpu_put => vmx_vcpu_pi_put ，这里vCPU要被调度出来的， 那么要把SN bit置位（中断抑制），告诉硬件我不在运行了，先别给我投递中断，我暂时无法处理。 static void vmx_vcpu_pi_put ( struct kvm_vcpu * vcpu ) { struct pi_desc * pi_desc = vcpu_to_pi_desc ( vcpu ); if ( ! kvm_arch_has_assigned_device ( vcpu -> kvm ) || ! irq_remapping_cap ( IRQ_POSTING_CAP ) || ! kvm_vcpu_apicv_active ( vcpu )) return ; /* Set SN when the vCPU is preempted */ if ( vcpu -> preempted ) pi_set_sn ( pi_desc ); # set SN bit here } vCPU 从 Running => Blocked 当vCPU在Running状态下非根模式执行hlt指令后会被VMM截获发生VM Exit（肯定不能让vCPU在非根模式下中止，这样会浪费CPU资源）， 这时候会调用 vcpu_block 函数来处理。 static inline int vcpu_block ( struct kvm * kvm , struct kvm_vcpu * vcpu ) { if ( ! kvm_arch_vcpu_runnable ( vcpu ) && ( ! kvm_x86_ops -> pre_block || kvm_x86_ops -> pre_block ( vcpu ) == 0 )) { srcu_read_unlock ( & kvm -> srcu , vcpu -> srcu_idx ); kvm_vcpu_block ( vcpu ); vcpu -> srcu_idx = srcu_read_lock ( & kvm -> srcu ); if ( kvm_x86_ops -> post_block ) kvm_x86_ops -> post_block ( vcpu ); if ( ! kvm_check_request ( KVM_REQ_UNHALT , vcpu )) return 1 ; } kvm_apic_accept_events ( vcpu ); switch ( vcpu -> arch . mp_state ) { case KVM_MP_STATE_HALTED : vcpu -> arch . pv . pv_unhalted = false ; vcpu -> arch . mp_state = KVM_MP_STATE_RUNNABLE ; case KVM_MP_STATE_RUNNABLE : vcpu -> arch . apf . halted = false ; break ; case KVM_MP_STATE_INIT_RECEIVED : break ; default : return - EINTR ; break ; } return 1 ; } vcpu_block细分为3个阶段Pre Block, Block 和 Post Block。Pre Block阶段会调用 pi_pre_block ， 这里会将vCPU添加到一个per pCPU的等待链表（waiting list）上， 这个链表记录了所有在这个pCPU上休眠的vCPU列表，然后更新NDST域。 static int pi_pre_block ( struct kvm_vcpu * vcpu ) { unsigned int dest ; struct pi_desc old , new ; struct pi_desc * pi_desc = vcpu_to_pi_desc ( vcpu ); # 虚拟机没有配置直通设备 || 不支持Posted Interrupt => 直接返回 if ( ! kvm_arch_has_assigned_device ( vcpu -> kvm ) || ! irq_remapping_cap ( IRQ_POSTING_CAP ) || ! kvm_vcpu_apicv_active ( vcpu )) return 0 ; # 关中断， 将当前vCPU线程加入到上次运行的pCPU的等待列表中 WARN_ON ( irqs_disabled ()); local_irq_disable (); if ( ! WARN_ON_ONCE ( vcpu -> pre_pcpu != -1 )) { vcpu -> pre_pcpu = vcpu -> cpu ; spin_lock ( & per_cpu ( blocked_vcpu_on_cpu_lock , vcpu -> pre_pcpu )); list_add_tail ( & vcpu -> blocked_vcpu_list , & per_cpu ( blocked_vcpu_on_cpu , vcpu -> pre_pcpu )); spin_unlock ( & per_cpu ( blocked_vcpu_on_cpu_lock , vcpu -> pre_pcpu )); } #刷新NDST，更新NV为wakeup vector do { old . control = new . control = pi_desc -> control ; WARN (( pi_desc -> sn == 1 ), \"Warning: SN field of posted-interrupts \" \"is set before blocking \\n \" ); /* * Since vCPU can be preempted during this process, * vcpu->cpu could be different with pre_pcpu, we * need to set pre_pcpu as the destination of wakeup * notification event, then we can find the right vCPU * to wakeup in wakeup handler if interrupts happen * when the vCPU is in blocked state. */ dest = cpu_physical_id ( vcpu -> pre_pcpu ); if ( x2apic_enabled ()) new . ndst = dest ; else new . ndst = ( dest << 8 ) & 0xFF00 ; /* set 'NV' to 'wakeup vector' */ new . nv = POSTED_INTR_WAKEUP_VECTOR ; } while ( cmpxchg64 ( & pi_desc -> control , old . control , new . control ) != old . control ); #如果在pre block阶段收到了中断，那么就不block了，直接转导Runnable状态去 /* We should not block the vCPU if an interrupt is posted for it. */ if ( pi_test_on ( pi_desc ) == 1 ) __pi_post_block ( vcpu ); local_irq_enable (); return ( vcpu -> pre_pcpu == -1 ); } Pre Block阶段过后会调用 kvm_vcpu_block ，在这个函数中会调用schdule()主动把vCPU调度出去（休眠），让出pCPU执行其他vCPU的代码。 vCPU 从 Blocked => Runnable 当vCPU休眠结束之后会调用 vmx_post_block => __pi_post_block 这时候vCPU结束睡眠被重新调度。 注意这里会更新NDST并将vCPU从pCPU等待链表上删除，并且把NV置位 POSTED_INTR_VECTOR 。 static void __pi_post_block ( struct kvm_vcpu * vcpu ) { struct pi_desc * pi_desc = vcpu_to_pi_desc ( vcpu ); struct pi_desc old , new ; unsigned int dest ; #再度更新NDST，因为block睡眠之后被再调度出来执行的时候可能换了pCPU！ do { old . control = new . control = pi_desc -> control ; WARN ( old . nv != POSTED_INTR_WAKEUP_VECTOR , \"Wakeup handler not enabled while the VCPU is blocked \\n \" ); dest = cpu_physical_id ( vcpu -> cpu ); if ( x2apic_enabled ()) new . ndst = dest ; else new . ndst = ( dest << 8 ) & 0xFF00 ; /* set 'NV' to 'notification vector' */ new . nv = POSTED_INTR_VECTOR ; } while ( cmpxchg64 ( & pi_desc -> control , old . control , new . control ) != old . control ); #将vCPU从等待列表中删除掉 if ( ! WARN_ON_ONCE ( vcpu -> pre_pcpu == -1 )) { spin_lock ( & per_cpu ( blocked_vcpu_on_cpu_lock , vcpu -> pre_pcpu )); list_del ( & vcpu -> blocked_vcpu_list ); spin_unlock ( & per_cpu ( blocked_vcpu_on_cpu_lock , vcpu -> pre_pcpu )); vcpu -> pre_pcpu = -1 ; } } 剩下一种状态转换路径 vCPU从 Runable => Blocked状态，这和从Running状态切换成Blocked状态一致，这里不再赘述！","tags":"virtualization"},{"title":"VT-d Interrupt Remapping Code Analysis","url":"https://kernelgo.org/vtd_interrupt_remapping_code_analysis.html","loc":"https://kernelgo.org/vtd_interrupt_remapping_code_analysis.html","text":"VT-d 中断重映射分析 本文中我们将一起来分析一下VT-d中断重映射的代码实现， 在看本文前建议先复习一下VT-d中断重映射的原理，可以参考 VT-D Interrupt Remapping 这篇文章。 看完中断重映射的原理我们必须明白：直通设备的中断是无法直接投递到Guest中的，需要先将其中断映射到host的某个中断上，然后再重定向（由VMM投递）到Guest内部． 我们将从 1.中断重映射Enable 2.中断重映射实现 3.中断重映射下中断处理流程 这3个层面去分析VT-d中断重映射的代码实现。 1.中断重映射Enable 当BIOS开启VT-d特性之后，操作系统初始化的时候会通过cpuid去检测硬件平台是否支持VT-d Interrupt Remapping能力， 然后做一些初始化工作后将操作系统的中断处理方式更改为Interrupt Remapping模式。 start_kernel --> late_time_init --> x86_late_time_init --> x86_init . irqs . intr_mode_init () --> apic_intr_mode_init --> default_setup_apic_routing --> enable_IR_x2apic --> irq_remapping_prepare # Step1 : 使能 Interrupt Reampping --> intel_irq_remap_ops . prepare () --> remap_ops = & intel_irq_remap_ops --> irq_remapping_enable # Step2 : 做一些工作 --> remap_ops -> enable () 从代码堆栈可以看到内核初始化的时候会初始化中断，在default_setup_apic_routing中会分2个阶段对Interrupt Remapping进行Enable。 这里涉及到一个关键的数据结构intel_irq_remap_ops，它是Intel提供的Intel CPU平台的中断重映射驱动方法集。 struct irq_remap_ops intel_irq_remap_ops = { . prepare = intel_prepare_irq_remapping , . enable = intel_enable_irq_remapping , . disable = disable_irq_remapping , . reenable = reenable_irq_remapping , . enable_faulting = enable_drhd_fault_handling , . get_ir_irq_domain = intel_get_ir_irq_domain , . get_irq_domain = intel_get_irq_domain , }; 阶段1调用intel_irq_remap_ops的prepare方法。该方法主要做的事情有： 调用了dmar_table_init从ACPI表中解析了DMAR Table关键信息。 关于VT-d相关的ACPI Table信息可以参考 VT-D Spec Chapter 8 BIOS Consideration 和 Introduction to Intel IOMMU 这篇文章。 遍历每个iommu检查是否支持中断重映射功能（dmar_ir_support）。 调用intel_setup_irq_remapping为每个IOMMU分配中断重映射表（Interrupt Remapping Table）。 static int intel_setup_irq_remapping ( struct intel_iommu * iommu ) { ir_table = kzalloc ( sizeof ( struct ir_table ), GFP_KERNEL ); //为IOMMU申请一块内存，存放ir_table和对应的bitmap pages = alloc_pages_node ( iommu -> node , GFP_KERNEL | __GFP_ZERO , INTR_REMAP_PAGE_ORDER ); bitmap = kcalloc ( BITS_TO_LONGS ( INTR_REMAP_TABLE_ENTRIES ), sizeof ( long ), GFP_ATOMIC ); // 创建ir_domain和ir_msi_domain iommu -> ir_domain = irq_domain_create_hierarchy ( arch_get_ir_parent_domain (), 0 , INTR_REMAP_TABLE_ENTRIES , fn , & intel_ir_domain_ops , iommu ); irq_domain_free_fwnode ( fn ); iommu -> ir_msi_domain = arch_create_remap_msi_irq_domain ( iommu -> ir_domain , \"INTEL-IR-MSI\" , iommu -> seq_id ); ir_table -> base = page_address ( pages ); ir_table -> bitmap = bitmap ; iommu -> ir_table = ir_table ; iommu_set_irq_remapping //最后将ir_table地址写入到寄存器中并最后enable中断重映射能力 } 阶段2调用irq_remapping_enable中判断Interrupt Remapping是否Enable如果还没有就Enable一下，然后set_irq_posting_cap设置Posted Interrupt Capability等。 2.中断重映射实现 要使得直通设备的中断能够工作在Interrupt Remapping模式下VFIO中需要做很多的准备工作． 首先，QEMU会通过PCI配置空间向操作系统呈现直通设备的MSI/MSI-X Capability， 这样Guest OS加载设备驱动程序时候会尝试去Enable直通设备的MSI/MSI-X中断． 为了方便分析代码流程这里以MSI中断为例。 Guest OS设备驱动尝试写配置空间来Enable设备中断，这时候会访问设备PCI配置空间发生VM Exit被QEMU截获处理． 对于MSI中断Enable会调用 vfio_msi_enable 函数． 从PCI Local Bus Spec 可以知道MSI中断的PCI Capability为 xxx QEMU Code : static void vfio_msi_enable ( VFIOPCIDevice * vdev ) { int ret , i ; vfio_disable_interrupts ( vdev ); #先 disable设备中断 vdev -> nr_vectors = msi_nr_vectors_allocated ( & vdev -> pdev ); #从设备配置空间读取设备 Enable的MSI中断数目 vdev -> msi_vectors = g_new0 ( VFIOMSIVector , vdev -> nr_vectors ); for ( i = 0 ; i < vdev -> nr_vectors ; i ++ ) { VFIOMSIVector * vector = & vdev -> msi_vectors [ i ]; vector -> vdev = vdev ; vector -> virq = -1 ; vector -> use = true ; if ( event_notifier_init ( & vector -> interrupt , 0 )) { error_report ( \"vfio: Error: event_notifier_init failed\" ); } qemu_set_fd_handler ( event_notifier_get_fd ( & vector -> interrupt ), // 绑定irqfd的处理函数 vfio_msi_interrupt , NULL , vector ); // 将中断信息刷新到kvm irq routing table里，其实就是建立起gsi和irqfd的映射关系 vfio_add_kvm_msi_virq ( vdev , vector , i , false ); } /* Set interrupt type prior to possible interrupts */ vdev -> interrupt = VFIO_INT_MSI ; // 使能msi中断！！！重点分析 ret = vfio_enable_vectors ( vdev , false ); .... } vfio_msi_enable 的主要流程是：从配置空间查询支持的中断数目 --> 对每个MSI中断进行初始化（分配一个irqfd） --> 将MSI gsi信息和irqfd绑定并刷新到中断路由表中 --> 使能中断（调用vfio-pci内核ioctl为MSI中断申请irte并刷新中断路由表表项）。 QEMU Code : vfio_pci_write_config --> vfio_msi_enable --> vfio_add_kvm_msi_virq --> kvm_irqchip_add_msi_route #为 MSI中断申请gsi ，并更新 irq routing table （ QEMU里面有一份Copy ） --> kvm_irqchip_commit_routes --> kvm_irqchip_add_irqfd_notifier_gsi #将 irqfd和gsi映射信息注册到kvm内核模块中fd = kvm_interrupt , gsi = virq , flags = 0 , rfd = NULL --> kvm_vm_ioctl ( s , KVM_IRQFD , & irqfd ) --> vfio_enable_vectors --> ioctl ( vdev -> vbasedev . fd , VFIO_DEVICE_SET_IRQS , irq_set ) #调用 vfio - pci内核接口使能中断 ，重点分析！ kvm_irqchip_commit_routes的逻辑比较简单这里跳过，kvm_irqchip_add_irqfd_notifier_gsi的逻辑稍微有些复杂后面专门写一篇来分析， 只需要知道这里是将gsi和irqfd信息注册到内核模块中（KVM irqfd提供了一种中断注入机制）并且可以在这个fd上监听事件来达到中断注入的目的。 这里重点分析 vfio_enable_vectors 的代码流程。 Kernel Code : vfio_enable_vectors --> vfio_pci_ioctl // irq_set 传入了一个irqfd数组 --> vfio_pci_set_irqs_ioctl --> vfio_pci_set_msi_trigger --> vfio_msi_enable # Step1 ：为 MSI中断申请Host IRQ ，这里会一直调用到 Interrupt Remapping框架分配IRTE --> pci_alloc_irq_vectors --> vfio_msi_set_block # Step2 ：这里绑定 irqfd ，建立好中断注入通道 --> vfio_msi_set_vector_signal vfio_pci_set_msi_trigger 函数中主要有2个关键步骤。 vfio_msi_enable vfio_msi_enable -> pci_alloc_irq_vectors -> pci_alloc_irq_vectors_affinity -> __pci_enable_msi_range -> msi_capability_init -> pci_msi_setup_msi_irqs -> arch_setup_msi_irqs -> x86_msi.setup_msi_irqs -> native_setup_msi_irqs -> msi_domain_alloc_irqs -> __irq_domain_alloc_irqs,irq_domain_activate_irq 这里内核调用栈比较深，我们只需要知道 vfio_msi_enable 最终调用到了 __irq_domain_alloc_irqs -> intel_irq_remapping_alloc . 在 intel_irq_remapping_alloc 中申请这个中断对应的IRTE。这里先调用的 alloc_irte 函数返回irte在中断重映射表中的index号（即中断重映射表的索引号），再调用 intel_irq_remapping_prepare_irte 去填充irte。 static int intel_irq_remapping_alloc ( struct irq_domain * domain , unsigned int virq , unsigned int nr_irqs , void * arg ) { index = alloc_irte ( iommu , virq , & data -> irq_2_iommu , nr_irqs ); #向 Interrupt Remapping Table申请index for ( i = 0 ; i < nr_irqs ; i ++ ) { irq_data = irq_domain_get_irq_data ( domain , virq + i ); irq_cfg = irqd_cfg ( irq_data ); irq_data -> hwirq = ( index << 16 ) + i ; irq_data -> chip_data = ird ; irq_data -> chip = & intel_ir_chip ; intel_irq_remapping_prepare_irte ( ird , irq_cfg , info , index , i ); } } irq_domain_activate_irq 最终会调用到：intel_irq_remapping_activate -> intel_ir_reconfigure_irte -> modify_irte 。 modify_irte 中会将新的irte刷新到中断重定向表中。 vfio_msi_set_block vfio_msi_set_block 中调用 vfio_msi_set_vector_signal 为每个msi中断安排其Host IRQ的信号处理钩子，用来完成中断注入。 其内核调用栈为： vfio_pci_ioctl vfio_pci_set_irqs_ioctl vfio_pci_set_msi_trigger vfio_msi_set_block irq_bypass_register_producer __connect kvm_arch_irq_bypass_add_producer vmx_update_pi_irte # 在 Posted Interrupt模式下在这里刷新irte为Posted Interrupt模式 irq_set_vcpu_affinity intel_ir_set_vcpu_affinity modify_irte 再看下一 vfio_msi_set_vector_signal 的代码主要流程。 可以看出vfio_msi_set_vector_signal中为设备MSI中断申请了一个ISR，即vfio_msihandler， 然后注册了一个producer。 static int vfio_msi_set_vector_signal ( struct vfio_pci_device * vdev , int vector , int fd , bool msix ) { irq = pci_irq_vector ( pdev , vector ); #获得每个 MSI中断的irq号 ， trigger = eventfd_ctx_fdget ( fd ); if ( msix ) { struct msi_msg msg ; get_cached_msi_msg ( irq , & msg ); pci_write_msi_msg ( irq , & msg ); } #在host上申请中断处理函数 ret = request_irq ( irq , vfio_msihandler , 0 , vdev -> ctx [ vector ]. name , trigger ); vdev -> ctx [ vector ]. producer . token = trigger ; # irqfd对应的event_ctx vdev -> ctx [ vector ]. producer . irq = irq ; ret = irq_bypass_register_producer ( & vdev -> ctx [ vector ]. producer ); vdev -> ctx [ vector ]. trigger = trigger ; } 这样直通设备的中断会触发Host上的vfio_msihandler这个中断处理函数。在这个函数中向这个irqfd发送了一个信号通知中断到来， 如此一来KVM irqfd机制在poll这个irqfd的时候会受到这个事件，随后调用事件的处理函数注入中断。 irqfd_wakeup -> EPOLLIN -> schedule_work(&irqfd->inject) -> irqfd_inject -> kvm_set_irq这样就把中断注入到虚拟机了。 static irqreturn_t vfio_msihandler ( int irq , void * arg ) { struct eventfd_ctx * trigger = arg ; eventfd_signal ( trigger , 1 ); return IRQ_HANDLED ; } 思考一下：为什么触发这个irqfd的写事件后，直通设备的中断就能够被 重映射 到虚拟机内部呢？ 原因在于，前面我们提到的直通设备MSI中断的GSI和irqfd是一对一绑定的， 所以直通设备在向Guest vCPU投递MSI中断的时候首先会被IOMMU截获， 中断被重定向到Host IRQ上，然后通过irqfd注入MSI中断到虚拟机内部。 3.中断重映射下中断处理流程 为了方便理解，我花了点时间画了下面这张图，方便读者理解中断重映射场景下直通设备的中断处理流程： 总结一下中断重映射Enable和处理流程： QEMU向虚拟机呈现设备的PCI配置空间信息 -> 设备驱动加载，读写 PCI配置空间Enable MSI -> VM Exit到QEMU中处理vfio_pci_write_config -> QEMU调用vfio_msi_enable使能直通设备MSI中断 vfio_msix_vector_do_use分配一个MSI中断 -> vfio_add_kvm_msi_virq kvm_irqchip_add_msi_route -> kvm_set_irq_routing新建一个MSI中断路由表项 ， kvm_irqchip_add_irqfd_notifier_gsi -> kvm_irqchip_assign_irqfd注册irqfd和gsi的映射关系 vfio_enable_vectors设置VFIO的MSI中断触发方式 ， VFIO_DEVICE_SET_IRQS vfio_pci_set_irqs_ioctl 内核 drivers / vfio / pci / vfio_pci . c vfio_pci_set_msi_trigger分配Host irq并分配对应的IRTE和刷新中断重映射表 ， vfio_msi_set_vector_signal注册Host irq的中断处理函数vfio_msihandler irq_bypass_register_producer 将 fd注册到kvm -> vfio_msihandler写了irqfd这样就触发了EPOLLIN事件 -> irqfd接受到EPOLLIN事件 ，调用 irqfd_wakeup -> kvm_arch_set_irq_inatomic 尝试直接注入中断，如果被 BLOCK了 （ vCPU没有退出 ？）就调用 schedule_work ( & irqfd -> inject ) ，让 kworker延后处理 -> irqfd_inject向虚拟机注入中断 -> 虚拟机退出的时候写对应 VCPU的vAPIC Page IRR字段注入中断到Guest内部 。 Done!","tags":"virtualization"},{"title":"Debug Linux Kernel Using QEMU and GDB","url":"https://kernelgo.org/kernel-debug-using-qemu.html","loc":"https://kernelgo.org/kernel-debug-using-qemu.html","text":"有的时候为了研究内核原理或者调试bios的时候，可以利用QEMU和gdb的方式来帮助我们调试问题． 这种操作利用了QEMU内建的gdb-stub能力． 重新编译内核 编译的时候开启内核参数CONFIG_DEBUG_INFO和CONFIG_GDB_SCRIPTS再进行编译， 如果硬件支持CONFIG_FRAME_POINTER也一并开启． make modules -j ` nproc ` make -j ` nproc ` 调试内核 用下的命令行拉起QEMU，这里可以从自己的OS上选取一个initramfs传给QEMU， 记得配上 nokaslr 以免内核段基地址被随机映射． 这里 -S 参数可以让QEMU启动后CPU先Pause住不运行， -s 参数是 -gdb tcp::1234 的简写，意思是让QEMU侧的gdb server侦听在1234端口等待调试． /mnt/code/qemu/x86_64-softmmu/qemu-system-x86_64 \\ -machine pc-i440fx-2.8,accel = kvm,kernel_irqchip \\ -cpu host \\ -m 4096 ,slots = 4 ,maxmem = 16950M \\ -smp 4 \\ -chardev pty,id = charserial0 \\ -device isa-serial,chardev = charserial0,id = serial0 \\ -netdev tap,id = tap0,ifname = virbr0-nic,vhost = on,script = no \\ -device virtio-net-pci,netdev = tap0 \\ -kernel $KERNEL_SRC /arch/x86/boot/bzImage \\ -initrd /boot/vmlinuz-4.14.0-rc2-fangying \\ -append 'console=ttyS0 nokaslr' \\ -vnc :9 \\ -S -s gdb可能会报错 Remote 'g' packet reply is too long: ，这个时候的解决办法是打上一个补丁然后重新编译gdb. 问题处在static void process_g_packet (struct regcache *regcache)函数，6113行，屏蔽对buf_len的判断． 如果gdb版本低(7.x)打这个补丁: if ( buf_len > 2 * rsa->sizeof_g_packet ) error ( _ ( \"Remote 'g' packet reply is too long: %s\" ) , rs->buf ) ; 改为: if ( buf_len > 2 * rsa->sizeof_g_packet ) { rsa->sizeof_g_packet = buf_len ; for ( i = 0 ; i < gdbarch_num_regs ( gdbarch ) ; i++ ) { if ( rsa->regs [ i ] .pnum == -1 ) continue ; if ( rsa->regs [ i ] .offset > = rsa->sizeof_g_packet ) rsa->regs [ i ] .in_g_packet = 0 ; else rsa->regs [ i ] .in_g_packet = 1 ; } } 如果gdb版本高(8.x)打这个补丁: if ( buf_len > 2 * rsa->sizeof_g_packet ) error ( _ ( \"Remote 'g' packet reply is too long: %s\" ) , rs->buf ) ; 改为: /* Further sanity checks, with knowledge of the architecture. */ if ( buf_len > 2 * rsa->sizeof_g_packet ) { rsa->sizeof_g_packet = buf_len ; for ( i = 0 ; i < gdbarch_num_regs ( gdbarch ) ; i++ ) { if ( rsa->regs [ i ] .pnum == -1 ) continue ; if ( rsa->regs [ i ] .offset > = rsa->sizeof_g_packet ) rsa->regs [ i ] .in_g_packet = 0 ; else rsa->regs [ i ] .in_g_packet = 1 ; } } 开始愉快地调试内核了: $ gdb vmlinux ( gdb ) target remote :1234 Remote debugging using :1234 0x000000000000fff0 in cpu_hw_events () ( gdb ) hb start_kernel Hardware assisted breakpoint 1 at 0xffffffff827dabb2: file init/main.c, line 538 . ( gdb ) c Continuing. Thread 1 hit Breakpoint 1 , start_kernel () at init/main.c:538 538 { ( gdb ) l 533 { 534 rest_init () ; 535 } 536 537 asmlinkage __visible void __init start_kernel ( void ) 538 { 539 char *command_line ; 540 char *after_dashes ; 541 542 set_task_stack_end_magic ( & init_task ) ; ( gdb ) 另外可以使用mkinitrd（mkinitramfs）命令来生成initramfs.img文件，例如： sudo mkinitrd -v initramfs.img 5.0.0-rc4+ 第一个参数是initramfs的输出文件名，第二个参数是内核版本号。 然后我们可以直接boot这个内核： x86_64-softmmu/qemu-system-x86_64 \\ -kernel /mnt/code/linux/arch/x86/boot/bzImage \\ -nographic \\ -append \"console=ttyS0 nokalsr\" \\ -enable-kvm \\ -cpu host \\ -initrd initramfs.img \\ -m 1024 参考文献 1. linux kernel debug with qemu","tags":"linux"},{"title":"Tips on Linux","url":"https://kernelgo.org/linux-tips.html","loc":"https://kernelgo.org/linux-tips.html","text":"QEMU gdb调试屏蔽若干信号 echo 'handle SIGUSR1 SIGUSR2 noprint nostop' >> ~/.gdbinit 生成内核符号表 make tags ARCH = x86 make cscope ARCH = x86 GDB设置条件断点 设置条件断点除了使用break if语句之外还有一种便捷的方法。 例如，我们在qemu的函数vmstate_save_sate_v上设置断点，抓取virtio-blk设备的状态保存。 可以这么做： b vmstate_save_sate_v continue # 等到获取断点后使用 condition 1 strcmp ( vmsd -> name , \"virtio-blk\" ) == 0 # i b 可以看到多了一个条件，表示额外需要满足这个条件在 hit这个断点 参考: https://www.fayewilliams.com/2011/07/13/gdb-conditional-breakpoints/ 密码输错超过最大次数，解锁骚操作 pam_tally2 --user root pam_tally2 -r -u root 更新let's encrypt证书 yum -y install yum-utils yum-config-manager --enable rhui-REGION-rhel-server-extras rhui-REGION-rhel-server-optional yum install python2-certbot-nginx certbot --nginx certbot renew --dry-run ftrace查看pCPU上的调度情况 这里以查看pCPU2的调度状况为例 mount -t debugfs none /sys/kernel/debug/ cd /sys/kernel/debug/tracing echo 1 > events/sched/enable ; sleep 2 ; echo 0 > events/sched/enable cat per_cpu/cpu2/trace > /home/trace.txt 可以看到PCPU上的调度情况 # tracer : nop # # entries - in - buffer / entries - written : 381200 / 1199928 #P : 24 # # _ -----=> irqs - off # / _ ----=> need - resched # | / _ ---=> hardirq / softirq # || / _ --=> preempt - depth # ||| / delay # TASK - PID CPU # |||| TIMESTAMP FUNCTION # | | | |||| | | < idle >- 0 [ 002 ] d ... 158158.084267 : sched_switch : prev_comm = swapper / 2 prev_pid = 0 prev_prio = 120 prev_state = R ==> next_comm = CPU 0 / KVM next_pid = 2046 next_prio = 120 CPU 0 / KVM - 2046 [ 002 ] d ... 158158.084280 : sched_stat_runtime : comm = CPU 0 / KVM pid = 2046 runtime = 16239 [ ns ] vruntime = 105862773195 [ ns ] CPU 0 / KVM - 2046 [ 002 ] d ... 158158.084281 : sched_switch : prev_comm = CPU 0 / KVM prev_pid = 2046 prev_prio = 120 prev_state = S ==> next_comm = swapper / 2 next_pid = 0 next_prio = 120 < idle >- 0 [ 002 ] dNh . 158158.085265 : sched_wakeup : comm = CPU 0 / KVM pid = 2046 prio = 120 success = 1 target_cpu = 002 < idle >- 0 [ 002 ] d ... 158158.085266 : sched_switch : prev_comm = swapper / 2 prev_pid = 0 prev_prio = 120 prev_state = R ==> next_comm = CPU 0 / KVM next_pid = 2046 next_prio = 120 CPU 0 / KVM - 2046 [ 002 ] d ... 158158.085280 : sched_stat_runtime : comm = CPU 0 / KVM pid = 2046 runtime = 16148 [ ns ] vruntime = 105862789343 [ ns ] CPU 0 / KVM - 2046 [ 002 ] d ... 158158.085281 : sched_switch : prev_comm = CPU 0 / KVM prev_pid = 2046 prev_prio = 120 prev_state = S ==> next_comm = swapper / 2 next_pid = 0 next_prio = 120 < idle >- 0 [ 002 ] dNh . 158158.086265 : sched_wakeup : comm = CPU 0 / KVM pid = 2046 prio = 120 success = 1 target_cpu = 002 测试NUMA NODE内存性能 这里以加压100M为例 #关闭NMI watchdog，防止压力过大把系统搞死 echo 0 > /proc/sys/kernel/nmi_watchdog #打开shell的cgroup隔离 echo $$ > /sys/fs/cgroup/cpuset/tasks 获取Stream加压工具 git clone https://github.com/jeffhammond/STREAM cd STREAM-master gcc -O -DSTREAM_ARRAY_SIZE=100000000 stream.c -o stream.100M #获取pcm带宽测试工具 git clone https://github.com/opcm/pcm chmod +x ./pcm-memory.x ./pcm-memory.x 查看带宽 如何查看任务的调度延时 通过perf sched 查看某段时间内进程的调度延时 perf sched record -- sleep 1 #观察调度延迟 perf sched latency -- sort = max No newline at end of file :set binary noeol 如何查找一个系统调用的定义在哪个文件中 grep -E \"SYSCALL_DEFINE[0-6]\\(listen\" -nr perf查看虚拟机性能数据 pid=$(pgrep qemu-kvm) perf kvm stat record -p $pid perf kvm stat report wget下载指定目录的指定文件到 wget -c -r -nd -np -k -L -p -A \"qemu*.rpm\" \\ http://mirrors.aliyun.com/centos/7.6.1810/updates/x86_64/Packages/ -c 断点续传， -r 递归， -nd不在本地创建对应文件夹， -np不下载父目录， -A 接受哪些类型的文件（这里可以试一个glob表达式） 查询设备/设备类属性 设备属性 virsh qemu-monitor-command vmname '{\"arguments\": {\"typename\": \"virtio-net-pci\"}, \"execute\":\"device-list-properties\"}' 设备类属性 virsh qemu-monitor-command vmname '{\"arguments\": {\"typename\": \"virtio-net-pci\"}, \"execute\":\"qom-list-properties\"}' 设备实例的属性 virsh qemu-monitor-command fangying --hmp \"info qtree\" | less 配置yum proxy # append to /etc/yum.conf proxy=http://xxxx proxy_username=xxxx proxy_password=xxxx 配置docker hub 众所周知,docker hub默认国外镜像在国内几乎无法访问,需要配置国内支持匿名pull的docker hub. edit /etc/docker/daemon.json { \"registry-mirrors\" : [ \"https://dockerhub.azk8s.cn\" , \"https://docker.mirrors.ustc.edu.cn\" , \"https://registry.docker-cn.com\" ] } 然后重新启动Docker服务 sudo systemctl daemon-reload sudo systemctl restart docker 编译firecracker docker run --user 0:0 --workdir /firecracker -it \\ --name firecracker --net host --volume $(pwd):/firecracker:z \\ --env OPT_LOCAL_IMAGES_PATH=/firecracker/build \\ --env PYTHONDONTWRITEBYTECODE=1 fcuvm/dev:v12 虚拟机文件系统扩容 情况1： 非lvm卷，rootfs是ext4文件系统 安装的虚拟机根分区太小，磁盘格式是qcow2的，可以直接扩容，先qemu-img扩展镜像大小，再到虚拟机里面扩展文件系统。 # root @ubuntu : ~ # lsblk NAME MAJ : MIN RM SIZE RO TYPE MOUNTPOINT sda 8 : 0 0 10 G 0 disk ├─ sda1 8 : 1 0 512 M 0 part / boot / efi └─ sda2 8 : 2 0 9.5 G 0 part / sr0 11 : 0 1 7.6 G 0 rom # qemu - img resize vm - imgage . qcow2 + 100 G # root @ubuntu : ~ # lsblk NAME MAJ : MIN RM SIZE RO TYPE MOUNTPOINT sda 8 : 0 0 110 G 0 disk ├─ sda1 8 : 1 0 512 M 0 part / boot / efi └─ sda2 8 : 2 0 9.5 G 0 part / sr0 11 : 0 1 7.6 G 0 rom 先删除原来的分区 ， 然后重新创建分区 ， 前提是这个分区是磁盘的最后一个分区 # gparted / dev / sda ( parted ) rm 2 Warning : Partition / dev / sda2 is being used . Are you sure you want to continue ? Yes / No ? Yes Error : Partition ( s ) 2 on / dev / sda have been written , but we have been unable to inform the kernel of the change , probably because it / they are in use . As a result , the old partition ( s ) will remain in use . You should reboot now before making further changes . Ignore / Cancel ? Ignore # gparted / dev / sda ( parted ) mkpart Start ? 538 MB End ? 100 G ( parted ) quit 文件系统扩容 # resize2fs / 重启虚拟机就OK了 情况2：lvm卷，rootfs是 xfs的 策略还是先调整qcow2磁盘大小，然后进入系统调整pv的大小，然后对lv进行扩容，最后再把xfs系统扩大。 # lsblk vda 252 : 0 0 10 G 0 part | __vda1 252 : 1 0 1 G 0 part / boot | __vda2 252 : 2 0 9 G 0 part | __fedora - root 253 : 0 0 8 G 0 lvm / | __fedora - swap 253 : 1 0 1 G 0 lvm [ SWAP ] 将磁盘扩大 ， 扩大个100G # qemu - img resize fedora_31_64_server . qcow2 + 100 G # cfdisk / dev / vda2 删除原来 / dev / vda2分期 ， 重建分区把后面扇区都扩进来 # lsblk vda 252 : 0 0 110 G 0 part | __vda1 252 : 1 0 1 G 0 part / boot | __vda2 252 : 2 0 9 G 0 part | __fedora - root 253 : 0 0 8 G 0 lvm / | __fedora - swap 253 : 1 0 1 G 0 lvm [ SWAP ] 接着对 / dev / vda2进行pv扩容 ， 将剩余的空间都划成PE # pvresize / dev / vda2 # pvdisplay 确认PE扩展成功 对LV进行扩容 # lvextend - L + 100 G / dev / mapper / fedora - root 对文件系统进行扩容 # xfs_growfs / 搞定了 Debug Qemu代码 QEMU_BINARY=/mnt/sdc/fangying/x86/qemu/x86_64-softmmu/qemu-system-x86_64 IMAGE=$(pwd)/plc_centos_7.4_64.qcow2 gdb --args $QEMU_BINARY \\ -machine pc-i440fx-2.8,accel=kvm,kernel_irqchip \\ -cpu host \\ -m 8192,slots=4,maxmem=16950M \\ -smp 4 \\ -chardev pty,id=charserial0 \\ -device isa-serial,chardev=charserial0,id=serial0 \\ -netdev tap,id=tap0,ifname=tap0,vhost=off,script=no \\ -device virtio-net-pci,netdev=tap0 \\ -drive file=$IMAGE \\ -vnc :9 将进程proc cmdline格式化一下 sed -i \"s/ -/ \\\\\\ \\n-/g\" cmd.txt && sed -i 's/ *$//' cmd.txt 使用quilt补丁管理工具来管理补丁 quilt add filename quilt diff quilt top quilt push quilt pop quilt refresh quilt edit quilt files quilt fork quilt graph quilt import quilt ... 请参看教程： https://www.cnblogs.com/openix/p/3984538.html macOS catalina 配置git 自动补全 https://dev.to/saltyshiomix/a-guide-for-upgrading-macos-to-catalina-and-migrating-the-default-shell-from-bash-to-zsh-4ep3 git 将本地自己拉出来的分支push到远程 git checkout - b test // do some commit git push -- set - upstream origin test DPDK源码编译和测试 参考： http://doc.dpdk.org/guides/linux_gsg/build_dpdk.html http://doc.dpdk.org/guides/linux_gsg/quick_start.html https://blog.51cto.com/10017068/2107562 git clone https://github.com/DPDK/dpdk.git git checkout v20.02 pip3 install meson ninja meson build ninja sudo ninja install ldconfig x86平台上使用 make config T = x86_64 - native - linux - gcc export RTE_SDK =/ usr / local # 指定 DPDK的安装路径 export RTE_TARGET = x86_64 - native - linuxapp - gcc # 指定 TARGET名称 make - j 编译出来的目标文件默认存放路径是： $RTE_SDK/$RTE_TARGET 例如： testpmd应用路径 $RTE_SDK/$RTE_TARGET/app/testpmd 安装到系统目录下，fedora下默认的DESTDIR=/usr/local目录： make install // install to /usr/local/share/dpdk 配置静态大页，2M大页可以动态配置但1G大页还是要重启虚拟机。 echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages 或者numa系统上 echo 1024 > /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages echo 1024 > /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages 挂在hugepagetlbfs文件系统 mkdir /mnt/huge mount -t hugetlbfs nodev /mnt/huge 写fstab nodev /mnt/huge hugetlbfs defaults 0 0 或者配置内核参数 default_hugepagesz=2M hugepagesz=2M hugepages=1024 绑定驱动到 vfio - pci sudo modprobe vfio vfio - pci sudo ./ usertools / dpdk - devbind . py -- bind = vfio - pci 0000 : 00 : 1 f . 6 绑定好了之后确认下是否成功了 $ lspci - vvvs 0000 : 00 : 1 f .6 00 : 1 f .6 Ethernet controller : Intel Corporation Ethernet Connection ( 2 ) I219 - V Subsystem : Gigabyte Technology Co ., Ltd Device e000 Control : I / O - Mem - BusMaster - SpecCycle - MemWINV - VGASnoop - ParErr - Stepping - SERR - FastB2B - DisINTx + Status : Cap + 66 MHz - UDF - FastB2B - ParErr - DEVSEL = fast > TAbort - < TAbort - < MAbort - > SERR - < PERR - INTx - Interrupt : pin A routed to IRQ 16 Region 0 : Memory at ef500000 ( 32 - bit , non - prefetchable ) [ disabled ] [ size=128K ] Capabilities : < access denied > Kernel driver in use : vfio - pci lspci : Unable to load libkmod resources : error - 12 执行一下helloworld测试用例： ╰─$ sudo . / helloworld [ sudo ] fang 的密码 ： EAL : Detected 8 lcore ( s ) EAL : Detected 1 NUMA nodes EAL : Multi - process socket / var / run / dpdk / rte / mp_socket EAL : Selected IOVA mode 'VA' EAL : No available hugepages reported in hugepages - 1048576 kB EAL : Probing VFIO support ... EAL : VFIO support initialized EAL : PCI device 0000 : 00 : 1 f .6 on NUMA socket - 1 EAL : Invalid NUMA socket , default to 0 EAL : probe driver : 8086 : 15 b8 net_e1000_em EAL : using IOMMU type 1 ( Type 1 ) hello from core 1 hello from core 2 hello from core 3 hello from core 4 hello from core 5 hello from core 6 hello from core 7 hello from core 0 再来测试一下报文转发，执行测试用例testpmd程序，进入交互模式后执行start sudo . / testpmd - l 0-3 - n 4 -- - i -- portmask = 0x1 -- nb - cores = 2 testpmd > start io packet forwarding - ports = 1 - cores = 1 - streams = 1 - NUMA support enabled , MP allocation mode : native Logical Core 1 ( socket 0 ) forwards packets on 1 streams : RX P = 0 / Q = 0 ( socket 0 ) -> TX P = 0 / Q = 0 ( socket 0 ) peer = 02 : 00 : 00 : 00 : 00 : 00 io packet forwarding packets / burst = 32 nb forwarding cores = 2 - nb forwarding ports = 1 port 0 : RX queue number : 1 Tx queue number : 1 Rx offloads = 0x0 Tx offloads = 0x0 RX queue : 0 RX desc = 256 - RX free threshold = 0 RX threshold registers : pthresh = 0 hthresh = 0 wthresh = 0 RX Offloads = 0x0 TX queue : 0 TX desc = 256 - TX free threshold = 0 TX threshold registers : pthresh = 0 hthresh = 0 wthresh = 0 TX offloads = 0x0 - TX RS bit threshold = 0 这时候我的台式机风扇就呜呜跑起来了，因为有一个CPU执行pmd驱动，CPU占用率达到了100%。 top - 21 : 21 : 17 up 8 : 30 , 3 users , load average : 0.40 , 0.13 , 0.04 Tasks : 291 total , 1 running , 290 sleeping , 0 stopped , 0 zombie % Cpu ( s ): 12.9 us , 0.3 sy , 0.0 ni , 86.7 id , 0.0 wa , 0.0 hi , 0.0 si , 0.0 st MiB Mem : 15982.0 total , 6987.8 free , 4255.0 used , 4739.1 buff / cache MiB Swap : 8192.0 total , 8148.2 free , 43.8 used . 10993.1 avail Mem PID USER PR NI VIRT RES SHR S % CPU % MEM TIME + COMMAND 624935 root 20 0 64.2 g 66428 16644 S 100.0 0.4 0 : 32.82 testpmd 1578 root 20 0 236660 14648 2768 S 1.0 0.1 4 : 02.44 phdaemon 169477 root 20 0 252768 29536 8136 S 0.3 0.2 0 : 30.76 sssd_kcm 1 root 20 0 172240 12648 8996 S 0.0 0.1 0 : 08.36 systemd Docker配置内部代理 方法一：写Dockerfile，启动容器OS后传入proxy From docker.io/fedora:latest RUN [ -n $http_proxy ] && sed -i \" $ a proxy= $http_proxy \" /etc/dnf/dnf.conf ; true RUN dnf install -y vim multifd live migration Since qemu 4.1.0 virsh migrate --live --unsafe --parallel --parallel-connections 4 \\ --migrateuri tcp://192.168.3.33 testvm qemu+tcp://192.168.3.33/system --verbose check ASLR是否开启 Check if ASLR is currently activated on the system: cat /proc/sys/kernel/randomize_va_space Activate ASLR: echo 0 > /proc/sys/kernel/randomize_va_space qemu常用hmp/qmp命令 查询qom设备树： virsh qemu-monitor-command openeuler-test --hmp \"info qom-tree\" 查询设备属性： virsh qemu-monitor-command openeuler-test '{\"execute\": \"qom-get\", \"arguments\": {\"path\": \"/machine\", \"property\": \"gic-version\"}}' virsh qemu-monitor-command openeuler-test '{\"execute\": \"qom-get\", \"arguments\": {\"path\": \"/machine/unattached/device[1]\", \"property\": \"mp-affinity\"}}' AArch64 Qemu Direct Boot QEMU_BIN=/root/fangying/opensrc/qemu/build/aarch64-softmmu/qemu-system-aarch64 $QEMU_BIN \\ -machine virt-4.1,accel=kvm,usb=off,dump-guest-core=off,gic-version=3 \\ -cpu host \\ -smp 1 \\ -m 2048 \\ -kernel /root/fangying/opensrc/linux/arch/arm64/boot/Image \\ -append \"console=ttyAMA0 root=/dev/vda rw pci=off reboot=k panic=1 \" \\ -drive file=/root/ljj/opensrc/stratovirt/rootfs.ext4,id=rootfs,readonly=off \\ -nographic AArch64 QEMU UEFI boot with kernel QEMU_BIN =/ root / fangying / opensrc / qemu / build / aarch64 - softmmu / qemu - system - aarch64 $ QEMU_BIN \\ - machine virt - 4.1 , accel = kvm , usb = off , dump - guest - core = off , gic - version = 3 \\ - cpu host \\ - smp 1 \\ - m 2048 \\ - kernel / root / fangying / opensrc / linux / arch / arm64 / boot / Image \\ - append \"console=ttyAMA0 root=/dev/vda rw pci=off reboot=k panic=1 \" \\ - drive file =/ usr / share / edk2 / aarch64 / QEMU_EFI - pflash . raw , if = pflash , format = raw , unit = 0 , readonly = on \\ - drive file =/ var / lib / libvirt / qemu / nvram / fangying_openeuler_VARS . fd , if = pflash , format = raw , unit = 1 \\ - drive file =/ root / ljj / opensrc / stratovirt / rootfs . ext4 , id = rootfs , readonly = off \\ - nographic EDK2日志开关 以ArmVirt主板为例，我们为例查看详细的UEFI启动日志，需要打开一个日志开关。 在文件：ArmVirtPkg/ArmVirt.dsc.inc中找到：DEBUG_PRINT_ERROR_LEVEL， 该开关通过bitmask控制日志级别。 [Defines] DEFINE DEBUG_PRINT_ERROR_LEVEL = 0x8000004F 改为： DEFINE DEBUG_PRINT_ERROR_LEVEL = 0xFFFFFFFF 然后再编译我们的edk2，使用DEBUG模式: yum install iasl libuuid-devel -y cd edk2 git submodule update --init make -C BaseTools -j NCPUS=`/usr/bin/getconf _NPROCESSORS_ONLN` BUILD_OPTION=\"-t GCC5 -n $NCPUS -b DEBUG\" . ./edksetup.sh build $BUILD_OPTION build -t GCC5 -n 128 -b DEBUG -a AARCH64 -p ArmVirtPkg/ArmVirtQemu.dsc CMake 引入外部库 例如我们将libboundscheck动态库引入工程，需要定义： link_directories 指定libboundscheck动态库的寻找路径； target_link_libraries 指定库名字为boundscheck； target_include_directories 指定头文件查找路径 cmake_minimum_required ( VERSION 3.15 ) project ( TestPrjt C ) set ( CMAKE_C_STANDARD 99 ) link_directories ( /usr/local/lib） add_executable(TestPrjt main.c ) target_link_libraries ( TestPrjt boundscheck ) target_include_directories ( TestPrjt PRIVATE /usr/local/include/libboundscheck ) VFIO 直通虚拟机 查询网卡的PCI外设号码 sudo ethtool - i enp6s0 [ sudo ] password for fang : driver : alx version : 5.6.0 - rc3 + firmware - version : expansion - rom - version : bus - info : 0000 : 00 : 1 f .6 supports - statistics : yes supports - test : no supports - eeprom - access : no supports - register - dump : no supports - priv - flags : no 加载驱动 modprobe vfio vfio-pci bind到vfio-pci驱动上 readlink /sys/bus/pci/devices/0000:00:1f.6/iommu_group echo \"bind\" echo \"vfio-pci\" > \"/sys/bus/pci/devices/0000:00:1f.6/driver_override\" echo 0000:00:1f.6 > /sys/bus/pci/drivers_probe echo \"unbind\" echo \"0000:00:1f.6\" > \"/sys/bus/pci/devices/0000:00:1f.6/driver/unbind\" echo \"0000:00:1f.6\" > /sys/bus/pci/drivers_probe 启动虚拟机了： gdb --args /home/fang/code/fang/qemu/build/x86_64-softmmu/qemu-system-x86_64 \\ -enable-kvm -m 2048 \\ -boot c \\ -drive driver=qcow2,file=$PWD/centos7.qcow2 \\ -netdev tap,id=tap0,ifname=$E1000_TAP_NAME,script=no \\ -device e1000,netdev=tap0,id=net0 \\ -netdev tap,id=tap1,ifname=$VIRTIO_NET_TAP_NAME,script=no \\ -device virtio-net-pci,netdev=tap1,id=net1,addr=05.0 \\ -device piix3-usb-uhci,id=uhci,addr=04.0 \\ -smp cpus=4 \\ -vnc :88 \\ -cdrom virtio-win-0.1.141.iso \\ -device vfio-pci,host=0000:00:1f.6,id=hostdev0 PL011 串口调试 QEMU_BIN =/ root / fangying / opensrc / qemu - fangying / build / aarch64 - softmmu / qemu - system - aarch64 $ QEMU_BIN \\ - machine virt - 4.0 , accel = kvm , gic - version = 3 \\ - cpu host \\ - smp 1 \\ - nographic \\ - m 512 M \\ - kernel / root / fangying / opensrc / linux / vmlinux . bin \\ - drive id = test , file =/ root / fangying / vm / rootfs_arm , format = raw , if = none \\ - device virtio - blk - device , drive = test \\ - device pci - bridge , chassis_nr = 1 , bus = pcie . 0 \\ - append 'console=ttyAMA0 root=/dev/vda' \\ - qmp unix : / tmp / qemu . sock , server , nowait \\ - trace events =/ tmp / events \\ - D / var / log / qemu . log x86 Direct Boot microvm qemu - system - x86_64 \\ - M microvm , x - option - roms = off , pit = off , pic = off , isa - serial = off , rtc = off \\ - enable - kvm - cpu host - m 512 m - smp 2 \\ - kernel vmlinux - append \" console=hvc0 root=/dev/vda \" \\ - nodefaults - no - user - config - nographic \\ - chardev stdio , id = virtiocon0 \\ - device virtio - serial - device \\ - device virtconsole , chardev = virtiocon0 \\ - drive id = test , file = test . img , format = raw , if = none \\ - device virtio - blk - device , drive = test \\ - netdev tap , id = tap0 , script = no , downscript = no \\ - device virtio - net - device , netdev = tap0 Rust调试 运行Rust程序传入RUST_BACKTRACE=full RUST_BACKTRACE=full cargo run ... Libvirt增加外部接口 scripts/check-aclrules.py 白名单 driver-hypervisor.h 定义声明 src/libvirt-domain.c 接口定义实现 src/qemu/qemu-driver.c driver接口实现 remote_driver.c增加定义 src/libvirt_public.syms 增加定义 虚拟机使用spice协议 <channel type= 'spicevmc' > <target type= 'virtio' name= 'com.redhat.spice.0' state= 'connected' /> <alias name= 'channel4' /> <address type= 'virtio-serial' controller= '0' bus= '0' port= '5' /> </channel> <input type= 'tablet' bus= 'usb' > <alias name= 'input0' /> <address type= 'usb' bus= '0' port= '1' /> </input> <input type= 'keyboard' bus= 'usb' > <alias name= 'input1' /> <address type= 'usb' bus= '0' port= '2' /> </input> <graphics type= 'spice' port= '5900' autoport= 'yes' listen= '0.0.0.0' > <listen type= 'address' address= '0.0.0.0' /> </graphics> <video> <model type= 'virtio' vram= '16384' heads= '1' primary= 'yes' /> <alias name= 'video0' /> <address type= 'pci' domain= '0x0000' bus= '0x03' slot= '0x04' function= '0x0' /> </video> 一个较为完整的虚拟机 / usr / bin / qemu - system - x86_64 \\ - name guest = fangying_uefi , debug - threads = on \\ - S \\ - object secret , id = masterKey0 , format = raw , file =/ var / lib / libvirt / qemu / domain - 2 - fangying_uefi / master - key . aes \\ - machine pc - i440fx - 4.1 , accel = kvm , usb = off , dump - guest - core = off \\ - cpu host , kvm - pv - eoi = on \\ - drive file =/ usr / share / edk2 / ovmf / OVMF . fd , if = pflash , format = raw , unit = 0 , readonly = on \\ - drive file =/ usr / share / edk2 / ovmf / OVMF_VARS . fd , if = pflash , format = raw , unit = 1 \\ - m 16384 \\ - overcommit mem - lock = off \\ - smp 16 , sockets = 16 , cores = 1 , threads = 1 \\ - uuid a1f00325 - 9972 - 486 e - a127 - 6 f0b14b44a3f \\ - no - user - config \\ - nodefaults \\ - chardev socket , id = charmonitor , fd = 30 , server , nowait \\ - mon chardev = charmonitor , id = monitor , mode = control \\ - rtc base = utc , clock = vm , driftfix = slew \\ - no - hpet \\ - global kvm - pit . lost_tick_policy = delay \\ - no - shutdown \\ - boot strict = on \\ - device piix3 - usb - uhci , id = usb , bus = pci . 0 , addr = 0x1 . 0x2 \\ - device usb - ehci , id = usb1 , bus = pci . 0 , addr = 0x4 \\ - device nec - usb - xhci , id = usb2 , bus = pci . 0 , addr = 0x5 \\ - device virtio - scsi - pci , id = scsi0 , bus = pci . 0 , addr = 0x6 \\ - device virtio - serial - pci , id = virtio - serial0 , bus = pci . 0 , addr = 0x7 \\ - drive file =/ mnt / sdc / fangying / x86 / fangying_uefi . qcow2 , format = qcow2 , if = none , id = drive - virtio - disk0 , cache = none , aio = native \\ - device virtio - blk - pci , scsi = off , bus = pci . 0 , addr = 0x9 , drive = drive - virtio - disk0 , id = virtio - disk0 , bootindex = 1 , write - cache = on \\ - netdev tap , fd = 33 , id = hostnet0 , vhost = on , vhostfd = 34 \\ - device virtio - net - pci , netdev = hostnet0 , id = net0 , mac = 52 : 24 : 01 : d9 : dc : 53 , bus = pci . 0 , addr = 0x3 \\ - chardev pty , id = charserial0 \\ - device isa - serial , chardev = charserial0 , id = serial0 \\ - vnc 0.0 . 0.0 : 0 \\ - device cirrus - vga , id = video0 , bus = pci . 0 , addr = 0x2 \\ - device virtio - balloon - pci , id = balloon0 , bus = pci . 0 , addr = 0x8 \\ - sandbox on , obsolete = deny , elevateprivileges = deny , spawn = deny , resourcecontrol = deny \\ - msg timestamp = on cpu topology QEMU_BIN =/ root / fangying / opensrc / qemu - fangying / build / aarch64 - softmmu / qemu - system - aarch64 $ QEMU_BIN \\ - machine virt - 4.1 , accel = kvm , usb = off , dump - guest - core = off , gic - version = 3 \\ - cpu host \\ - drive file =/ usr / share / edk2 / aarch64 / QEMU_EFI - pflash . raw , if = pflash , format = raw , unit = 0 , readonly = on \\ - drive file =/ var / lib / libvirt / qemu / nvram / fangying_openeuler_VARS . fd , if = pflash , format = raw , unit = 1 \\ - m 8192 \\ - smp 4 , sockets = 1 , dies = 1 , cores = 4 , threads = 1 \\ - no - user - config \\ - nodefaults \\ - rtc base = utc \\ - no - shutdown \\ - device pcie - root - port , port = 0x8 , chassis = 1 , id = pci . 1 , bus = pcie . 0 , multifunction = on , addr = 0x1 \\ - device pcie - root - port , port = 0x9 , chassis = 2 , id = pci . 2 , bus = pcie . 0 , addr = 0x1 . 0x1 \\ - device pcie - pci - bridge , id = pci . 3 , bus = pci . 1 , addr = 0x0 \\ - device pcie - root - port , port = 0xa , chassis = 4 , id = pci . 4 , bus = pcie . 0 , addr = 0x1 . 0x2 \\ - device pcie - root - port , port = 0xb , chassis = 5 , id = pci . 5 , bus = pcie . 0 , addr = 0x1 . 0x3 \\ - device pcie - root - port , port = 0xc , chassis = 6 , id = pci . 6 , bus = pcie . 0 , addr = 0x1 . 0x4 \\ - device pcie - root - port , port = 0xd , chassis = 7 , id = pci . 7 , bus = pcie . 0 , addr = 0x1 . 0x5 \\ - device pcie - root - port , port = 0xe , chassis = 8 , id = pci . 8 , bus = pcie . 0 , addr = 0x1 . 0x6 \\ - device pcie - root - port , port = 0xf , chassis = 9 , id = pci . 9 , bus = pcie . 0 , addr = 0x1 . 0x7 \\ - device usb - ehci , id = usb , bus = pci . 3 , addr = 0x1 \\ - device virtio - scsi - pci , id = scsi0 , bus = pci . 4 , addr = 0x0 \\ - device virtio - serial - pci , id = virtio - serial0 , bus = pci . 3 , addr = 0x2 \\ - chardev pty , id = charserial0 \\ - serial chardev : charserial0 \\ - device usb - tablet , id = input0 , bus = usb . 0 , port = 1 \\ - device usb - kbd , id = input1 , bus = usb . 0 , port = 2 \\ - vnc 0.0 . 0.0 : 99 \\ - device virtio - gpu - pci , id = video0 , max_outputs = 1 , bus = pci . 3 , addr = 0x4 x86 UEFI Direct Boot qemu - system - x86_64 \\ - M pc \\ - cpu host \\ - smp cpus = 4 \\ - enable - kvm \\ - m 2048 \\ - nographic \\ - net none \\ - drive if = pflash , format = raw , unit = 0 , file =/ home / fang / code / opensrc / edk2 / Build / OvmfX64 / DEBUG_GCC5 / FV / OVMF_CODE . fd , readonly = on \\ - drive if = pflash , format = raw , unit = 1 , file =/ home / fang / code / opensrc / edk2 / Build / OvmfX64 / DEBUG_GCC5 / FV / OVMF_VARS . fd \\ - device virtio - blk - pci , id = blk0 , drive = image \\ - device piix3 - usb - uhci , id = uhci \\ - device usb - tablet \\ - drive if = none , id = image , file = $ PWD / dummy . raw , format = raw \\ - monitor telnet :: 12345 , server , nowait \\ - serial stdio Make rootfs using installroot sudo dnf --releasever 33 --nogpgcheck --installroot /mnt/rootfs install systemd yum passwd dnf fedora-release --noplugins sed -i \"s|root:x|root:|\" etc/password QOM qmp example virsh qemu-monitor-command vmname '{\"execute\": \"qom-list-types}\"' --pretty virsh qemu-monitor-command vmname '{\"execute\": \"qom-list\", \"arguments\": {\"path\": \"/machine/\"}}' --pretty virsh qemu-monitor-command vmname '{\"execute\": \"qom-list\", \"arguments\": {\"path\": \"/machine/peripheral\"}}' --pretty virsh qemu-monitor-command vmname '{\"execute\": \"qom-list\", \"arguments\": {\"path\": \"/machine/unattached\"}}' --pretty virsh qemu-monitor-command vmname '{\"execute\": \"device-list-properities, \"arguments\": {\"typename\": \"virtio-serial-pci\"}}' --pretty","tags":"linux"},{"title":"Insight Into VFIO","url":"https://kernelgo.org/vfio-insight.html","loc":"https://kernelgo.org/vfio-insight.html","text":"本文则主要探讨一下VFIO实现中的关键点，主要包括： VFIO中如实现对直通设备的I/O地址空间访问的？ VFIO中如何实现MSI/MSI-X，Interrupt Remapping，以及Posted Interrupt的支持？ VFIO中是如何建立DMA Remapping映射关系？ VFIO中又是如何支持设备热插拔的？ 上面4个问题你能回答上来吗？ 1.VFIO中如实现对直通设备的I/O地址空间访问？ 在设备直通的场景下guest OS到底该如何访问设备I/O空间？ 有两种方法可选： 方法A：直接呈现，将设备在主机上的PCI BAR呈现给guest，并通过VMCS的I/O bitmap和EPT页表使guest访问设备的PIO和MMIO都不引起VM-Exit，这样guest驱动程序可以直接访问设备的I/O地址空间。 方法B：建立转换表，呈现虚拟的PCI BAR给guest，当guest访问到虚拟机的I/O地址空间时VMM截获操作并通过转换表将I/O请求转发到设备在主机上的I/O地址空间上。 方法A看起来很高效，因为直接呈现的方式下不引入VM-Exit，但实际上是有问题的！ 原因是 ： 设备的PCI BAR空间是由host的BIOS配置并由host操作系统直接使用的， guest的PCI BAR空间是由guest的虚拟BIOS（例如Seabios）配置的， 那么问题来了，到底该由谁来配置设备的PCI BAR空间呢？肯定不能两个都生效否则就打架了！ 我们应该阻止guest来修改真实设备的PCI BAR地址以防止造成host上PCI设备的BAR空间冲突导致可能出现的严重后果。 所以我们要选择方案B，建立转换表，明白这一点很重要！ 对于直通设备的PIO访问而言，通过设置VMCS的I/O bitmap控制guest访问退出到VMM中然后通过转换表（模拟的方式）将PIO操作转发到真实物理设备上。对于MMIO的访问，可以通过EPT方式将虚拟的MMIO地址空间映射到物理设备的MMIO地址空间上，这样guest访问MMIO时并不需要VM-Exit。 直通设备的PCI Config Space模拟 PCI配置空间是用来报告设备I/O信息的区域，可以通过PIO或者MMIO方式进行访问。 设备直通场景的配置空间并不直接呈现给guest而是由VFIO配合qemu进行模拟的。 vfio_realize函数中， QEMU会读取物理设备的PCI配置空间以此为基础然后对配置空间做些改动然后呈现给虚拟机。 /* Get a copy of config space */ // 读取设备的原始PCI Config Space信息 ret = pread ( vdev -> vbasedev . fd , vdev -> pdev . config , MIN ( pci_config_size ( & vdev -> pdev ), vdev -> config_size ), vdev -> config_offset ); // 调用vfio-pci内核中的vfio_pci_read实现 ...... /* vfio emulates a lot for us, but some bits need extra love */ vdev -> emulated_config_bits = g_malloc0 ( vdev -> config_size ); // 我们可以选择性的Enable/Disable一些Capability /* QEMU can choose to expose the ROM or not */ memset ( vdev -> emulated_config_bits + PCI_ROM_ADDRESS , 0xff , 4 ); /* QEMU can also add or extend BARs */ memset ( vdev -> emulated_config_bits + PCI_BASE_ADDRESS_0 , 0xff , 6 * 4 ); // 调用vfio_add_emulated_word修改模拟的PCI配置空间信息 vfio_add_emulated_word /* * Clear host resource mapping info. If we choose not to register a * BAR, such as might be the case with the option ROM, we can get * confusing, unwritable, residual addresses from the host here. */ memset ( & vdev -> pdev . config [ PCI_BASE_ADDRESS_0 ], 0 , 24 ); memset ( & vdev -> pdev . config [ PCI_ROM_ADDRESS ], 0 , 4 ); vfio_bars_prepare ( vdev ); // 重点分析 vfio_bars_register ( vdev ); // 重点分析 vfio_add_capabilities ( vdev , errp ); 通常MSI/MSIX等信息都需要被QEMU修改，因为这些都是QEMU使用VFIO去模拟的。 直通设备MMIO（BAR空间）映射 vfio_realize函数中会对直通设备的MMIO空间进行映射，大致包含以下几个步骤: 调用vfio_populate_device从VFIO中查询出设备的BAR空间信息 把设备的MMIO（BAR空间）重映射（mmap）到QEMU进程的虚拟地址空间 将该段虚拟机地址空间标记为RAM类型注册给虚拟机 这样一来，guest访问MMIO地址空间时直接通过EPT翻译到HPA不需要VM-Exit。我们分析下具体流程： vfio_realize |-> vfio_populate_device |-> vfio_region_setup |-> vfio_get_region_info // call ioct VFIO_DEVICE_GET_REGION_INFO |-> memory_region_init_io // init region->mem MR as I/O |-> vfio_bars_prepare -> vfio_bar_prepare // probe info of each pci bar from PCI cfg space |-> vfio_bars_register -> vfio_bar_register |-> memory_region_init_io // int bar->mr |-> memory_region_add_subregion // add bar->mr into region->mem MR |-> vfio_region_mmap |-> mmap // map device bar space into QEMU process address space -> iova |-> memory_region_init_ram_device_ptr // register iova into VM physical AS |-> memory_region_add_subregion // add region->mmaps[i].mem into region->mem MR |-> pci_register_bar 为了方便理解这个过程，我画了一张示意图： QEMU首先调用vfio_region_mmap， 通过mmap region->vbasedev->fd 把设备MMIO映射到QEMU进程的虚拟地址空间， 这实际上通过调用vfio-pci内核驱动vfio_pci_mmap -> remap_pfn_range， remap_pfn_range 是内核提供的API， 可以将一段连续的物理地址空间映射到进程的虚拟地址空间， 这里用它将设备的BAR空间的MMIO先映射到QEMU进程的虚拟地址空间再注册给虚拟机。 static int vfio_pci_mmap ( void * device_data , struct vm_area_struct * vma ) { req_len = vma -> vm_end - vma -> vm_start ; // MMIO size vma -> vm_pgoff = ( pci_resource_start ( pdev , index ) >> PAGE_SHIFT ) + pgoff ; // MMIO page address return remap_pfn_range ( vma , vma -> vm_start , vma -> vm_pgoff , req_len , vma -> vm_page_prot ); } 再来看下QEMU是如何注册这段虚拟地址(IOVA)到虚拟机的。 vfio_region_mmap调用memory_region_init_ram_device_ptr把前面mmap过来的 这段IOVA作为RAM类型设备注册给虚拟机。 int vfio_region_mmap ( VFIORegion * region ) { name = g_strdup_printf ( \"%s mmaps[%d]\" , memory_region_name ( region -> mem ), i ); memory_region_init_ram_device_ptr ( & region -> mmaps [ i ]. mem , memory_region_owner ( region -> mem ), name , region -> mmaps [ i ]. size , region -> mmaps [ i ]. mmap ); memory_region_add_subregion ( region -> mem , region -> mmaps [ i ]. offset , & region -> mmaps [ i ]. mem ); } memory_region_init_ram_device_ptr中会标志 mr->ram = true， 那么QEMU就会通过kvm_set_phys_mem注册这段内存给虚拟机（是RAM类型才会建立EPT映射关系）， 这样KVM就会为这段地址空间建立EPT页表， 虚拟机访问设备的MMIO空间时通过EPT页表翻直接访问不需要VM-Exit。 例如，网卡的收发包场景，虚拟机可以直接操作真实网卡的相关寄存器（MMIO映射）而没有陷入先出开销，大幅度提升了虚拟化场景下的I/O性能。 static void kvm_set_phys_mem ( KVMMemoryListener * kml , MemoryRegionSection * section , bool add ) { if ( ! memory_region_is_ram ( mr )) { // mr->ram = true 会注册到KVM if ( writeable || ! kvm_readonly_mem_allowed ) { return ; } else if ( ! mr -> romd_mode ) { /* If the memory device is not in romd_mode, then we actually want * to remove the kvm memory slot so all accesses will trap. */ add = false ; } } ram = memory_region_get_ram_ptr ( mr ) + section -> offset_within_region + ( start_addr - section -> offset_within_address_space ); kvm_set_user_memory_region // 作为RAM设备注册到KVM中 } 2.VFIO中如何实现MSI/MSI-X，Interrupt Remapping，以及Posted Interrupt的支持？ 对于VFIO设备直通而言，设备中断的处理方式共有4种: INTx 最传统的PCI设备引脚Pin方式 MSI/MSI-X方式 Interrupt Remapping方式 VT-d Posted Interrupt方式 那么它们分别是如何设计实现的呢？ 这里我们来重点探索一下MSI/MSI-X的实现方式以及VT-d Posted Interrupt方式。 如果忘了MSI和MSI-X的知识点可以看下《 PCI Local Bus Specification Revision 3.0 》的Chapter 6有比较详细的介绍。 先看下QEMU这边中断初始化和中断使能相关的函数调用关系图： vfio_realize |-> vfio_get_device // get device info: num_irqs, num_regions, flags |-> vfio_msix_early_setup // get MSI-X info: table_bar,table_offset, pba_ -> pci_device_route_intx_to_irqbar,pba_offset, entries |-> vfio_pci_fixup_msix_region |-> vfio_pci_relocate_msix |-> vfio_add_capabilities |-> vfio_add_std_cap |-> vfio_msi_setup -> msi_init |-> vfio_msix_setup -> msix_init |-> vfio_intx_enable // enable intx |-> pci_device_route_intx_to_irq |-> event_notifier_init & vdev -> intx . interrupt |-> ioctl VFIO_DEVICE_SET_IRQS kvm_cpu_exec ... |-> vfio_pci_write_config |-> vfio_msi_enable |-> event_notifier_init // init eventfd as irqfd |-> vfio_add_kvm_msi_virq ... -> kvm_irqchip_assign_irqfd |-> vfio_enable_vectors false |-> vfio_msix_enable |-> vfio_msix_vector_do_use -> msix_vector_use |-> vfio_msix_vector_release |-> msix_set_vector_notifiers 从图中可以看出，直通设备初始化时候会从物理设备的PCI配置空间读取INTx、MSI、MSI-X的相关信息并且进行一些必要的初始化（setup）再进行中断使能（enable）的。 根据调试的结果来看，INTx的enable是最早的， 而MSI/MSI-X初始化是在guest启动后进行enable。 这里以MSI-X为例， 首先调用vfio_msix_early_setup函数从硬件设备的PCI配置空间查询MSI-X相关信息包括: MSI-X Table Size ：MSI-X Table 大小 MSI-X Table BAR Indicator ：MSI-X Table存放的BAR空间编号 MSI-X Table Offset ：存放MSI-X Table在Table BAR空间中的偏移量 MSI-X PBA BIR ：存放MSI-X 的Pending Bit Array的BAR空间编号 MSI-X PBA Offset ：存放MSI-X Table在PBA BAR空间中的偏移量 获取必要信息之后，通过vfio_msix_setup来完成直通设备的MSI-X的初始化工作， 包括调用pci_add_capability为设备添加PCI_CAP_ID_MSIX Capability， 并注册MSI-X的BAR空间到虚拟机的物理地址空间等。 static void vfio_msix_early_setup ( VFIOPCIDevice * vdev , Error ** errp ) { if ( pread ( fd , & ctrl , sizeof ( ctrl ), vdev -> config_offset + pos + PCI_MSIX_FLAGS ) != sizeof ( ctrl )) { error_setg_errno ( errp , errno , \"failed to read PCI MSIX FLAGS\" ); return ; } if ( pread ( fd , & table , sizeof ( table ), vdev -> config_offset + pos + PCI_MSIX_TABLE ) != sizeof ( table )) { error_setg_errno ( errp , errno , \"failed to read PCI MSIX TABLE\" ); return ; } if ( pread ( fd , & pba , sizeof ( pba ), vdev -> config_offset + pos + PCI_MSIX_PBA ) != sizeof ( pba )) { error_setg_errno ( errp , errno , \"failed to read PCI MSIX PBA\" ); return ; } ctrl = le16_to_cpu ( ctrl ); table = le32_to_cpu ( table ); pba = le32_to_cpu ( pba ); msix = g_malloc0 ( sizeof ( * msix )); msix -> table_bar = table & PCI_MSIX_FLAGS_BIRMASK ; msix -> table_offset = table & ~ PCI_MSIX_FLAGS_BIRMASK ; msix -> pba_bar = pba & PCI_MSIX_FLAGS_BIRMASK ; msix -> pba_offset = pba & ~ PCI_MSIX_FLAGS_BIRMASK ; msix -> entries = ( ctrl & PCI_MSIX_FLAGS_QSIZE ) + 1 ; } static int vfio_msix_setup ( VFIOPCIDevice * vdev , int pos , Error ** errp ) { vdev -> msix -> pending = g_malloc0 ( BITS_TO_LONGS ( vdev -> msix -> entries ) * sizeof ( unsigned long )); ret = msix_init ( & vdev -> pdev , vdev -> msix -> entries , vdev -> bars [ vdev -> msix -> table_bar ]. mr , vdev -> msix -> table_bar , vdev -> msix -> table_offset , vdev -> bars [ vdev -> msix -> pba_bar ]. mr , vdev -> msix -> pba_bar , vdev -> msix -> pba_offset , pos , & err ); memory_region_set_enabled ( & vdev -> pdev . msix_pba_mmio , false ); if ( object_property_get_bool ( OBJECT ( qdev_get_machine ()), \"vfio-no-msix-emulation\" , NULL )) { memory_region_set_enabled ( & vdev -> pdev . msix_table_mmio , false ); } } 最后guest启动后调用vfio_msix_enable使能MSI-X中断。 irqfd 和 ioeventfd 我们知道QEMU本身有一套完整的模拟PCI设备INTx、MSI、MSI-X中断机制， 其实现方式是irqfd（QEMU中断注入到guest）和ioeventfd（guest中断通知到QEMU）， 内部实现都是基于内核提供的eventfd机制。 看代码的时候一直没明白设备中断绑定的irqfd是在什么时候注册的，从代码上看不是中断enable时候。 后来结合代码调试才明白， 原来中断enable时候将设备的MSI/MSI-X BAR空间映射用MMIO方式注册给了虚拟机（参考msix_init, msi_init函数实现）， 当虚拟机内部第一次访问MSI-X Table BAR空间的MMIO时会退出到用户态完成irqfd的注册， 调用堆栈为： #0 kvm_irqchip_assign_irqfd #1 in kvm_irqchip_add_irqfd_notifier_gsi #2 in vfio_add_kvm_msi_virq #3 in vfio_msix_vector_do_use #4 in vfio_msix_vector_use #5 in msix_fire_vector_notifier #6 in msix_handle_mask_update #7 in msix_table_mmio_write #8 in memory_region_write_accessor #9 in access_with_adjusted_size #10 in memory_region_dispatch_write #11 in address_space_write_continue #12 in address_space_write #13 in address_space_rw #14 in kvm_cpu_exec 关于irqfd的社区patch可以从这里获取 https://lwn.net/Articles/332924 。 备注：Alex Williamson在最新的VFIO模块中加入新特性，支持直接使用物理设备的MSIX BAR空间， 这样一来可以直接将物理设备的MSI-X BAR空间直接mmap过来然后呈现给虚拟机， guest直接使用而不用再进行模拟了。 commit ae0215b2bb56a9d5321a185dde133bfdd306a4c0 Author : Alexey Kardashevskiy < aik @ozlabs . ru > Date : Tue Mar 13 11 : 17 : 31 2018 - 0600 vfio - pci : Allow mmap of MSIX BAR At the moment we unconditionally avoid mapping MSIX data of a BAR and emulate MSIX table in QEMU . However it is 1 ) not always necessary as a platform may provide a paravirt interface for MSIX configuration ; 2 ) can affect the speed of MMIO access by emulating them in QEMU when frequently accessed registers share same system page with MSIX data , this is particularly a problem for systems with the page size bigger than 4 KB . A new capability - VFIO_REGION_INFO_CAP_MSIX_MAPPABLE - has been added to the kernel [ 1 ] which tells the userspace that mapping of the MSIX data is possible now . This makes use of it so from now on QEMU tries mapping the entire BAR as a whole and emulate MSIX on top of that . [ 1 ] https : // git . kernel . org / pub / scm / linux / kernel / git / torvalds / linux . git / commit / ? id = a32295c612c57990d17fb0f41e7134394b2f35f6 Signed - off - by : Alexey Kardashevskiy < aik @ozlabs . ru > Reviewed - by : David Gibson < david @gibson . dropbear . id . au > Signed - off - by : Alex Williamson < alex . williamson @redhat . com > 3. VFIO中是如何建立DMA Remapping映射关系？ 前面的文章中我们反复提到VT-d DMA Remapping的原理和意义，那么在vfio中这又是如何实现的呢？ 实现的原理其实不难，我们知道QEMU会维护虚拟机的物理地址空间映射关系， 而VT-d DMA Remapping需要建立GPA->HVA地址空间的映射关系， 那么当虚拟机的地址空间布局发生变化时我们都会尝试更新对应的DMA Remapping关系， 这是通过vfio_memory_listener来实现的。 static const MemoryListener vfio_memory_listener = { . region_add = vfio_listener_region_add , . region_del = vfio_listener_region_del , }; vfio_connect_container函数中会将vfio_memory_listener注册给QEMU的物理地址空间address_space_memory， 这样vfio_memory_listener会监听虚拟机的物理地址空间变化， 调用对应的回调函数更新DMA Remapping关系。 memory_listener_register ( & container -> listener , container -> space -> as ); 那么说了很久的IOMMU页表是如何创建和更新的呢？ 看下vfio_listener_region_add/vfio_listener_region_del函数的实现就知道。 在该函数中先check对应的section是否是RAM（只对RAM类型的区域进行DMA Remapping）， 再进行一些Sanity Check后调用vfio_dma_map将映射关系建立起来， 所以重点还是在于vfio_dma_map的函数实现。 static void vfio_listener_region_add ( MemoryListener * listener , MemoryRegionSection * section ) { VFIOContainer * container = container_of ( listener , VFIOContainer , listener ); hwaddr iova , end ; Int128 llend , llsize ; void * vaddr ; int ret ; VFIOHostDMAWindow * hostwin ; bool hostwin_found ; if ( vfio_listener_skipped_section ( section )) { // do dma_map only if MRS is RAM type trace_vfio_listener_region_add_skip ( section -> offset_within_address_space , section -> offset_within_address_space + int128_get64 ( int128_sub ( section -> size , int128_one ()))); return ; } if ( unlikely (( section -> offset_within_address_space & ~ TARGET_PAGE_MASK ) != ( section -> offset_within_region & ~ TARGET_PAGE_MASK ))) { error_report ( \"%s received unaligned region\" , __func__ ); return ; } iova = TARGET_PAGE_ALIGN ( section -> offset_within_address_space ); llend = int128_make64 ( section -> offset_within_address_space ); llend = int128_add ( llend , section -> size ); llend = int128_and ( llend , int128_exts64 ( TARGET_PAGE_MASK )); if ( int128_ge ( int128_make64 ( iova ), llend )) { return ; } end = int128_get64 ( int128_sub ( llend , int128_one ())); if ( container -> iommu_type == VFIO_SPAPR_TCE_v2_IOMMU ) { hwaddr pgsize = 0 ; /* For now intersections are not allowed, we may relax this later */ QLIST_FOREACH ( hostwin , & container -> hostwin_list , hostwin_next ) { if ( ranges_overlap ( hostwin -> min_iova , hostwin -> max_iova - hostwin -> min_iova + 1 , section -> offset_within_address_space , int128_get64 ( section -> size ))) { ret = -1 ; goto fail ; } } ret = vfio_spapr_create_window ( container , section , & pgsize ); if ( ret ) { goto fail ; } vfio_host_win_add ( container , section -> offset_within_address_space , section -> offset_within_address_space + int128_get64 ( section -> size ) - 1 , pgsize ); #ifdef CONFIG_KVM if ( kvm_enabled ()) { VFIOGroup * group ; IOMMUMemoryRegion * iommu_mr = IOMMU_MEMORY_REGION ( section -> mr ); struct kvm_vfio_spapr_tce param ; struct kvm_device_attr attr = { . group = KVM_DEV_VFIO_GROUP , . attr = KVM_DEV_VFIO_GROUP_SET_SPAPR_TCE , . addr = ( uint64_t )( unsigned long ) & param , }; if ( ! memory_region_iommu_get_attr ( iommu_mr , IOMMU_ATTR_SPAPR_TCE_FD , & param . tablefd )) { QLIST_FOREACH ( group , & container -> group_list , container_next ) { param . groupfd = group -> fd ; if ( ioctl ( vfio_kvm_device_fd , KVM_SET_DEVICE_ATTR , & attr )) { error_report ( \"vfio: failed to setup fd %d \" \"for a group with fd %d: %s\" , param . tablefd , param . groupfd , strerror ( errno )); return ; } trace_vfio_spapr_group_attach ( param . groupfd , param . tablefd ); } } } #endif } hostwin_found = false ; QLIST_FOREACH ( hostwin , & container -> hostwin_list , hostwin_next ) { if ( hostwin -> min_iova <= iova && end <= hostwin -> max_iova ) { hostwin_found = true ; break ; } } if ( ! hostwin_found ) { error_report ( \"vfio: IOMMU container %p can't map guest IOVA region\" \" 0x%\" HWADDR_PRIx \"..0x%\" HWADDR_PRIx , container , iova , end ); ret = - EFAULT ; goto fail ; } memory_region_ref ( section -> mr ); // increase ref of MR by one if ( memory_region_is_iommu ( section -> mr )) { // guest IOMMU emulation VFIOGuestIOMMU * giommu ; IOMMUMemoryRegion * iommu_mr = IOMMU_MEMORY_REGION ( section -> mr ); trace_vfio_listener_region_add_iommu ( iova , end ); /* * FIXME: For VFIO iommu types which have KVM acceleration to * avoid bouncing all map/unmaps through qemu this way, this * would be the right place to wire that up (tell the KVM * device emulation the VFIO iommu handles to use). */ giommu = g_malloc0 ( sizeof ( * giommu )); giommu -> iommu = iommu_mr ; giommu -> iommu_offset = section -> offset_within_address_space - section -> offset_within_region ; giommu -> container = container ; llend = int128_add ( int128_make64 ( section -> offset_within_region ), section -> size ); llend = int128_sub ( llend , int128_one ()); iommu_notifier_init ( & giommu -> n , vfio_iommu_map_notify , IOMMU_NOTIFIER_ALL , section -> offset_within_region , int128_get64 ( llend )); QLIST_INSERT_HEAD ( & container -> giommu_list , giommu , giommu_next ); memory_region_register_iommu_notifier ( section -> mr , & giommu -> n ); memory_region_iommu_replay ( giommu -> iommu , & giommu -> n ); return ; } /* Here we assume that memory_region_is_ram(section->mr)==true */ vaddr = memory_region_get_ram_ptr ( section -> mr ) + // get hva section -> offset_within_region + ( iova - section -> offset_within_address_space ); trace_vfio_listener_region_add_ram ( iova , end , vaddr ); llsize = int128_sub ( llend , int128_make64 ( iova )); // calc map size if ( memory_region_is_ram_device ( section -> mr )) { hwaddr pgmask = ( 1ULL << ctz64 ( hostwin -> iova_pgsizes )) - 1 ; if (( iova & pgmask ) || ( int128_get64 ( llsize ) & pgmask )) { trace_vfio_listener_region_add_no_dma_map ( memory_region_name ( section -> mr ), section -> offset_within_address_space , int128_getlo ( section -> size ), pgmask + 1 ); return ; } } ret = vfio_dma_map ( container , iova , int128_get64 ( llsize ), // do VFIO_IOMMU_MAP_DMA vaddr , section -> readonly ); if ( ret ) { error_report ( \"vfio_dma_map(%p, 0x%\" HWADDR_PRIx \", \" \"0x%\" HWADDR_PRIx \", %p) = %d (%m)\" , container , iova , int128_get64 ( llsize ), vaddr , ret ); if ( memory_region_is_ram_device ( section -> mr )) { /* Allow unexpected mappings not to be fatal for RAM devices */ return ; } goto fail ; } return ; fail : if ( memory_region_is_ram_device ( section -> mr )) { error_report ( \"failed to vfio_dma_map. pci p2p may not work\" ); return ; } /* * On the initfn path, store the first error in the container so we * can gracefully fail. Runtime, there's not much we can do other * than throw a hardware error. */ if ( ! container -> initialized ) { if ( ! container -> error ) { container -> error = ret ; } } else { hw_error ( \"vfio: DMA mapping failed, unable to continue\" ); } } vfio_dma_map函数中会传入建立DMA Remapping的基本信息， 这里是用数据结构vfio_iommu_type1_dma_map来描述， 然后交给内核去做DMA Remapping。 struct vfio_iommu_type1_dma_map map = { . argsz = sizeof ( map ), . flags = VFIO_DMA_MAP_FLAG_READ , // flags . vaddr = ( __u64 )( uintptr_t ) vaddr , // HVA . iova = iova , // iova -> gpa . size = size , // map size }; 该函数对应的内核调用栈见下面的图，其主要流程包含2个步骤，简称pin和map： vfio_pin_pages_remote 把虚拟机的物理内存都pin住，物理内存不能交换 vfio_iommu_map 创建虚拟机domain的IOMMU页表，DMA Remapping地址翻译时候使用 值得注意的是在pin步骤中， pin虚拟机物理内存是调用get_user_pages_fast来实现的， 如果虚拟机的内存未申请那么会先将内存申请出来， 这个过程可能会非常耗时并且会持有进程的mmap_sem大锁。 vfio_dma_do_map -------------------------------------------------------------- vfio_pin_map_dma |-> vfio_pin_pages_remote // pin pages in memory |-> vaddr_get_pfn | get_user_pages_fast | gup_pud_range gup_pud_range gup_pte_range get_page // pin page, without mm->mmap_sem held | get_user_pages_unlocked __get_user_pages_locked __get_user_pages | handle_mm_fault __handle_mm_fault // do_page_fault process alloc pud -> pmd -> pte handle_pte_fault do_anonymous_page alloc_zeroed_user_highpage_movable ... alloc page with __GFP_ZERO | _get_page atomic_inc ( & page_count ) // pin page | get_user_pages_remote __get_user_pages_locked |-> vfio_iommu_map // create IOMMU domain page table |-> iommu_map |-> intel_iommu_map // intel-iommu.c |-> domain_pfn_mapping |-> pfn_to_dma_pte 同理，vfio_listener_region_add回调函数实现了DMA Remapping关系的反注册。 4. VFIO中又是如何支持设备热插拔的？ vfio热插拔仍然是走的QEMU设备热插拔流程，大致流程如下： qdev_device_add |-> device_set_realized |-> hotplug_handler_plug |-> piix4_device_plug_cb |-> piix4_send_gpe // inject ACPI GPE to guest OS |-> pci_qdev_realize |-> vfio_realize device_set_realized函数负责将设备上线， piix4_send_gpe函数中会注入一个ACPI GPE事件通知GuestOS， vfio_realize函数负责整个vfio的设备初始化流程。 那么vfio_realize中主要做了哪些事情呢？ 可以用下面这一张图概括~ 点击链接查看原图 vfio_realize的具体流程这里就不再展开免得啰嗦！ 感兴趣的请自己分析代码去，所有的vfio实现细节都在这里面。","tags":"virtualization"},{"title":"Intel IOMMU Introduction","url":"https://kernelgo.org/intel_iommu.html","loc":"https://kernelgo.org/intel_iommu.html","text":"对于Intel的硬件辅助虚拟化方案而言核心的两大技术分别是VT-x和VT-d。 其中VT-x中主要引入了non-root模式(VMCS)以及EPT页表等技术，主要关注于vCPU的虚拟化和内存虚拟化。 而VT-d的引入则是重点关注设备直通(passthrough)方面（即IO虚拟化）。 VT-x中在non-root模式下，MMU直接使用EPT page table来完成GPA->HVA->HPA的两级翻译， VT-d中在non-root模式下，则由IOMMU来使用Context Table和IOMMU page table完成设备DMA请求过程中的HPA->HVA->GPA的翻译． 二者极为相似，唯一的不同之处在于CPU访问内存（直通设备IO Memory）是通过MMU查找EPT页表完成地址翻译， 而直通设备访问内存的请求则是通过IOMMU查找IOMMU页表来完成地址翻译的。本文重点来探索一下Intel IOMMU的工作机制。 硬件结构 先看下一个典型的X86物理服务器视图： 在多路服务器上我们可以有多个DMAR Unit（这里可以直接理解为多个IOMMU硬件）， 每个DMAR会负责处理其下挂载设备的DMA请求进行地址翻译。 例如上图中， PCIE Root Port (dev:fun) (14:0)下面挂载的所有设备的DMA请求由DMAR #1负责处理， PCIE Root Port (dev:fun) (14:1)下面挂载的所有设备的DMA请求由DMAR #2负责处理， 而DMAR #3下挂载的是一个Root-Complex集成设备[29:0]，这个设备的DMA请求被DMAR #3承包， DMAR #4的情况比较复杂，它负责处理Root-Complex集成设备[30:0]以及I/OxAPIC设备的DMA请求。 这些和IOMMU相关的硬件拓扑信息需要BIOS通过ACPI表呈现给OS，这样OS才能正确驱动IOMMU硬件工作。 关于硬件拓扑信息呈现，这里有几个概念需要了解一下： DRHD: DMA Remapping Hardware Unit Definition 用来描述DMAR Unit(IOMMU)的基本信息 RMRR: Reserved Memory Region Reporting 用来描述那些保留的物理地址，这段地址空间不被重映射 ATSR: Root Port ATS Capability 仅限于有Device-TLB的情形，Root Port需要向OS报告支持ATS的能力 RHSA: Remapping Hardware Static Affinity Remapping亲和性，在有NUMA的系统下可以提升DMA Remapping的性能 BIOS通过在ACPI表中提供一套DMA Remapping Reporting Structure 信息来表述物理服务器上的IOMMU拓扑信息， 这样OS在加载IOMMU驱动的时候就知道如何建立映射关系了。 附：我们可以使用一些工具将ACPI表相关信息Dump出来查看 # acpidump --table DMAR -b > dmar.out # iasl -d dmar.out # cat dmar.dsl 数据结构 Intel IOMMU Driver的关键数据结构可以描述为（ 点击链接查看原图 ）： 按照自上而下的视图来看，首先是IOMMU硬件层面， struct dmar_drhd_unit数据结构从系统BIOS角度去描述了一个IOMMU硬件： list 用来把所有的DRHD串在一个链表中便于维护 acpi_dmar_head *hdr 指向IOMMU设备的ACPI表信息 device_cnt 表示当前IOMMU管理的设备数量 include_all 表示该IOMMU是否管理平台上所有的设备（单IOMMU的物理物理服务器） reg_base_addr 表示IOMMU的寄存器基地址 intel_iommu *iommu 指针指向struct intel_iommu数据结构 struct intel_iommu 进一步详细描述了IOMMU的所以相关信息 cap和ecap 记录IOMMU硬件的Capability和Extended Capability信息 root_entry 指向了此IOMMU的Root Entry Table ir_table 指向了IOMMU的Interrupt Remapping Table（中断重映射表） struct iommu_device iommu 从linux设备驱动的角度描述这个IOMMU并用来绑定sysfs struct dmar_domain ***domains 比较关键，它记录了这个IOMMU下面管理的所有dmar_domain信息 在虚拟化场景下多个设备可以直通给同一个虚拟机，他们共享一个IOMMU Page Table， 这种映射关系就是通过DMAR Domain来表述的， 也就是说多个直通设备可以加入到一个DMAR Domain中， 他们之间使用同一套页表完成地址DMA 请求的地址翻译。 那我们接着往下走，来看DMAR Domain： struct dmar_domain 数据结构用来描述DMAR Domain这种映射关系的 struct list_head devices 链表记录了这个Domain中的所有设备 struct iova_domain iovad 数据结构用一个红黑树来记录iova->hpa的地址翻译关系 struct dma_pte *pgd 这个指针指向了IOMMU页表的基地址是IOMMU页表的入口 bool has_iotlb_device 表示这个Domain里是否有具备IO-TLB的设备 struct iommu_domain domain 主要包含了iommu_ops *ops指针，记录了一堆与domain相关的操作 Intel IOMMU初始化 首先探测平台环境上是否有IOMMU硬件：IOMMU_INIT_POST(detect_intel_iommu)， detect_intel_iommu函数中调用dmar_table_detect函数从ACPI表中查询DMAR相关内容： /** * dmar_table_detect - checks to see if the platform supports DMAR devices */ static int __init dmar_table_detect ( void ) { acpi_status status = AE_OK ; /* if we could find DMAR table, then there are DMAR devices */ status = acpi_get_table ( ACPI_SIG_DMAR , 0 , & dmar_tbl ); if ( ACPI_SUCCESS ( status ) && ! dmar_tbl ) { pr_warn ( \"Unable to map DMAR \\n \" ); status = AE_NOT_FOUND ; } return ACPI_SUCCESS ( status ) ? 0 : - ENOENT ; } 如果查询到信息就validate_drhd_cb验证DRHD的有效性设置iommu_detected = 1， 如果查询不到DMAR信息那么认为没有IOMMU硬件，跳过后续初始化流程。 接着pci_iommu_init中调用x86_init.iommu.iommu_init()来初始化Intel IOMMU，主要的流程为： intel_iommu_init |-> dmar_table_init -> parse_dmar_table -> dmar_walk_dmar_table //重点分析 |-> dmar_dev_scope_init |-> dmar_acpi_dev_scope_init -> dmar_acpi_insert_dev_scope //重点分析 |-> dmar_pci_bus_add_dev -> dmar_insert_dev_scope |-> bus_register_notifier |-> dmar_init_reserved_ranges // init RMRR |-> init_no_remapping_devices // init no remapping devices |-> init_dmars //重点分析 |-> dma_ops = & intel_dma_ops |-> iommu_device_sysfs_add , iommu_device_set_ops , iommu_device_register |-> bus_set_iommu ( & pci_bus_type , & intel_iommu_ops ) |-> bus_register_notifier ( & pci_bus_type , & device_nb ) 在dmar_table_init函数中我们完成了DMA Remapping相关的ACPI表解析流程，这个parse_dmar_table的函数实现非常精妙，不禁让人感叹！它将每种Remapping Structure Types的解析函数封装成dmar_res_callback，然后调用dmar_walk_dmar_table通过一个for循环撸一遍就完成了全部的解析，代码精简思路清晰、一气呵成。 static int __init parse_dmar_table ( void ) { struct acpi_table_dmar * dmar ; int drhd_count = 0 ; int ret ; struct dmar_res_callback cb = { . print_entry = true , . ignore_unhandled = true , . arg [ ACPI_DMAR_TYPE_HARDWARE_UNIT ] = & drhd_count , . cb [ ACPI_DMAR_TYPE_HARDWARE_UNIT ] = & dmar_parse_one_drhd , . cb [ ACPI_DMAR_TYPE_RESERVED_MEMORY ] = & dmar_parse_one_rmrr , . cb [ ACPI_DMAR_TYPE_ROOT_ATS ] = & dmar_parse_one_atsr , . cb [ ACPI_DMAR_TYPE_HARDWARE_AFFINITY ] = & dmar_parse_one_rhsa , . cb [ ACPI_DMAR_TYPE_NAMESPACE ] = & dmar_parse_one_andd , }; /* * Do it again, earlier dmar_tbl mapping could be mapped with * fixed map. */ dmar_table_detect (); // 重新detect dmar table /* * ACPI tables may not be DMA protected by tboot, so use DMAR copy * SINIT saved in SinitMleData in TXT heap (which is DMA protected) */ dmar_tbl = tboot_get_dmar_table ( dmar_tbl ); dmar = ( struct acpi_table_dmar * ) dmar_tbl ; if ( ! dmar ) return - ENODEV ; if ( dmar -> width < PAGE_SHIFT - 1 ) { pr_warn ( \"Invalid DMAR haw \\n \" ); return - EINVAL ; } pr_info ( \"Host address width %d \\n \" , dmar -> width + 1 ); ret = dmar_walk_dmar_table ( dmar , & cb ); //遍历ACPI表完成解析 if ( ret == 0 && drhd_count == 0 ) pr_warn ( FW_BUG \"No DRHD structure found in DMAR table \\n \" ); return ret ; } dmar_dev_scope_init函数负责完成IOMMU的Device Scope解析。 dmar_acpi_insert_dev_scope中多层的遍历，建立了IOMMU和设备之间的映射关系。 static void __init dmar_acpi_insert_dev_scope ( u8 device_number , struct acpi_device * adev ) { struct dmar_drhd_unit * dmaru ; struct acpi_dmar_hardware_unit * drhd ; struct acpi_dmar_device_scope * scope ; struct device * tmp ; int i ; struct acpi_dmar_pci_path * path ; for_each_drhd_unit ( dmaru ) { drhd = container_of ( dmaru -> hdr , struct acpi_dmar_hardware_unit , header ); for ( scope = ( void * )( drhd + 1 ); ( unsigned long ) scope < (( unsigned long ) drhd ) + drhd -> header . length ; scope = (( void * ) scope ) + scope -> length ) { if ( scope -> entry_type != ACPI_DMAR_SCOPE_TYPE_NAMESPACE ) continue ; if ( scope -> enumeration_id != device_number ) continue ; path = ( void * )( scope + 1 ); pr_info ( \"ACPI device \\\" %s \\\" under DMAR at %llx as %02x:%02x.%d \\n \" , dev_name ( & adev -> dev ), dmaru -> reg_base_addr , scope -> bus , path -> device , path -> function ); for_each_dev_scope ( dmaru -> devices , dmaru -> devices_cnt , i , tmp ) if ( tmp == NULL ) { dmaru -> devices [ i ]. bus = scope -> bus ; dmaru -> devices [ i ]. devfn = PCI_DEVFN ( path -> device , path -> function ); rcu_assign_pointer ( dmaru -> devices [ i ]. dev , get_device ( & adev -> dev )); return ; } BUG_ON ( i >= dmaru -> devices_cnt ); } } pr_warn ( \"No IOMMU scope found for ANDD enumeration ID %d (%s) \\n \" , device_number , dev_name ( & adev -> dev )); } init_dmars函数最后再对描述IOMMU的intel_iommu结构进行初始化，主要的流程包括： init_dmars |-> intel_iommu_init_qi // qeueu invalidation |-> iommu_init_domains |-> init_translation_status |-> iommu_alloc_root_entry //创建Root Entry |-> translation_pre_enabled |-> iommu_set_root_entry |-> iommu_prepare_rmrr_dev |-> iommu_prepare_isa // 0-16MiB 留给ISA设备 |-> dmar_set_interrupt // IOMMU中断初始化 这里不再展开，但每个点都值得探索一下，例如： IOMMU中断是用来做什么的？ iommu_prepare_identity_map 是在做什么？ 一个IOMMU最多支持多少个DMAR Domain？ qeueue invalidation是用来做什么的？ 可以多问自己一些问题带着问题去看代码，从代码中找到答案，从更深层次去分析问题，理解特性。 参考文献 https://software.intel.com/sites/default/files/managed/c5/15/vt-directed-io-spec.pdf https://elixir.bootlin.com/linux/v4.16.12/source/drivers/iommu/intel-iommu.c","tags":"virtualization"},{"title":"MMIO Emulation","url":"https://kernelgo.org/mmio.html","loc":"https://kernelgo.org/mmio.html","text":"我们知道X86体系结构上对设备进行访问可以通过PIO方式和MMIO(Memory Mapped I/O)两种方式进行， 那么QEMU-KVM具体是如何实现设备MMIO访问的呢？ MMIO是直接将设备I/O映射到物理地址空间内，虚拟机物理内存的虚拟化又是通过EPT机制来完成的， 那么模拟设备的MMIO实现也需要利用EPT机制．虚拟机的EPT页表是在 EPT_VIOLATION 异常处理的时候建立起来的， 对于模拟设备而言访问MMIO肯定要触发 VM_EXIT 然后交给QEMU/KVM去处理，那么怎样去标志MMIO访问异常呢？ 查看Intel SDM知道这是通过利用 EPT_MISCONFIG 来实现的．那么 EPT_VIOLATION 与 EPT_MISCONFIG 的区别是什么? EXIT_REASON_EPT_VIOLATION is similar to a \"page not present\" pagefault. EXIT_REASON_EPT_MISCONFIG is similar to a \"reserved bit set\" pagefault. EPT_VIOLATION 表示的是对应的物理页不存在，而 EPT_MISCONFIG 表示EPT页表中有非法的域． 那么这里有２个问题需要弄清楚． 1 KVM如何标记EPT是MMIO类型 ? hardware_setup 时候虚拟机如果开启了ept支持就调用 ept_set_mmio_spte_mask 初始化 shadow_mmio_mask ， 设置EPT页表项最低3bit为：110b就会触发 ept_msconfig （110b表示该页可读可写但是还未分配或者不存在，这显然是一个错误的EPT页表项）. static void ept_set_mmio_spte_mask ( void ) { /* * EPT Misconfigurations can be generated if the value of bits 2:0 * of an EPT paging-structure entry is 110b (write/execute). */ kvm_mmu_set_mmio_spte_mask ( VMX_EPT_RWX_MASK , VMX_EPT_MISCONFIG_WX_VALUE ); } 同时还要对EPT的一些特殊位进行标记来标志该spte表示MMIO而不是虚拟机的物理内存，例如这里 (1)set the special mask: SPTE_SPECIAL_MASK． (2)reserved physical address bits: the setting of a bit in the range 51:12 that is beyond the logical processor's physical-address width 关于EPT_MISCONFIG在SDM中有详细说明． void kvm_mmu_set_mmio_spte_mask ( u64 mmio_mask , u64 mmio_value ) { BUG_ON (( mmio_mask & mmio_value ) != mmio_value ); shadow_mmio_value = mmio_value | SPTE_SPECIAL_MASK ; shadow_mmio_mask = mmio_mask | SPTE_SPECIAL_MASK ; } EXPORT_SYMBOL_GPL ( kvm_mmu_set_mmio_spte_mask ); static void kvm_set_mmio_spte_mask ( void ) { u64 mask ; int maxphyaddr = boot_cpu_data . x86_phys_bits ; /* * Set the reserved bits and the present bit of an paging-structure * entry to generate page fault with PFER.RSV = 1. */ /* Mask the reserved physical address bits. */ mask = rsvd_bits ( maxphyaddr , 51 ); /* Set the present bit. */ mask |= 1ull ; #ifdef CONFIG_X86_64 /* * If reserved bit is not supported, clear the present bit to disable * mmio page fault. */ if ( maxphyaddr == 52 ) mask &= ~ 1ull ; #endif kvm_mmu_set_mmio_spte_mask ( mask , mask ); } KVM在建立EPT页表项之后设置了这些标志位再访问对应页的时候会触发EPT_MISCONFIG退出了，然后调用 handle_ept_misconfig --> handle_mmio_page_fault 来完成MMIO处理操作． KVM内核相关代码 ： handle_ept_misconfig --> kvm_emulate_instruction --> x86_emulate_instruction --> x86_emulate_insn writeback --> segmented_write --> emulator_write_emulated --> emulator_read_write --> emulator_read_write_onepage --> ops -> read_write_mmio [ write_mmio ] --> vcpu_mmio_write --> kvm_io_bus_write --> __kvm_io_bus_write --> kvm_iodevice_write --> dev -> ops -> write [ ioeventfd_write ] 最后会调用到 ioeventfd_write ，写 eventfd给QEMU发送通知事件 /* MMIO/PIO writes trigger an event if the addr/val match */ static int ioeventfd_write ( struct kvm_vcpu * vcpu , struct kvm_io_device * this , gpa_t addr , int len , const void * val ) { struct _ioeventfd * p = to_ioeventfd ( this ); if ( ! ioeventfd_in_range ( p , addr , len , val )) return - EOPNOTSUPP ; eventfd_signal ( p -> eventfd , 1 ); return 0 ; } 2 QEMU如何标记设备的MMIO ? 这里以e1000网卡模拟为例，设备初始化MMIO时候时候注册的MemoryRegion为IO类型（不是RAM类型）． static void e1000_mmio_setup ( E1000State * d ) { int i ; const uint32_t excluded_regs [] = { E1000_MDIC , E1000_ICR , E1000_ICS , E1000_IMS , E1000_IMC , E1000_TCTL , E1000_TDT , PNPMMIO_SIZE }; // 这里注册MMIO，调用memory_region_init_io，mr->ram = false！！！ memory_region_init_io ( & d -> mmio , OBJECT ( d ), & e1000_mmio_ops , d , \"e1000-mmio\" , PNPMMIO_SIZE ); memory_region_add_coalescing ( & d -> mmio , 0 , excluded_regs [ 0 ]); for ( i = 0 ; excluded_regs [ i ] != PNPMMIO_SIZE ; i ++ ) memory_region_add_coalescing ( & d -> mmio , excluded_regs [ i ] + 4 , excluded_regs [ i + 1 ] - excluded_regs [ i ] - 4 ); memory_region_init_io ( & d -> io , OBJECT ( d ), & e1000_io_ops , d , \"e1000-io\" , IOPORT_SIZE ); } 结合QEMU-KVM内存管理知识我们知道， QEMU调用 kvm_set_phys_mem 注册虚拟机的物理内存到KVM相关的数据结构中的时候 会调用 memory_region_is_ram 来判断该段物理地址空间是否是RAM设备， 如果不是RAM设备直接return了． static void kvm_set_phys_mem ( KVMMemoryListener * kml , MemoryRegionSection * section , bool add ) { ...... if ( ! memory_region_is_ram ( mr )) { if ( writeable || ! kvm_readonly_mem_allowed ) { return ; // 设备MR不是RAM但可以写，那么这里直接return不注册到kvm里面 } else if ( ! mr -> romd_mode ) { /* If the memory device is not in romd_mode, then we actually want * to remove the kvm memory slot so all accesses will trap. */ add = false ; } } ...... } 对于MMIO类型的内存QEMU不会调用 kvm_set_user_memory_region 对其进行注册， 那么KVM会认为该段内存的pfn类型为 KVM_PFN_NOSLOT ， 进而调用 set_mmio_spte 来设置该段地址对应到spte， 而该函数中会判断pfn是否为NOSLOT标记以确认这段地址空间为MMIO． static bool set_mmio_spte ( struct kvm_vcpu * vcpu , u64 * sptep , gfn_t gfn , kvm_pfn_t pfn , unsigned access ) { if ( unlikely ( is_noslot_pfn ( pfn ))) { mark_mmio_spte ( vcpu , sptep , gfn , access ); return true ; } return false ; } 3 总结 MMIO是通过设置spte的保留位来标志的． 虚拟机内部第一次访问MMIO的gpa时，发生了EPT_VIOLATION然后check gpa发现对应的pfn不存在（QEMU没有注册），那么认为这是个MMIO，于是 set_mmio_spte 来标志它的spte是一个MMIO． 后面再次访问这个gpa时就发生EPT_MISCONFIG了，进而愉快地调用 handle_ept_misconfig -> handle_mmio_page_fault -> x86_emulate_instruction 来处理所有的MMIO操作了．","tags":"virtualization"},{"title":"VFIO Introduction","url":"https://kernelgo.org/vfio-introduction.html","loc":"https://kernelgo.org/vfio-introduction.html","text":"Virtual Function I/O (VFIO) 是一种现代化的设备直通方案，它充分利用了VT-d/AMD-Vi技术提供的DMA Remapping和Interrupt Remapping特性， 在保证直通设备的DMA安全性同时可以达到接近物理设备的I/O的性能。 用户态进程可以直接使用VFIO驱动直接访问硬件，并且由于整个过程是在IOMMU的保护下进行因此十分安全， 而且非特权用户也是可以直接使用。 换句话说，VFIO是一套完整的用户态驱动(userspace driver)方案，因为它可以安全地把设备I/O、中断、DMA等能力呈现给用户空间。 为了达到最高的IO性能，虚拟机就需要VFIO这种设备直通方式，因为它具有低延时、高带宽的特点，并且guest也能够直接使用设备的原生驱动。 这些优异的特点得益于VFIO对VT-d/AMD-Vi所提供的DMA Remapping和Interrupt Remapping机制的应用。 VFIO使用DMA Remapping为每个Domain建立独立的IOMMU Page Table将直通设备的DMA访问限制在Domain的地址空间之内保证了用户态DMA的安全性， 使用Interrupt Remapping来完成中断重映射和Interrupt Posting来达到中断隔离和中断直接投递的目的。 1. VFIO 框架简介 整个VFIO框架设计十分简洁清晰，可以用下面的一幅图描述： +-------------------------------------------+ | | | VFIO Interface | | | +---------------------+---------------------+ | | | | vfio_iommu | vfio_pci | | | | +---------------------+---------------------+ | | | | iommu driver | pci_bus driver | | | | +---------------------+---------------------+ 最上层的是VFIO Interface Layer，它负责向用户态提供统一访问的接口，用户态通过约定的ioctl设置和调用VFIO的各种能力。 中间层分别是vfio_iommu和vfio_pci，vfio_iommu是VFIO对iommu层的统一封装主要用来实现DMAP Remapping的功能，即管理IOMMU页表的能力。 vfio_pci是VFIO对pci设备驱动的统一封装，它和用户态进程一起配合完成设备访问直接访问，具体包括PCI配置空间模拟、PCI Bar空间重定向，Interrupt Remapping等。 最下面的一层则是硬件驱动调用层，iommu driver是与硬件平台相关的实现，例如它可能是intel iommu driver或amd iommu driver或者ppc iommu driver， 而同时vfio_pci会调用到host上的pci_bus driver来实现设备的注册和反注册等操作。 在了解VFIO之前需要了解3个基本概念：device, group, container，它们在逻辑上的关系如上图所示。 Group 是IOMMU能够进行DMA隔离的最小硬件单元，一个group内可能只有一个device，也可能有多个device，这取决于物理平台上硬件的IOMMU拓扑结构。 设备直通的时候一个group里面的设备必须都直通给一个虚拟机。 不能够让一个group里的多个device分别从属于2个不同的VM，也不允许部分device在host上而另一部分被分配到guest里， 因为就这样一个guest中的device可以利用DMA攻击获取另外一个guest里的数据，就无法做到物理上的DMA隔离。 另外，VFIO中的group和iommu group可以认为是同一个概念。 Device 指的是我们要操作的硬件设备，不过这里的\"设备\"需要从IOMMU拓扑的角度去理解。如果该设备是一个硬件拓扑上独立的设备，那么它自己就构成一个iommu group。 如果这里是一个multi-function设备，那么它和其他的function一起组成一个iommu group，因为多个function设备在物理硬件上就是互联的， 他们可以互相访问对方的数据，所以必须放到一个group里隔离起来。值得一提的是，对于支持PCIe ACS特性的硬件设备，我们可以认为他们在物理上是互相隔离的。 Container 是一个和地址空间相关联的概念，这里可以简单把它理解为一个VM Domain的物理内存空间。 从上图可以看出，一个或多个device从属于某个group，而一个或多个group又从属于一个container。 如果要将一个device直通给VM，那么先要找到这个设备从属的iommu group，然后将整个group加入到container中即可。关于如何使用VFIO可以参考内核文档： vfio.txt 2. VFIO 数据结构关系 Linux内核设备驱动充分利用了\"一切皆文件\"的思想，VFIO驱动也不例外，VFIO中为了方便操作device, group, container等对象将它们和对应的设备文件进行绑定。 VFIO驱动在加载的时候会创建一个名为 /dev/vfio/vfio 的文件，而这个文件的句柄关联到了vfio_container上，用户态进程打开这个文件就可以初始化和访问vfio_container。 当我们把一个设备直通给虚拟机时，首先要做的就是将这个设备从host上进行解绑，即解除host上此设备的驱动，然后将设备驱动绑定为\"vfio-pci\"， 在完成绑定后会新增一个 /dev/vfio/$groupid 的文件，其中$groupid为此PCI设备的iommu group id， 这个id号是在操作系统加载iommu driver遍历扫描host上的PCI设备的时候就已经分配好的，可以使用 readlink -f /sys/bus/pci/devices/$bdf/iommu_group 来查询。 类似的， /dev/vfio/$groupid 这个文件的句柄被关联到vfio_group上，用户态进程打开这个文件就可以管理这个iommu group里的设备。 然而VFIO中并没有为每个device单独创建一个文件，而是通过VFIO_GROUP_GET_DEVICE_FD这个ioctl来获取device的句柄，然后再通过这个句柄来管理设备。 VFIO框架中很重要的一部分是要完成DMA Remapping，即为Domain创建对应的IOMMU页表，这个部分是由vfio_iommu_driver来完成的。 vfio_container包含一个指针记录vfio_iommu_driver的信息，在x86上vfio_iommu_driver的具体实现是由vfio_iommu_type1来完成的。 其中包含了vfio_iommu, vfio_domain, vfio_group, vfio_dma等关键数据结构（注意这里是iommu里面的）， vfio_iommu可以认为是和container概念相对应的iommu数据结构，在虚拟化场景下每个虚拟机的物理地址空间映射到一个vfio_iommu上。 vfio_group可以认为是和group概念对应的iommu数据结构，它指向一个iommu_group对象，记录了着iommu_group的信息。 vfio_domain这个概念尤其需要注意，这里绝不能把它理解成一个虚拟机domain，它是一个与DRHD（即IOMMU硬件）相关的概念， 它的出现就是为了应对多IOMMU硬件的场景，我们知道在大规格服务器上可能会有多个IOMMU硬件，不同的IOMMU硬件有可能存在差异， 例如IOMMU 0支持IOMMU_CACHE而IOMMU 1不支持IOMMU_CACHE（当然这种情况少见，大部分平台上硬件功能是具备一致性的），这时候我们不能直接将分别属于不同IOMMU硬件管理的设备直接加入到一个container中， 因为它们的IOMMU页表SNP bit是不一致的。 因此，一种合理的解决办法就是把一个container划分多个vfio_domain，当然在大多数情况下我们只需要一个vfio_domain就足够了。 处在同一个vfio_domain中的设备共享IOMMU页表区域，不同的vfio_domain的页表属性又可以不一致，这样我们就可以支持跨IOMMU硬件的设备直通的混合场景。 经过上面的介绍和分析，我们可以把VFIO各个组件直接的关系用下图表示( 点击链接查看原图 )，读者可以按照图中的关系去阅读相关代码实现。 3. VFIO 中的技术关键点 除了DMA Remapping这一关键点之外，在虚拟化场景下VFIO还需要解决下面一些关键问题，需要进行探讨： VFIO对完备的设备访问支持：其中包括MMIO， I/O Port，PCI 配置空间，PCI BAR空间； VFIO中高效的设备中断机制，其中包括MSI/MSI-X，Interrupt Remapping，以及Posted Interrupt等； VFIO对直通设备热插拔支持。","tags":"virtualization"},{"title":"VT-d DMA Remapping","url":"https://kernelgo.org/dma-remapping.html","loc":"https://kernelgo.org/dma-remapping.html","text":"本文主要探讨一下VT-d DMA Remapping机制。在分析DMA Remapping之前回顾下什么是DMA，DMA是指在不经过CPU干预的情况下外设直接访问(Read/Write)主存(System Memroy)的能力。 DMA带来的最大好处是：CPU不再需要干预外设对内存的访问过程，而是可以去做其他的事情，这样就大大提高了CPU的利用率。 在设备直通(Device Passthough)的虚拟化场景下，直通设备在工作的时候同样要使用DMA技术来访问虚拟机的主存以提升IO性能。那么问题来了，直接分配给某个特定的虚拟机的，我们必须要保证直通设备DMA的安全性，一个VM的直通设备不能通过DMA访问到其他VM的内存，同时也不能直接访问Host的内存，否则会造成极其严重的后果。因此，必须对直通设备进行\"DMA隔离\"和\"DMA地址翻译\"，隔离将直通设备的DMA访问限制在其所在VM的物理地址空间内保证不发生访问越界，地址翻译则保证了直通设备的DMA能够被正确重定向到虚拟机的物理地址空间内。为什么直通设备会存在DMA访问的安全性问题呢？原因也很简单：由于直通设备进行DMA操作的时候guest驱动直接使用gpa来访问内存的，这就导致如果不加以隔离和地址翻译必然会访问到其他VM的物理内存或者破坏Host内存，因此必须有一套机制能够将gpa转换为对应的hpa这样直通设备的DMA操作才能够顺利完成。 VT-d DMA Remapping的引入就是为了解决直通设备DMA隔离和DMA地址翻译的问题，下面我们将对其原理进行分析，主要参考资料是 Intel VT-d SPEC Chapter 3 。 1 DMA Remapping 简介 VT-d DMA Remapping的硬件能力主要是由IOMMU来提供，通过引入根Context Entry和IOMMU Domain Page Table等机制来实现直通设备隔离和DMA地址转换的目的。那么具体是怎样实现的呢？下面将对其进行介绍。 根据DMA Request是否包含地址空间标志(address-space-identifier)我们将DMA Request分为2类： Requests without address-space-identifier: 不含地址空间标志的DMA Request，这种一般是endpoint devices的普通请求，请求内容仅包含请求的类型(read/write/atomics)，DMA请求的address/size以及请求设备的标志符等。 Requests with address-space-identifier: 包含地址空间描述标志的DMA Request，此类请求需要包含额外信息以提供目标进程的地址空间标志符(PASID)，以及Execute-Requested (ER) flag和 Privileged-mode-Requested 等细节信息。 为了简单，通常称上面两类DMA请求简称为：Requests-without-PASID和Requests-with-PASID。本节我们只讨论Requests-without-PASID，后面我们会在讨论Shared Virtual Memory的文中单独讨论Requests-with-PASID。 首先要明确的是DMA Isolation是以Domain为单位进行隔离的，在虚拟化环境下可以认为每个VM的地址空间为一个Domain，直通给这个VM的设备只能访问这个VM的地址空间这就称之为\"隔离\"。根据软件的使用模型不同，直通设备的DMA Address Space可能是某个VM的Guest Physical Address Space或某个进程的虚拟地址空间（由分配给进程的PASID定义）或是由软件定义的一段抽象的IO Virtual Address space (IOVA)，总之DMA Remapping就是要能够将设备发起的DMA Request进行DMA Translation重映射到对应的HPA上。下面的图描述了DMA Translation的原理，这和MMU将虚拟地址翻译成物理地址的过程非常的类似。 值得一提的是，Host平台上可能会存在一个或者多个DMA Remapping硬件单元，而每个硬件单元支持在它管理的设备范围内的所有设备的DMA Remapping。例如，你的台式机CPU Core i7 7700k在MCH中只集成一个DMA Remapping硬件单元(IOMMU)，但在多路服务器上可能集成有多个DMA Remapping硬件单元。每个硬件单元负责管理挂载到它所在的PCIe Root Port下所有设备的DMA请求。BIOS会将平台上的DMA Remapping硬件信息通过ACPI协议报告给操作系统，再由操作系统来初始化和管理这些硬件设备。 为了实现DMA隔离，我们需要对直通设备进行标志，而这是通过PCIe的Request ID来完成的。根据PCIe的SPEC，每个PCIe设备的请求都包含了PCI Bus/Device/Function信息，通过BDF号我们可以唯一确定一个PCIe设备。 同时为了能够记录直通设备和每个Domain的关系，VT-d引入了root-entry/context-entry的概念，通过查询root-entry/context-entry表就可以获得直通设备和Domain之间的映射关系。 Root-table是一个4K页，共包含了256项root-entry，分别覆盖了PCI的Bus0-255，每个root-entry占16-Byte，记录了当前PCI Bus上的设备映射关系，通过PCI Bus Number进行索引。 Root-table的基地址存放在Root Table Address Register当中。Root-entry中记录的关键信息有： Present Flag：代表着该Bus号对应的Root-Entry是否呈现，CTP域是否初始化； Context-table pointer (CTP)：CTP记录了当前Bus号对应点Context Table的地址。 同样每个context-table也是一个4K页，记录一个特定的PCI设备和它被分配的Domain的映射关系，即对应Domain的DMA地址翻译结构信息的地址。 每个root-entry包含了该Bus号对应的context-table指针，指向一个context-table，而每张context-table包又含256个context-entry， 其中每个entry对应了一个Device Function号所确认的设备的信息。通过2级表项的查询我们就能够获得指定PCI被分配的Domain的地址翻译结构信息。Context-entry中记录的信息有： Present Flag：表示该设备对应的context-entry是否被初始化，如果当前平台上没有该设备Preset域为0，索引到该设备的请求也会被block掉。 Translation Type：表示哪种请求将被允许； Address Width：表示该设备被分配的Domain的地址宽度； Second-level Page-table Pointer：二阶页表指针提供了DMA地址翻译结构的HPA地址（这里仅针对Requests-without-PASID而言）； Domain Identifier: Domain标志符表示当前设备的被分配到的Domain的标志，硬件会利用此域来标记context-entry cache，这里有点类似VPID的意思； Fault Processing Disable Flag：此域表示是否需要选择性的disable此entry相关的remapping faults reporting。 因为多个设备有可能被分配到同一个Domain，这时只需要将其中每个设备context-entry项的 Second-level Page-table Pointer 设置为对同一个Domain的引用， 并将Domain ID赋值为同一个Domian的就行了。 2 DMA隔离和地址翻译 VT-d中引入root-table和context-table的目的比较明显，这些额外的table的存在就是为了记录每个直通设备和其被分配的Domain之间的映射关系。 有了这个映射关系后，DMA隔离的实现就变得非常简单。 IOMMU硬件会截获直通设备发出的请求，然后根据其Request ID查表找到对应的Address Translation Structure即该Domain的IOMMU页表基地址， 这样一来该设备的DMA地址翻译就只会按这个Domain的IOMMU页表的方式进行翻译，翻译后的HPA必然落在此Domain的地址空间内（这个过程由IOMMU硬件中自动完成）， 而不会访问到其他Domain的地址空间，这样就达到了DMA隔离的目的。 DMA地址翻译的过程和虚拟地址翻译的过程是完全一致的，唯一不同的地方在于MMU地址翻译是将进程的虚拟地址(HVA)翻译成物理地址(HPA)，而IOMMU地址翻译则是将虚拟机物理地址空间内的GPA翻译成HPA。IOMMU页表和MMU页表一样，都采用了多级页表的方式来进行翻译。例如，对于一个48bit的GPA地址空间的Domain而言，其IOMMU Page Table共分4级，每一级都是一个4KB页含有512个8-Byte的目录项。和MMU页表一样，IOMMU页表页支持2M/1G大页内存，同时硬件上还提供了IO-TLB来缓存最近翻译过的地址来提升地址翻译的速度。","tags":"virtualization"},{"title":"VT-d Posted Interrupt","url":"https://kernelgo.org/posted-interrupt.html","loc":"https://kernelgo.org/posted-interrupt.html","text":"VT-d Interrupt Remapping的引入改变了以往设备中断的投递方式， Remapping格式的中断请求不再包含目标CPU的APIC-ID、中断vector号、投递方式等重要信息， 而是仅仅提供了一个16 bit的interrupt_index用来索引中断重定向表项(IRTE)， 这个改变带来的最大好处是 提升了中断处理的灵活性 。 在虚拟化的环境下，为了提升虚拟机的中断实时性，Intel在Interrupt Remapping的基础上加以改进 引入了Interrupt Posting机制， 从硬件层面实现了中断隔离和中断自动迁移等重要特性 。 1 Interrupt Posting 简介 VT-d Interrupt Posting是基于Interrupt Remapping的一种扩展的中断处理方式，其主要用途是在虚拟化场景下， 可以大幅提升VMM处理直通设备中断的效率。硬件通过Capability Register(CAP_REG)的PI位来报告interrupt posting capability。 根据前面介绍 Interrupt Remapping 的文章可以知道， 所有的Remapping格式中断请求都需要通过中断重映射表来投递， IRTE中的Mode域(IM)用来指定这个remappable中断请求是interrupt-remapping方式还是interrupt-posting方式。 IRTE的IM位为0表示中断按照remappable方式处理； IRTE的IM位为1表示中断按照posted方式来处理。 在Interrupt Posting模式下，新增了一个与VCPU相关的内存数据结构叫做\"Posted Interrupt Descriptor\"(PD)， 这是一个64-Byte对齐的数据结构并且直接被硬件用来记录将要post的中断请求。PD结构包含以下的域： Posted Interrupt Request (PIR)域，提供记录需要post的中断占256bit每个bit代表一个中断号。 Outstanding Notification (ON)域，由硬件来自动更新，用来表示是否有中断请求pending。当此位为0时，硬件通过修改其为1来产生一个通知事件告知中断请求到来。接收这个通知事件的实体(处理器或者软件)在处理这个posted interrupt时后必须将其清零。 Suppress Notification (SN)域，表示non-urgent中断请求的通知事件是否要被supressed(抑制)。 Notification Vector (NV)域，用来指定产生posted-interrupt\"通知事件\"(notification event)的vector号。 Notification Destination (NDST)域，用来指定此中断要投递的vCPU所运行物理CPU的APIC-ID。 在Interrupt Posting模式下IRTE格式相当于Remapping模式有很大不同（参考附录），IRTE的格式相对于Remapping模式新增了以下几个域： 中断请求对应的Posted Interrupt Descriptor数据结构地址，包含高地址和低地址2个域； Urgent ( URG )标志来指定中断请求是否需要 实时处理 ； 一个用来指定要post的vector号的Vector域，与Remapping格式不同的是posted-format的IRTEs的Vector域是用来决定Posted Interrupt Descriptor里的PIR域的哪个bit要置位。 2 Interrupt Posting 的硬件处理步骤 当一个Remapping格式的中断请求IM位为1时，意味着这个中断请求要按照Interrupt Posting方式进行处理。整个过程中断硬件处理流程如下: 如果中断请求索引到的IRTE的IM位被置位(1b)： 硬件按照posted format解读IRTE，如果IRTE的格式检查不通过，那么该请求被blocked。如果检查通过从IRTE中提取Posted Interrupt Descriptor的地址(PDA-L/PDA-H)，中断请求的vector号以及中断请求是否为URG等信息。 硬件会对Posted Interrupt Descriptor内存数据结构执行一个read-modify-write原子操作： 首先读取PD的内容并对其进行检测，如果发现格式不对（例如reserved域不为0）那么将该请求block掉。如果检测通过那么获取当前的PIR,ON,NV,NDST域信息后，按照下面的规则对PD进行原子更新： 根据IRTE的Vecotr域设置PIR对应的bit 计算出 X = ((ON == 0) & (URG | (SN == 0))), 如果X==1那么把ON置位。 如果X==1，那么产生一个\"通知事件中断\"，并且这个中断的属性为： NSDT表示VCPU所在的CPU的physical APIC-ID (注意：xAPIC和x2APIC模式下的不同) NV域指定了被用来通知目的CPU有个posted-interrupt已在pending的\"通知事件\"的中断向量。（注意不是要post的中断请求vector号，这个仅仅用做通知用） Delivery mode域被强制设定为Fixed (000b) Re-direction Hint域强制清零 (0b) Triger Mode域被设置为Edge (0b) Trigger Mode Level域被设置为Asserted (1b) 3 Interrupt Posting 的软件处理步骤 当一个设备被直通给虚拟机后，虚拟机初始化的过程中VMM会设置好此设备的MSI/MSI-X中断对应的IRTE并标志IM位为1b，标志这是一个Posted Interrupt。当直通设备投递一个中断后，硬件首先会去查询irq对应的IRTE并从IRTE中提取记录的Posted Interrupt Descriptor地址和vector信息，然后更新PIR域和ON域并且将vector信息写入到VCPU的vAPIC Page中，直接给处于None Root模式的VCPU注入一个中断，整个过程不需要VMM的介入从而十分高效。Intel的虚拟化专家FengWu使用下面的图很好的描述了Interrupt Posting的处理过程： 从上面的描述来看，Interrupt Posting是不是看起来很简单？然而，实际实现上却还是要复杂多，不过也不要被吓到额！ 从软件层面来说，VMM需要参与进来做以下一些额外的工作来使能Interrupt Posting机制： 为虚拟机的每个VCPU分配一个PD用来存放此VCPU的Posted Interrupt信息（PD的地址会被记录到VCPU的VMSC里面）； VMM需要在每个PCPU上安排2个中断vector用来接受通知事件： 其中一个物理vector被称之为'Active Notification Vector' (ANV)，它被用来post通知事件到处于Running状态的VCPU上（这个IPI中断是guest接收的）。 另一个物理vector被称之为'Wake-up Notification Vector' (WNV)，它被用来post通知事件到处于Blocked状态的VCPU上（这个IPI中断是host接收的）。 对于直通到此虚拟机的直通设备，VMM都会干预进来（因为虚拟机的IOxAPIC,LAPIC等都是kvm内核模块来模拟的），VMM能够知道到每个VCPU上的vector号分配情况； 对于每个直通设备的中断： VMM会为每个中断源分配一个IRTE，并且把对应的guest分配的vecotr号填入到IRTE的vector域。 VMM会将每个VCPU对应的PD地址填入到此中断源的对用的IRTE地址域。 如果此中断需要立即处理，那么VMM会将对此中断源对应的IRTE中URG域置成1。 同时VMM还需要为VCPU使能APICv特性（包括了'virtual-interrupt delivery'和'process posted interrupts'），并且将此VCPU的VMCS域POSTED_INTR_NV配置为ANV，并将申请的PD的地址配置到VMCS的POSTED_INTR_DESC_ADDR域来告诉VCPU它关联的PD在哪儿。（注：这些操作在VCPU初始化流程中完成） 在VCPU调度的过程中，VMM需要按照下面的方式来管理VCPU的调度状态： 当VCPU被scheduler选中调度进来运行的的时候，此时VCPU的状态被标志为'Active'状态。这个时候VMM需要将PD的NV域更新为ANV的值。 同时在这种场景下，此VCPU上接受的Posted Interrupt中断会被直接复制到vAPIC Page中，guest在非根模式下就能直接处理此中断，而不需要VMM的参与。 当一个VCPU被抢占（Preempted），例如时间片到期了，这时候需要将PD的SN域标志为1，即将VCPU更新为'Preempted'状态，告诉硬件当前VCPU已经没在非根模式下运行了。 此时，这个VCPU上的non-urgent中断都会被接受但不会产生通知事件。 但如果这个VCPU上有标志为URG类型的中断时，VMM同时也会将PD的NV域修改为WNV，这样一来VMM就能够将URG中断请求投递给处于not running状态的VCPU，并进行适当的软件处理（例如，抢占正在同一个物理CPU上运行状态的其他VCPU，并将自己调度进来）。 当一个VCPU执行了hlt指令或者触发了ple，VMM也会干预进来将VCPU给block出来，并且将VCPU状态标识为Hlted状态。在此状态下VMM需要将VCPU对应的PD的NV域设置为WNV。这样一来，当中断请求post到此VCPU时，VMM能够接受到Wake-up Notification Event事件通知并做出适当的软件操作。（例如：立即对此VCPU进行一次调度） 当VCPU重新进入非根模式或者从hlt恢复执行时（注意这个时候vCPU还没进入根模式下），VMM对此VCPU上处于pending状态的posted interrupt进行处理： 首先将PD的NV域设置为ANV以标志VCPU为Active状态； 扫描PD的PIR域检测是否有处于pending状态的posted interrupt请求； 如果有处于pending状态的posted interrupt请求，VMM会在LAPIC上生成一个vector号为ANV的self-IPI(注意：在还未真正enter guest之前当前CPU处于关中断状态)。 那么当VCPU刚刚打开中断，准备进入到非根模式下的时候，就立刻接受到一个self-IPI，那么处理器硬件这时候就会将它当做posted-interrupt通知事件来处理，立刻从LAPIC中读取pending的中断并进行处理。 这样的好处是将guest对于posted interrupt的处理完全off load到处理器硬件上。 VMM同样能够将 posted interrupt processing 技术应用到模拟设备产生的虚拟设备中断处理上（不仅仅是直通设备额）。而这只需VMM执行原子操作'post'一个虚拟中断到PD上，并给PD中NDST的逻辑CPU发送一个IPI作为通知事件告诉该CPU有posted interrupt到来。（这里说的是VT-x的Posted Interrupt） 当VCPU在不同PCPU之间进行迁移时，VMM会对VCPU对应的PD的NDST域进行更新，将NDST改为VCPU要迁移的目的PCPU的APIC-ID。这也就是说，在VCPU迁移的过程中我们也顺便完成了中断迁移。这样一来，新的posted interrupt 通知事件到来时就会被自动路由的新的PCPU上，是不是很巧妙？ 备注：请思考，ANV和WNV通知事件的目的分别是什么？ ANV事件通知的目的是： 当vCPU被抢占或者处于Post Block阶段，这个时候vCPU是处于Runnable状态（已经被调度器选中，但还没进入非根模式enter_guest），这个时候如果收到直通设备中断，那么就需要ANV事件来告知一下，这样在vCPU进入非根模式下能够立刻处理这个中断。 WNV事件通知的目的是：当vCPU被Block出来的时候会进入休眠状态，没有特殊事件不会被调度器调度，那么这时候来中断了就用WNV来通知一下， 将vCPU唤醒进来参与调度，让vCPU及时处理中断。 再次引用一下FengWu的图片来说明下VCPU在发生状态改变的时候，VMM做了哪些操作来保证posted interrupt能够顺利完成，请读者配合上面的描述自行进行梳理。 4 附: Posting格式中断重映射表项的格式","tags":"virtualization"},{"title":"VT-d Interrupt Remapping","url":"https://kernelgo.org/interrupt-remapping.html","loc":"https://kernelgo.org/interrupt-remapping.html","text":"Intel VT-d 虚拟化方案主要目的是解决IO虚拟化中的安全和性能这两个问题，这其中最为核心的技术就是DMA Remapping和Interrupt Remapping。 DMA Remapping通过IOMMU页表方式将直通设备对内存的访问限制到特定的domain中，在提高IO性能的同时完成了直通设备的隔离，保证了直通设备DMA的安全性。Interrupt Remapping则提供IO设备的中断重映射和路由功能，来达到中断隔离和中断迁移的目的，提升了虚拟化环境下直通设备的中断处理效率。 思考一下为什么要搞中断重映射这么一套东西呢？ 直通设备的中断不能直通到虚拟机内部吗？ 我们知道直通场景下直通设备的MSI/MSI-X Msg信息都是由Guest直接分配的，那么问题来了设备发送中断的时候写的Msg地址是GPA，肯定不能直接往host上投递，否则就乱套了。在虚拟化场景下，直通设备的中断是无法直接投递到Guest中的，那么我们该怎么办？ 我们可以由IOMMU截获中断，先将其中断映射到host的某个中断上，然后再重定向（由VMM投递）到Guest内部。明白这一点，很重要！ 下面对VT-d Interrupt Remapping机制进行一点分析，主要参考资料是 Intel VT-d SPEC Chapter 5 。 1 Interrupt Remapping 简介 Interrupt Remapping的出现改变了x86体系结构上的中断投递方式，外部中断源发出的中断请求格式发生了较大的改变， 中断请求会先被中断重映射硬件截获后再通过查询中断重映射表的方式最终投递到目标CPU上。 这些外部设备中断源则包括了中断控制器(I/OxAPICs)以及MSI/MSIX兼容设备PCI/PCIe设备等。 Interrupt Remapping是需要硬件来支持的，这里的硬件应该主要是指的IOMMU（尽管intel手册并没有直接说明），Interrupt Remapping的Capability是通过Extended Capability Register来报告的。 在没有使能Interrupt Remapping的情况下，设备中断请求格式称之为 Compatibility format ，其结构主要包含一个32bit的Address和一个32bit的Data字段，Address字段包含了中断要投递的目标CPU的APIC ID信息，Data字段主要包含了要投递的vecotr号和投递方式。结构如下图： 其中Address的bit 4为Interrupt Format位，用来标志这个Request是Compatibility format（bit4=0）还是Remapping format (bit 4=1)。 在开启了Interrupt Remapping之后，设备的中断请求格式称之为 Remapping format ，其结构同样由一个32bit的Address和一个32bit的Data字段构成。但与Compatibility format不同的是此时Adress字段不再包含目标CPU的APIC ID信息而是提供了一个16bit的HANDLE索引，并且Address的bit 4为\"1\"表示Request为Remapping format。同时bit 3是一个标识位(SHV)，用来标志Request是否包含了SubHandle，当该位置位时表示Data字段的低16bit为SubHandle索引。Remapping format的中断请求格式如下图： 在Interrupt Remapping模式下，硬件查询系统软件在内存中预设的中断重映射表(Interrupt Remapping Table)来投递中断。中断重映射表由中断重映射表项(Interrupt Remapping Table Entry)构成，每个IRTE占用16字节（具体格式介绍见文末），中断重映射表的基地址存放在Interrupt Remapping Table Address Register中。硬件通过下面的方式去计算中断的 interrupt_index ： if ( address . SHV == 0 ) { interrupt_index = address . handle ; } else { interrupt_index = ( address . handle + data . subhandle ) ; } 备注：引入subhandle的目的是为了兼容MSI中断场景支持对\"单个address多个data\"的中断投递方式。 中断重映射硬件通过 interrupt_index 去重映射表中索引对应的IRTE，中断重映射硬件可以缓存那些经常使用的IRTE以提升性能。(注:由于handle为16bit，故每个IRT包含65536个IRTE，占用1MB内存空间) 2 外设的中断投递方式和中断处理 针对不同的中断源，需要采用不同的方式来投递Remapping格式的中断。 对I/OxAPIC而言，其Remapping格式中断投递格式如下图，软件需要按图中的格式来发起Remapping中断请求，这就要求需要修改\"中断重定向表项\"(Interrupt Redirection Table Entry)， 注意 不要将ioapic中断重定向和vtd中断重映射搞混淆，这是两个不同的概念，读者可以参考 wiki 对比下RTE相比于Compatibility格式有哪些不同。值得注意的是bit48这里需要设置为\"1\"用来标志此RTE为Remapping format，并且RTE的bit10:8固定为000b(即没有SubHandle)。而且vector字段必须和IRTE的vector字段相同！ 对于MSI和MSI-X而言，其Remapping格式中断投递格式如下图，值得注意的是在Remapping格式下MSI中断支持multiple vector（大于32个中断向量），但软件必须连续分配N个连续的IRTE并且 interrupt_index 对应HANDLE号必须为N个连续的IRTE的首个。同样bit 4必须为\"1\"用来表示中断请求为Remapping格式。Data位全部设置为\"0\"! 中断重映射的硬件处理步骤如下： 硬件识别到物理地址0xFEEx_xxxx范围内的DWORD写请时，将该请求认定为中断请求； 当Interrupt Remapping没有使能时，所有的中断都按照Compatibility format来处理； 当Intgrrupt Remapping被使能时，中断请求处理流程如下： 如果来的中断请求为Compatibility format： 先检测 IRTA (Interrupt Remapping Table Address Register)寄存器的EIME位，如果该位为\"1\"那么Compatibility format的中断被blocked，否则Compatibility format中断请求都按照pass-through方式处理（传统方式）。 如果来的中断请求为Remapping format： 先检测reserved fileds是否为0，如果检查失败那么中断请求被blocked。接着硬件按照上面提到的算法计算出 interrupt_index 并检测其是否合法，如果该 interrupt_index 合法那么根据 interrupt_index 索引中断重映射表找到对应的IRTE，然后检测IRTE中的Present位，如果Preset位为0那么中断请求被blocked，如果Present位为1，硬件校验IRTE其他field合法后按照IRTE的约定产生一条中断请求。 中断重映射的软件处理步骤如下： 分配一个IRTE并且按照IRTE的格式要求填好IRTE的每个属性； 按照Remapping format的要求对中断源进行编程，在合适的时候触发一个Remapping format格式的中断请求。 附：Remapping格式中断重映射表项的格式 Interrupt Remapping格式的中断重映射表项的格式为（下篇会介绍Interrupt Posting格式的中断重映射表项）: 其中比较关键的中断描述信息为： Present域(P)：0b表示此IRTE还没有被分配到任何的中断源，索引到此IRTE的Remapping中断将被blocked掉，1b表示此IRTE是有效的，已经被分配到某个设备。 Destination Mode域(DM)：0b表示Destination ID域为Physical APIC-ID，1b表示Destination ID域是Logical APIC-ID。 IRTE Mode域(IM)：0b表示此中断请求是一个Remapped Interrupt中断请求，1b表示此中断请求是一个Posted Interrupt中断请求。 Vector域(V)：共8个Byte表示了此Remapped Interrupt中断请求的vector号(Remapped Interrupt)。 Destination ID域(DST)：表示此中断请求的目标CPU，根据当前Host中断方式不同具有不同的格式。xAPIC Mode (Cluster)为bit[40:47]， xAPIC Mode (Flat)和xAPIC Mode (Physical)为bit[47:40]， x2APIC Mode (Cluster)和x2APIC Mode (Physical)为bit[31:0]。 SID, SQ, SVT则联合起来表示了中断请求的设备PCI/PCI-e request-id信息。","tags":"virtualization"},{"title":"kprobe kretprobe example","url":"https://kernelgo.org/kprobe.html","loc":"https://kernelgo.org/kprobe.html","text":"有时候想知道下发某个操作后内核在做些什么，这个时候就要内内核进行调试， 然而KGDB这种方法操作起来相对麻烦，这个时候我们就可以使用kprobe来探测内核的行为。 介绍kprobe和kretprobe的文档为: https://www.kernel.org/doc/Documentation/kprobes.txt 划重点 ，这里是文档和很多博客没有解释清楚的地方： 对于kprobe而言，理论上它可以probe任何一个地方（只需要指定某个代码段地址就行了，例如函数的地址）； 对于kprobe而言，pre_handler回调函数执行是发生在probe断点执行之前，post_handler回调执行是发生在probe断点 单步执行 之后，而不是函数返回之前； 对于kretprobe而言，一般只用来探测函数，entry_handler回调执行是在函数入口的地方（这时候我们可以探测函数的入参），handler回调函数执行是发生在函数准备返回的时候，注意这个时候参数都已经弹栈，我们只能探测函数的返回值。 下面举一个最简单的例子，介绍如何使用kprobe来查看 inet_bind 这个函数的调用情况。 inet_bind 函数是在发生ipv4 socket bind阶段调用的一个内核函数。 #include <linux/kernel.h> #include <linux/module.h> #include <linux/sched/clock.h> #include <linux/kprobes.h> #include <linux/errno.h> #include <linux/stddef.h> #include <linux/bug.h> #include <linux/ptrace.h> #include <linux/socket.h> #include <linux/kmod.h> #include <linux/sched.h> #include <linux/string.h> #include <linux/sockios.h> #include <linux/net.h> #include <linux/inet.h> #include <net/ip.h> #include <net/tcp.h> #include <linux/skbuff.h> #include <net/sock.h> #include <net/inet_common.h> /* For each probe you need to allocate a kprobe structure */ static struct kprobe kp = { . symbol_name = \"inet_bind\" , }; /* kprobe pre_handler: called just before the probed instruction is executed */ static int handler_pre ( struct kprobe * p , struct pt_regs * regs ) { #ifdef CONFIG_ARM64 struct socket * sock = regs -> regs [ 0 ]; struct sockaddr * uaddr = regs -> regs [ 1 ]; #endif #ifdef CONFIG_X86 struct socket * sock = regs -> di ; struct sockaddr * uaddr = regs -> si ; #endif struct sockaddr_in * addr = ( struct sockaddr_in * ) uaddr ; unsigned short snum = ntohs ( addr -> sin_port ); pr_info ( \"%s name:%s pid:%d socket bind port=%d \\n \" , p -> symbol_name , current -> comm , task_pid_nr ( current ), snum ); return 0 ; } /* kprobe post_handler: called after the probed instruction is executed */ static void handler_post ( struct kprobe * p , struct pt_regs * regs , unsigned long flags ) { pr_info ( \"%s called \\n \" , __func__ ); } /* * fault_handler: this is called if an exception is generated for any * instruction within the pre- or post-handler, or when Kprobes * single-steps the probed instruction. */ static int handler_fault ( struct kprobe * p , struct pt_regs * regs , int trapnr ) { printk ( KERN_INFO \"fault_handler: p->addr = 0x%p, trap #%dn\" , p -> addr , trapnr ); /* Return 0 because we don't handle the fault. */ return 0 ; } static int __init kprobe_init ( void ) { int ret ; kp . pre_handler = handler_pre ; kp . post_handler = handler_post ; kp . fault_handler = handler_fault ; ret = register_kprobe ( & kp ); if ( ret < 0 ) { printk ( KERN_INFO \"register_kprobe failed, returned %d \\n \" , ret ); return ret ; } printk ( KERN_INFO \"Planted kprobe at %p \\n \" , kp . addr ); return 0 ; } static void __exit kprobe_exit ( void ) { unregister_kprobe ( & kp ); printk ( KERN_INFO \"kprobe at %p unregistered \\n \" , kp . addr ); } module_init ( kprobe_init ) module_exit ( kprobe_exit ) MODULE_LICENSE ( \"GPL\" ); 值得一提的是，kprobe里面我们在probe某个函数的时候，获取函数参数的时候是和体系结构相关的。 例如：在x86平台上，根据C ABI ptrace 接口规范，函数的参数和pt_regs的对应关系是： struct pt_regs { /* * C ABI says these regs are callee-preserved. They aren't saved on kernel entry * unless syscall needs a complete, fully filled \"struct pt_regs\". */ unsigned long r15 ; unsigned long r14 ; unsigned long r13 ; unsigned long r12 ; unsigned long bp ; unsigned long bx ; /* These regs are callee-clobbered. Always saved on kernel entry. */ unsigned long r11 ; unsigned long r10 ; unsigned long r9 ; unsigned long r8 ; unsigned long ax ; unsigned long cx ; // mapped to arg[3] unsigned long dx ; // mapped to arg[2] unsigned long si ; // mapped to arg[1] unsigned long di ; // mapped to arg[0] /* * On syscall entry, this is syscall#. On CPU exception, this is error code. * On hw interrupt, it's IRQ number: */ unsigned long orig_ax ; /* Return frame for iretq */ unsigned long ip ; unsigned long cs ; unsigned long flags ; unsigned long sp ; unsigned long ss ; /* top of stack page */ }; 在ARM64平台上，根据C ABI ptrace 规范， 函数的参数和pt_regs的对应关系是：入参args[0]对应了regs[0]，入参args[1]对应regs[1]依此类推。 /* * This struct defines the way the registers are stored on the stack during an * exception. Note that sizeof(struct pt_regs) has to be a multiple of 16 (for * stack alignment). struct user_pt_regs must form a prefix of struct pt_regs. */ struct pt_regs { union { struct user_pt_regs user_regs ; struct { u64 regs [ 31 ]; u64 sp ; u64 pc ; u64 pstate ; }; }; u64 orig_x0 ; #ifdef __AARCH64EB__ u32 unused2 ; s32 syscallno ; #else s32 syscallno ; u32 unused2 ; #endif u64 orig_addr_limit ; /* Only valid when ARM64_HAS_IRQ_PRIO_MASKING is enabled. */ u64 pmr_save ; u64 stackframe [ 2 ]; }; 使用下面的Makefile文件，对其进行编译。 obj-m : = kprobe_example.o CROSS_COMPILE = '' KDIR : = /lib/modules/ $( shell uname -r ) /build all: make -C $( KDIR ) M = $( PWD ) modules clean: rm -f *.ko *.o *.mod.o *.mod.c .*.cmd *.symvers modul* 编译完成后会生产一个名为 kprobe_example.ko 的内核模块文件，执行 insmod kprobe_example.ko 后内核模块立即生效，通过 dmesg 命令可以查看到 inet_bind 这个函数的调用情况。从dmesg日志可以看到： [46071.632951] inet_bind name:test pid:68248 socdmket bind port=49152 [46071.632984] inet_bind name:test pid:68248 socket bind port=49152 [46071.632995] inet_bind name:test pid:68248 socket bind port=49152 kretprobe可以用来探测函数的返回值，示例中我们用它来探测 inet_release 函数的返回值和执行时间： /* * kretprobe_example.c * * Here's a sample kernel module showing the use of return probes to * report the return value and total time taken for probed function * to run. * * usage: insmod kretprobe_example.ko func=<func_name> * * If no func_name is specified, inet_release is instrumented * * For more information on theory of operation of kretprobes, see * Documentation/kprobes.txt * * Build and insert the kernel module as done in the kprobe example. * You will see the trace data in /var/log/messages and on the console * whenever the probed function returns. (Some messages may be suppressed * if syslogd is configured to eliminate duplicate messages.) */ #include <linux/kernel.h> #include <linux/module.h> #include <linux/kprobes.h> #include <linux/ktime.h> #include <linux/limits.h> #include <linux/sched.h> static char func_name [ NAME_MAX ] = \"inet_release\" ; module_param_string ( func , func_name , NAME_MAX , S_IRUGO ); MODULE_PARM_DESC ( func , \"Function to kretprobe; this module will report the\" \" function's execution time\" ); /* per-instance private data */ struct my_data { ktime_t entry_stamp ; }; /* Here we use the entry_hanlder to timestamp function entry */ static int entry_handler ( struct kretprobe_instance * ri , struct pt_regs * regs ) { struct my_data * data ; if ( ! current -> mm ) return 1 ; /* Skip kernel threads */ data = ( struct my_data * ) ri -> data ; data -> entry_stamp = ktime_get (); return 0 ; } /* * Return-probe handler: Log the return value and duration. Duration may turn * out to be zero consistently, depending upon the granularity of time * accounting on the platform. */ static int ret_handler ( struct kretprobe_instance * ri , struct pt_regs * regs ) { int retval = regs_return_value ( regs ); struct my_data * data = ( struct my_data * ) ri -> data ; s64 delta ; ktime_t now ; now = ktime_get (); delta = ktime_to_ns ( ktime_sub ( now , data -> entry_stamp )); printk ( KERN_INFO \"%s returned %d and took %lld ns to execute \\n \" , func_name , retval , ( long long ) delta ); return 0 ; } static struct kretprobe my_kretprobe = { . handler = ret_handler , . entry_handler = entry_handler , . data_size = sizeof ( struct my_data ), /* Probe up to 20 instances concurrently. */ . maxactive = 20 , }; static int __init kretprobe_init ( void ) { int ret ; my_kretprobe . kp . symbol_name = func_name ; ret = register_kretprobe ( & my_kretprobe ); if ( ret < 0 ) { printk ( KERN_INFO \"register_kretprobe failed, returned %d \\n \" , ret ); return -1 ; } printk ( KERN_INFO \"Planted return probe at %s: %p \\n \" , my_kretprobe . kp . symbol_name , my_kretprobe . kp . addr ); return 0 ; } static void __exit kretprobe_exit ( void ) { unregister_kretprobe ( & my_kretprobe ); printk ( KERN_INFO \"kretprobe at %p unregistered \\n \" , my_kretprobe . kp . addr ); /* nmissed > 0 suggests that maxactive was set too low. */ printk ( KERN_INFO \"Missed probing %d instances of %s \\n \" , my_kretprobe . nmissed , my_kretprobe . kp . symbol_name ); } module_init ( kretprobe_init ) module_exit ( kretprobe_exit ) MODULE_LICENSE ( \"GPL\" ); 探测的结果输出如下： [ 60362 .085372 ] inet_release returned 0 and took 16360 ns to execute [ 60362 .091124 ] inet_release returned 0 and took 8880 ns to execute [ 60362 .091147 ] inet_release returned 0 and took 7640 ns to execute [ 60362 .091173 ] inet_release returned 0 and took 7900 ns to execute [ 60362 .941665 ] inet_release returned 0 and took 9100 ns to execute [ 60363 .099577 ] inet_release returned 0 and took 9240 ns to execute [ 60363 .126682 ] inet_release returned 0 and took 6000 ns to execute [ 60363 .153610 ] inet_release returned 0 and took 9060 ns to execute [ 60363 .153820 ] inet_release returned 0 and took 3220 ns to execute [ 60363 .154699 ] inet_release returned 0 and took 3260 ns to execute [ 60363 .159178 ] inet_release returned 0 and took 3200 ns to execute [ 60363 .180098 ] inet_release returned 0 and took 3080 ns to execute","tags":"linux"}]};